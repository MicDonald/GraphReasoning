{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd03d364",
   "metadata": {},
   "source": [
    "# GraphReasoning: Scientific Discovery through Knowledge Extraction and Multimodal Graph-based Representation and Reasoning\n",
    "\n",
    "Markus J. Buehler, MIT, 2024 mbuehler@MIT.EDU\n",
    "\n",
    "### Example: GraphReasoning: Loading graph and graph analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbeb084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b24034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Markdown\n",
    "from huggingface_hub import hf_hub_download\n",
    "from GraphReasoning import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a4f00a",
   "metadata": {},
   "source": [
    "### Load graph and embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef91193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dir='./GRAPHDATA_TSMC/'\n",
    "data_dir_output='./GRAPHDATA_TSMC_OUTPUT/'\n",
    "\n",
    "#Hugging Face repo\n",
    "# repository_id = \"lamm-mit/GraphReasoning\"\n",
    "# graph_name='BioGraph.graphml'\n",
    "\n",
    "# make_dir_if_needed(data_dir)\n",
    "# make_dir_if_needed(data_dir_output)\n",
    "\n",
    "model=\"Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(f'/{model}', ) \n",
    "embedding_model = AutoModel.from_pretrained(f'{data_dir}/{model}', )\n",
    "\n",
    "# filename = f\"{data_dir}/{graph_name}\"\n",
    "# file_path = hf_hub_download(repo_id=repository_id, filename=filename,  local_dir='./')\n",
    "# print(f\"File downloaded at: {file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "graph_name = 'graph_30_augmented_graphML_integrated.graphml'\n",
    "graph_path = f'{data_dir_output}{graph_name}'\n",
    "\n",
    "G = nx.read_graphml(graph_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628503b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM']='true'\n",
    "\n",
    "embedding_file='TSMC_embeddings.pkl'\n",
    "generate_new_embeddings=True\n",
    "\n",
    "if os.path.exists(f'{data_dir}/{embedding_file}'):\n",
    "    generate_new_embeddings=False\n",
    "\n",
    "if generate_new_embeddings:\n",
    "    node_embeddings = generate_node_embeddings(G, embedding_tokenizer, embedding_model, )\n",
    "    save_embeddings(node_embeddings, f'{data_dir}/{embedding_file}')\n",
    "    \n",
    "else:\n",
    "    filename = f\"{data_dir}/{embedding_file}\"\n",
    "    # file_path = hf_hub_download(repo_id=repository_id, filename=filename, local_dir='./')\n",
    "    # print(f\"File downloaded at: {file_path}\")\n",
    "    node_embeddings = load_embeddings(f'{data_dir}/{embedding_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4f17f1",
   "metadata": {},
   "source": [
    "### Graph statistics and properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdbd16d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_embeddings_2d_pretty_and_sample(node_embeddings, n_clusters=10, n_samples=10, data_dir=data_dir_output, alpha=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a941c34c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "describe_communities_with_plots_complex(G, N=6, data_dir=data_dir_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d3fbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_statistics_and_plots_for_large_graphs(G, data_dir=data_dir_output,include_centrality=False,\n",
    "                                               make_graph_plot=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c9f0b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "is_scale_free (G, data_dir=data_dir_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d68aed",
   "metadata": {},
   "source": [
    "### Working with the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73906df7",
   "metadata": {},
   "source": [
    "#### Find best fitting node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f0410",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_fitting_node_list(\"semiconductor\", node_embeddings, embedding_tokenizer, embedding_model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9800874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_fitting_node_list(\"etching\", node_embeddings , embedding_tokenizer, embedding_model, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceed6c0",
   "metadata": {},
   "source": [
    "#### Find path in graph based on two keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a30c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "(best_node_1, best_similarity_1, best_node_2, best_similarity_2), path, path_graph, shortest_path_length, fname, graph_GraphML=find_path( G, node_embeddings,\n",
    "                                embedding_tokenizer, embedding_model , second_hop=False, data_dir=data_dir_output,\n",
    "                                  keyword_1 = \"semiconductor\", keyword_2 = \"graphene\",\n",
    "                                      similarity_fit_ID_node_1=0, similarity_fit_ID_node_2=0,\n",
    "                                       )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616b9aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9bd261",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list, path_string=print_path_with_edges_as_list(G , path)\n",
    "path_list,path_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee689d19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_paths_pretty([path_list], 'knowledge_graph_paths.svg', display_graph=True,data_dir=data_dir_output, scale=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd5503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets=find_all_triplets(path_graph) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe6171",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cd81e3-893e-4445-988f-1c85c9aca617",
   "metadata": {},
   "source": [
    "### Load LLM: clean Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c568f9e-dda9-40c4-831f-570a2c92b52d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filename = f\"{data_dir}/{graph_name}\"\n",
    "# file_path = hf_hub_download(repo_id=repository_id, filename=filename,  local_dir='./')\n",
    "# print(f\"File downloaded at: {file_path}\")\n",
    "\n",
    "# graph_name=f'{data_dir}/{graph_name}'\n",
    "# G = nx.read_graphml(graph_name)\n",
    "\n",
    "repository_id='MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF'\n",
    "filename='Mistral-7B-Instruct-v0.3.Q8_0.gguf'\n",
    "\n",
    "# repository_id='bartowski/Mistral-7B-Instruct-v0.3-GGUF'\n",
    "# filename='Mistral-7B-Instruct-v0.3-Q8_0.gguf'\n",
    "\n",
    "file_path = hf_hub_download(repo_id=repository_id, filename=filename,  local_dir='/home/mkychsu/pool/llm')\n",
    "# file_path = f'{model}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba60bad7-0afa-4b92-be0e-24cb77561ca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import llama_cpp\n",
    "\n",
    "llm = Llama(model_path=file_path,\n",
    "             n_gpu_layers=-1,verbose= True, #False,#False,\n",
    "             n_ctx=10000,\n",
    "             main_gpu=0,\n",
    "             # chat_format='mistral-instruct',\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b998174-b8ce-4474-8ef8-f38e82b9434c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_Mistral (system_prompt='You are a semiconductor engineer. Try to find the clear relationship in the provided information', \n",
    "                         prompt=\"How to make silicon into chip?\",temperature=0.333,\n",
    "                         max_tokens=10000, \n",
    "                         ):\n",
    "\n",
    "    if system_prompt==None:\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    else:\n",
    "        messages=[\n",
    "            {\"role\": \"system\",  \"content\": system_prompt, },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "\n",
    "    result=llm.create_chat_completion(\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "    return result['choices'][0]['message']['content']\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6314c8-5d24-41f9-9082-988cc630aa49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "verbatim=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd96ced-d7bd-428a-ac2d-c29e3f432679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = generate_Mistral(\n",
    "    system_prompt = 'You are given a set of information from a graph that describes the relationship, between materials and manufacturing process. You analyze these logically through reasoning.',\n",
    "    prompt='Develop a new research idea and very detail steps which improve the current manufacturing process of semiconductors.',\n",
    ")\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c89160-2644-4792-bc51-cc60c495c7de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response, (best_node_1, best_similarity_1, best_node_2, best_similarity_2), path, path_graph, shortest_path_length, fname, graph_GraphML = find_path_and_reason(\n",
    "    G, \n",
    "    node_embeddings,\n",
    "    embedding_tokenizer, \n",
    "    embedding_model, \n",
    "    generate_Mistral, \n",
    "    data_dir=data_dir_output,\n",
    "    verbatim=verbatim,\n",
    "    include_keywords_as_nodes=True,  # Include keywords in the graph analysis\n",
    "    keyword_1=\"semiconductor\",\n",
    "    keyword_2=\"manufacture\",\n",
    "    N_limit=9999,  # The limit for keywords, triplets, etc.\n",
    "    instruction='Develop a new research idea and very detail steps which improve the current manufacturing process of semiconductors.',\n",
    "    keywords_separator=', ',\n",
    "    graph_analysis_type='nodes and relations',\n",
    "    temperature=0.3, \n",
    "    inst_prepend='### ',  # Instruction prepend text\n",
    "    prepend='''You are given a set of information from a graph that describes the relationship \n",
    "               between materials and manufacturing process. You analyze these logically \n",
    "               through reasoning.\\n\\n''',  # Prepend text for analysis\n",
    "    visualize_paths_as_graph=True,  # Whether to visualize paths as a graph\n",
    "    display_graph=True,  # Whether to display the graph\n",
    ")\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a41f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "nx.draw(G)\n",
    "nt = Network('500px', '500px')\n",
    "nt.from_nx()\n",
    "nt.show('.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5374c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import KnowledgeGraphRAGRetriever\n",
    "\n",
    "graph_rag_retriever = KnowledgeGraphRAGRetriever(\n",
    "    storage_context=storage_context,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    graph_rag_retriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cec7d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"Tell me about Peter Quill?\",\n",
    ")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b74ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
