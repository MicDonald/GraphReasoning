{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a4c4a0",
   "metadata": {},
   "source": [
    "# GraphReasoning: Scientific Discovery through Knowledge Extraction and Multimodal Graph-based Representation and Reasoning\n",
    "\n",
    "Markus J. Buehler, MIT, 2024 mbuehler@MIT.EDU\n",
    "\n",
    "### Example: GraphReasoning: Loading graph and graph analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e744c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# device='cuda:0'\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Markdown\n",
    "from huggingface_hub import hf_hub_download\n",
    "from GraphReasoning import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee71b51-cfa6-4dca-aa55-de6b48f19c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbatim=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e892a7b-ec8c-4385-b429-d3c10c49ec52",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa0f7e-3597-4273-9a62-01b91fec153a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "doc_data_dir = '/home/mkychsu/pool/TSMC/GraphRAG/'\n",
    "# doc_list = []\n",
    "doc_list=[f'{doc_data_dir}dry-etching-technology-for-semiconductors_compress.txt',\n",
    "          f'{doc_data_dir}plasma-etching-an-introduction_compress.txt',\n",
    "          f'{doc_data_dir}handbook-of-silicon-wafer-cleaning-technology-third-edition_compress.txt',\n",
    "          f'{doc_data_dir}Ultraclean Surface Processing of Silicon Wafers - PDF Free Download.txt',\n",
    "          f'{doc_data_dir}Atomic Layer Processing_semiconductor.txt'   \n",
    "]\n",
    "\n",
    "doc_list_all=sorted(glob.glob(f'{doc_data_dir}/*.txt'))\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "for i, doc in enumerate(doc_list_all):\n",
    "    if doc in doc_list:\n",
    "        continue\n",
    "    try:\n",
    "        temp_doc = doc_list_all[i+1]\n",
    "        sim = similar(temp_doc.lower(), doc.lower())\n",
    "        if sim < 0.9:\n",
    "            doc_list.append(doc)\n",
    "        else:\n",
    "            if abs(os.stat(doc).st_size - os.stat(temp_doc).st_size)/os.stat(doc).st_size < 1e-3:\n",
    "                print(f'{i}:{sim},\\n {doc} \\n {temp_doc}')\n",
    "            else:\n",
    "                doc_list.append(doc)\n",
    "    except:\n",
    "        pass\n",
    "print(len(doc_list),doc_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5fe5fb-49b6-4ccd-a4bd-c78028252ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9780425d-3b58-4b4d-9175-5a0631b66117",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import glob\n",
    "\n",
    "# doc_data_dir = '/home/mkychsu/pool/TSMC/dataset/'\n",
    "# doc_list=[f'{doc_data_dir}dry-etching-technology-for-semiconductors_compress.pdf',\n",
    "#           f'{doc_data_dir}plasma-etching-an-introduction_compress.pdf',\n",
    "#           f'{doc_data_dir}handbook-of-silicon-wafer-cleaning-technology-third-edition_compress.pdf',\n",
    "#           f'{doc_data_dir}Ultraclean Surface Processing of Silicon Wafers - PDF Free Download.pdf',\n",
    "#           f'{doc_data_dir}Atomic Layer Processing_semiconductor.pdf'   \n",
    "# ]\n",
    "\n",
    "# doc_list_all=sorted(glob.glob(f'{doc_data_dir}*.pdf'))\n",
    "\n",
    "# from difflib import SequenceMatcher\n",
    "\n",
    "# def similar(a, b):\n",
    "#     return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "# for i, doc in enumerate(doc_list_all):\n",
    "#     if doc in doc_list:\n",
    "#         continue\n",
    "#     try:\n",
    "#         temp_doc = doc_list_all[i+1]\n",
    "#         sim = similar(temp_doc.lower(), doc.lower())\n",
    "#         if sim < 0.9:\n",
    "#             doc_list.append(doc)\n",
    "#         else:\n",
    "#             if abs(os.stat(doc).st_size - os.stat(temp_doc).st_size)/os.stat(doc).st_size < 1e-3:\n",
    "#                 print(f'{i}:{sim},\\n {doc} \\n {temp_doc}')\n",
    "#             else:\n",
    "#                 doc_list.append(doc)\n",
    "#     except:\n",
    "#         pass\n",
    "# print(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c542cb2b-55fb-4490-ac88-665b0a890e64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# file_to_check = doc_list[0].split('/')\n",
    "# file_to_check[-2] = 'dataset_textbook'\n",
    "# file_to_check[-1]=f'0.txt'\n",
    "# file_to_check='/'.join(file_to_check)\n",
    "# file_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac7bf8-8764-450f-ba0c-90132595ea1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if not os.path.exists(file_to_check):\n",
    "#     from langchain_community.document_loaders import PyPDFium2Loader as PDFLoader\n",
    "#     for i, doc in enumerate(doc_list[:5]):\n",
    "#         try:\n",
    "#             doc_pages = PDFLoader(doc).load_and_split()\n",
    "#             txt=''\n",
    "#             for page in doc_pages:\n",
    "#                 txt += page.page_content.replace('\\n', ' ')\n",
    "#             with open(f'/home/mkychsu/pool/TSMC/dataset_textbook/{i}.txt', 'w') as f:\n",
    "#                 f.write(f'{txt}')\n",
    "#                 f.close()\n",
    "#         except: # Exception as e:\n",
    "#             pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a54e02",
   "metadata": {},
   "source": [
    "### Load the LLM and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf447a3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Hugging Face repo\n",
    "# repository_id = \"lamm-mit/GraphReasoning\"\n",
    "data_dir='./GRAPHDATA_TSMC'    \n",
    "data_dir_output='./GRAPHDATA_TSMC_OUTPUT/'\n",
    "\n",
    "# data_dir_output='./GRAPHDATA_OUTPUT/'\n",
    "# graph_name='BioGraph.graphml'\n",
    "\n",
    "# make_dir_if_needed(data_dir)\n",
    "# make_dir_if_needed(data_dir_output)\n",
    "\n",
    "tokenizer_model=\"BAAI/bge-large-en-v1.5\"\n",
    "# tokenizer_model=\"f'/home/mkychsu/pool/llm/Mistral-7B-Instruct-v0.3/tokenizer.json\"\n",
    "\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(tokenizer_model, ) \n",
    "embedding_model = AutoModel.from_pretrained(tokenizer_model, )\n",
    "# embedding_model.to('cuda:0')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a886615-4c05-4be8-80df-fc8476e21c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filename = f\"{data_dir}/{graph_name}\"\n",
    "# file_path = hf_hub_download(repo_id=repository_id, filename=filename,  local_dir='./')\n",
    "# print(f\"File downloaded at: {file_path}\")\n",
    "\n",
    "# graph_name=f'{data_dir}/{graph_name}'\n",
    "# G = nx.read_graphml(graph_name)\n",
    "\n",
    "# repository_id='MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF'\n",
    "# filename='Mistral-7B-Instruct-v0.3Q8_0.gguf'\n",
    "\n",
    "repository_id='bartowski/Mistral-7B-Instruct-v0.3-GGUF'\n",
    "filename='Mistral-7B-Instruct-v0.3-Q8_0.gguf'\n",
    "\n",
    "file_path = hf_hub_download(repo_id=repository_id, filename=filename,  local_dir='/home/mkychsu/pool/llm')\n",
    "# file_path = f'{model}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393e1660",
   "metadata": {},
   "source": [
    "### Load LLM: clean Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc83101",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "# import llama_cpp\n",
    "\n",
    "llm = Llama(model_path=file_path,\n",
    "             n_gpu_layers=-1,verbose= True, #False,#False,\n",
    "             n_ctx=10000,\n",
    "             main_gpu=0,\n",
    "             # chat_format='mistral-instruct',\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76f24ed-e4c3-4226-8dff-b2568f6294bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm.verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5db975",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_Mistral (system_prompt='You are a semiconductor engineer. Try to find the clear relationship in the provided information', \n",
    "                         prompt=\"How to make silicon into chip?\",temperature=0.333,\n",
    "                         max_tokens=8192, \n",
    "                         ):\n",
    "\n",
    "    if system_prompt==None:\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    else:\n",
    "        messages=[\n",
    "            {\"role\": \"system\",  \"content\": system_prompt, },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "\n",
    "    result=llm.create_chat_completion(\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "    return result['choices'][0]['message']['content']\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677eb899",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# q='''Explain how semiconductor is made in a very professional way with as much detail as possible'''\n",
    "# start_time = time.time()\n",
    "# res=generate_Mistral( system_prompt='You are an expert in semiconductor fields. Try to find the clear relation in the provided information. Skip the authorship information if it is not relevant', \n",
    "#          prompt=q, max_tokens=1024, temperature=0.3,  )\n",
    "\n",
    "# print (res)\n",
    "# deltat=time.time() - start_time\n",
    "# print(\"--- %s seconds ---\" % deltat)\n",
    "# display (Markdown(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e673800-9a97-49e1-aef0-73ad55ff9dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# graph_HTML, graph_GraphML, G, net, output_pdf = make_graph_from_text(res, generate_Mistral,\n",
    "#                                                                      chunk_size=1000,chunk_overlap=200,\n",
    "#                                                                      do_distill=True, data_dir='temp', verbatim=True,\n",
    "#                                                                      repeat_refine=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aae8bf5-a710-4c43-9e53-a1fa818de30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM']='true'\n",
    "\n",
    "embedding_file='TSMC_KG_mistral_instruct_v0.3.pkl'\n",
    "generate_new_embeddings=True\n",
    "\n",
    "if os.path.exists(f'{data_dir}/{embedding_file}'):\n",
    "    generate_new_embeddings=False\n",
    "\n",
    "if generate_new_embeddings:\n",
    "    node_embeddings = generate_node_embeddings(G, embedding_tokenizer, embedding_model, )\n",
    "    save_embeddings(node_embeddings, f'{data_dir}/{embedding_file}')\n",
    "    \n",
    "else:\n",
    "    filename = f\"{data_dir}/{embedding_file}\"\n",
    "    # file_path = hf_hub_download(repo_id=repository_id, filename=filename, local_dir='./')\n",
    "    # print(f\"File downloaded at: {file_path}\")\n",
    "    node_embeddings = load_embeddings(f'{data_dir}/{embedding_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c1e53-9ed2-4816-9371-24f442ba4421",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, doc in enumerate(doc_list):\n",
    "    \n",
    "    graph_root=f'graph_{i}'\n",
    "    _graph_GraphML= f'{data_dir_output}/{graph_root}_augmented_graphML_integrated.graphml'\n",
    "    txt=''\n",
    "\n",
    "    if os.path.exists(_graph_GraphML):\n",
    "        G = nx.read_graphml(_graph_GraphML)\n",
    "        print(f'Main KG loaded: {_graph_GraphML}, {G}')\n",
    "        continue\n",
    "        \n",
    "    elif os.path.exists(f'{i}_err.txt'):\n",
    "        print(f'No. {i}: {doc} got something wrong.')\n",
    "        continue\n",
    "\n",
    "    elif os.path.exists(f'{data_dir}/graph_{i}_graph.graphml'):\n",
    "        print(f'Found a graph fragment to merge: {i}: {doc}.')\n",
    "        graph_GraphML = f'{data_dir}/graph_{i}_graph.graphml'\n",
    "    \n",
    "    else:\n",
    "        print(f'Generating a knowledge graph from {doc}')\n",
    "        with open(doc, \"r\") as f:\n",
    "            txt = \" \".join(f.read().splitlines())  # separate lines with a single space\n",
    "\n",
    "        try:\n",
    "            _, graph_GraphML, _, _, _ = make_graph_from_text(txt,generate_Mistral,\n",
    "                                  include_contextual_proximity=False,\n",
    "                                  graph_root=graph_root,\n",
    "                                  chunk_size=1000,chunk_overlap=200,\n",
    "                                  repeat_refine=0,verbatim=False,\n",
    "                                  data_dir=data_dir,\n",
    "                                  save_PDF=False,#TO DO\n",
    "                                 )\n",
    "        except Exception as e:\n",
    "            print(f'Something is wrong with No. {i}: {doc}.')\n",
    "            f = open(f'{i}_err.txt', 'w')\n",
    "            f.write(f'{e}\\n{doc}\\n{txt}')\n",
    "            f.close()          \n",
    "            continue\n",
    "        print(f'Merging graph No. {i}: {doc} to the main one')\n",
    "    \n",
    "    try:\n",
    "        _, G, _, node_embeddings, res = add_new_subgraph_from_text(txt, generate_Mistral,\n",
    "                           node_embeddings, embedding_tokenizer, embedding_model,\n",
    "                           original_graph=G, data_dir_output=data_dir_output, graph_root=graph_root,\n",
    "                           chunk_size=1000,chunk_overlap=200,\n",
    "                           do_simplify_graph=True,size_threshold=10,\n",
    "                           repeat_refine=0,similarity_threshold=0.95,\n",
    "                           do_Louvain_on_new_graph=True, include_contextual_proximity=False,\n",
    "                           #whether or not to simplify, uses similiraty_threshold defined above\n",
    "                           return_only_giant_component=False,\n",
    "                           save_common_graph=False,G_to_add=None,graph_GraphML_to_add=graph_GraphML,\n",
    "                           verbatim=True,)\n",
    "\n",
    "        save_embeddings(node_embeddings, f'{data_dir}/{embedding_file}')\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6263631d-e3ac-4def-9bec-2b46b5bf27a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "G = nx.read_graphml(f'{data_dir}/graph_0_graph.graphml')\n",
    "print(f'KG loaded: {G}')\n",
    "# node_embeddings = generate_node_embeddings(G, embedding_tokenizer, embedding_model, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff08c7c6-7b29-4248-9443-7c3e3a6fc29d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def split_documents_into_chunks(documents, chunk_size=600, overlap_size=100):\n",
    "#     chunks = []\n",
    "#     for document in documents:\n",
    "#         for i in range(0, len(document), chunk_size - overlap_size):\n",
    "#             chunk = document[i:i + chunk_size]\n",
    "#             chunks.append(chunk)\n",
    "#     return chunks\n",
    "\n",
    "# def extract_elements_from_chunks(chunks):\n",
    "#     elements = []\n",
    "#     for index, chunk in enumerate(chunks):\n",
    "#         response = client.chat.completions.create(\n",
    "#             model=\"gpt-4\",\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": \"Extract entities and relationships from the following text.\"},\n",
    "#                 {\"role\": \"user\", \"content\": chunk}\n",
    "#             ]\n",
    "#         )\n",
    "#         entities_and_relations = response.choices[0].message.content\n",
    "#         elements.append(entities_and_relations)\n",
    "#     return elements\n",
    "\n",
    "# def summarize_elements(elements):\n",
    "#     summaries = []\n",
    "#     for index, element in enumerate(elements):\n",
    "#         response = client.chat.completions.create(\n",
    "#             model=\"gpt-4\",\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": \"Summarize the following entities and relationships in a structured format. Use \\\"->\\\" to represent relationships, after the \\\"Relationships:\\\" word.\"},\n",
    "#                 {\"role\": \"user\", \"content\": element}\n",
    "#             ]\n",
    "#         )\n",
    "#         summary = response.choices[0].message.content\n",
    "#         summaries.append(summary)\n",
    "#     return summaries\n",
    "\n",
    "# def build_graph_from_summaries(summaries):\n",
    "#     G = nx.Graph()\n",
    "#     for summary in summaries:\n",
    "#         lines = summary.split(\"\\n\")\n",
    "#         entities_section = False\n",
    "#         relationships_section = False\n",
    "#         entities = []\n",
    "#         for line in lines:\n",
    "#             if line.startswith(\"### Entities:\") or line.startswith(\"**Entities:**\"):\n",
    "#                 entities_section = True\n",
    "#                 relationships_section = False\n",
    "#                 continue\n",
    "#             elif line.startswith(\"### Relationships:\") or line.startswith(\"**Relationships:**\"):\n",
    "#                 entities_section = False\n",
    "#                 relationships_section = True\n",
    "#                 continue\n",
    "#             if entities_section and line.strip():\n",
    "#                 entity = line.split(\".\", 1)[1].strip() if line[0].isdigit() and line[1] == \".\" else line.strip()\n",
    "#                 entity = entity.replace(\"**\", \"\")\n",
    "#                 entities.append(entity)\n",
    "#                 G.add_node(entity)\n",
    "#             elif relationships_section and line.strip():\n",
    "#                 parts = line.split(\"->\")\n",
    "#                 if len(parts) >= 2:\n",
    "#                     source = parts[0].strip()\n",
    "#                     target = parts[-1].strip()\n",
    "#                     relation = \" -> \".join(parts[1:-1]).strip()\n",
    "#                     G.add_edge(source, target, label=relation)\n",
    "#     return G\n",
    "\n",
    "def detect_communities(graph):\n",
    "    # communities = []\n",
    "#     for component in nx.weakly_connected_components(graph):\n",
    "#         subgraph = graph.subgraph(component)\n",
    "#         if len(subgraph.nodes) > 1:\n",
    "#             try:\n",
    "#                 # sub_communities = algorithms.leiden(subgraph)\n",
    "#                 sub_communities = nx.community.girvan_newman(subgraph)\n",
    "                \n",
    "#                 # for community in sub_communities.communities:\n",
    "#                 for community in tqdm(sub_communities):\n",
    "#                     communities.append(list(community))\n",
    "       \n",
    "#                 communities = sorted(map(sorted, next_level_communities))\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing community: {e}\")\n",
    "#         else:\n",
    "#             communities.append(list(subgraph.nodes))\n",
    "\n",
    "    communities_generator = nx.community.girvan_newman(G)\n",
    "    top_level_communities = next(communities_generator)\n",
    "    next_level_communities = next(communities_generator)\n",
    "    communities = sorted(map(sorted, next_level_communities))\n",
    "    return communities\n",
    "\n",
    "def summarize_communities(communities, graph, generate):\n",
    "    community_summaries = []\n",
    "    for index, community in tqdm(enumerate(communities)):\n",
    "        subgraph = graph.subgraph(community)\n",
    "        nodes = list(subgraph.nodes)\n",
    "        edges = list(subgraph.edges(data=True))\n",
    "        description = \"Entities: \" + \", \".join(nodes) + \"\\nRelationships: \"\n",
    "        relationships = []\n",
    "        for edge in edges:\n",
    "            relationships.append(\n",
    "                f\"{edge[0]} -> {edge[2]['title']} -> {edge[1]}\")\n",
    "        description += \", \".join(relationships)\n",
    "        try:\n",
    "            response = generate(system_prompt= \"Summarize the following community of entities and relationships.\",\n",
    "                                       prompt= description)\n",
    "        # response = client.chat.completions.create(\n",
    "        #     model=\"gpt-4\",\n",
    "        #     messages=[\n",
    "        #         {\"role\": \"system\", \"content\": \"Summarize the following community of entities and relationships.\"},\n",
    "        #         {\"role\": \"user\", \"content\": description}\n",
    "        #     ]\n",
    "        # )\n",
    "        # summary = response.choices[0].message.content.strip()\n",
    "        except:\n",
    "            print(description)\n",
    "        summary = response.strip()\n",
    "        community_summaries.append(summary)\n",
    "    return community_summaries\n",
    "\n",
    "def generate_answers_from_communities(community_summaries, generate, query):\n",
    "    intermediate_answers = []\n",
    "    for summary in tqdm(community_summaries):\n",
    "        try:\n",
    "            response = generate(system_prompt= \"Answer the following query based on the provided summary.\",\n",
    "                                       prompt=f\"Query: {query} Summary: {summary}\")\n",
    "            # response = client.chat.completions.create(\n",
    "            #     model=\"gpt-4\",\n",
    "            #     messages=[\n",
    "            #         {\"role\": \"system\", \"content\": \"Answer the following query based on the provided summary.\"},\n",
    "            #         {\"role\": \"user\", \"content\": f\"Query: {query} Summary: {summary}\"}\n",
    "            #     ]\n",
    "            # )\n",
    "            intermediate_answers.append(response)\n",
    "        except:\n",
    "            print(f'TL;DR: {summary[0:100]}...{summary[-100:]}')\n",
    "            return 0\n",
    "    final_response = generate(system_prompt= \"Combine these answers into a final, concise response.\",\n",
    "                                prompt=f\"Intermediate answers: {' '.join(intermediate_answers)}\")\n",
    "\n",
    "    # final_response = client.chat.completions.create(\n",
    "    #     model=\"gpt-4\",\n",
    "    #     messages=[\n",
    "    #         {\"role\": \"system\", \"content\": \"Combine these answers into a final, concise response.\"},\n",
    "    #         {\"role\": \"user\", \"content\": }\n",
    "    #     ]\n",
    "    # )\n",
    "    # final_answer = final_response.choices[0].message.content\n",
    "    return final_response\n",
    "\n",
    "# def graph_rag_pipeline(documents, query, chunk_size=600, overlap_size=100):\n",
    "def graph_rag_pipeline(graph, generate, query):\n",
    "    # chunks = split_documents_into_chunks(documents, chunk_size, overlap_size)\n",
    "    # elements = extract_elements_from_chunks(chunks)\n",
    "    # summaries = summarize_elements(elements)\n",
    "    # graph = build_graph_from_summaries(summaries)\n",
    "    \n",
    "    communities = detect_communities(graph)\n",
    "    if verbatim:\n",
    "        print(\"Number of Communities = \", len(communities))\n",
    "    community_summaries = summarize_communities(communities, graph, generate)\n",
    "    final_answer = generate_answers_from_communities(community_summaries, generate, query)\n",
    "    return final_answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1cb9e9-af69-4ba6-a987-25c05ad886f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph=G\n",
    "generate = generate_Mistral\n",
    "communities = detect_communities(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63873d32-20d6-4671-9bb0-c456ad56df90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "community_summaries = summarize_communities(communities, graph, generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2ba938-cee4-415e-947d-e0565f086aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "community_summaries_sorted = sorted(community_summaries, key=lambda x: -len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef69e070-7984-4da7-8f14-229cf0343544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What are the main techniques to make semiconductors?\"\n",
    "\n",
    "last_response=''\n",
    "for i, summary in tqdm(enumerate(community_summaries_sorted)):\n",
    "    response = generate(system_prompt= \"Answer the query detailedly based on the collected information and the combined with the last thought you have. \",\n",
    "                               prompt=f\"Query: {query} Collected information: {summary} You last thought: {last_response}\")\n",
    "    last_response=response\n",
    "    print(last_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a84f00a-e452-4082-b23f-f7f0cf60a419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "last_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eca421-f4da-41a2-9303-74dc57e290ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_response = generate(system_prompt= \"Combine these answers into a final, concise response.\",\n",
    "                            prompt=f\" answers: {last_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef2746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response, (best_node_1, best_similarity_1, best_node_2, best_similarity_2), path, path_graph, shortest_path_length, fname, graph_GraphML = find_path_and_reason(\n",
    "#     G, \n",
    "#     node_embeddings,\n",
    "#     embedding_tokenizer, \n",
    "#     embedding_model, \n",
    "#     generate_Mistral, \n",
    "#     data_dir=data_dir_output,\n",
    "#     verbatim=verbatim,\n",
    "#     include_keywords_as_nodes=True,  # Include keywords in the graph analysis\n",
    "#     keyword_1=\"semiconductors\",\n",
    "#     keyword_2=\"etching\",\n",
    "#     N_limit=9999,  # The limit for keywords, triplets, etc.\n",
    "#     instruction='What is the best etching method to manufacture semiconductors.',\n",
    "#     keywords_separator=', ',\n",
    "#     graph_analysis_type='nodes and relations',\n",
    "#     temperature=0.3, \n",
    "#     inst_prepend='### ',  # Instruction prepend text\n",
    "#     prepend='''You are given a set of information from a graph that describes the relationship \n",
    "#                between materials and manufacturing process. You analyze these logically \n",
    "#                through reasoning.\\n\\n''',  # Prepend text for analysis\n",
    "#     visualize_paths_as_graph=True,  # Whether to visualize paths as a graph\n",
    "#     display_graph=True,  # Whether to display the graph\n",
    "# )\n",
    "# display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad6679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response, (best_node_1, best_similarity_1, best_node_2, best_similarity_2), path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d434a729-4756-47ca-988c-c2e0e6ec7c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings_2d_pretty_and_sample(node_embeddings, n_clusters=10, n_samples=10, data_dir=data_dir_output, alpha=.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0111b640-7a92-4357-ab62-32621999f4ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# describe_communities_with_plots_complex(G, N=6, data_dir=data_dir_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3279fbe-e558-445f-9829-c7912fdff04d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# graph_statistics_and_plots_for_large_graphs(G, data_dir=data_dir_output,include_centrality=False,\n",
    "                                               # make_graph_plot=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f82e50-44bb-4b34-8953-91e2b287f68d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "is_scale_free (G, data_dir=data_dir_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd8547-9599-41c1-a1df-e011c8090307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_best_fitting_node_list(\"semiconductor\", node_embeddings, embedding_tokenizer, embedding_model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3726c6-defc-426d-8d10-a5ac7b77c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_best_fitting_node_list(\"better manufactoring process for semiconductor\", node_embeddings , embedding_tokenizer, embedding_model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f53de6b-0929-47a9-a9f3-9fa0b6bb77d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ee778-2838-4045-b0b6-0c2f09832947",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (best_node_1, best_similarity_1, best_node_2, best_similarity_2), path, path_graph, shortest_path_length, fname, graph_GraphML=find_path( G, node_embeddings,\n",
    "                                # embedding_tokenizer, embedding_model , second_hop=False, data_dir=data_dir_output,\n",
    "                                #   keyword_1 = \"new materials\", keyword_2 = \"semiconductor\",\n",
    "                                #       similarity_fit_ID_node_1=0, similarity_fit_ID_node_2=0,\n",
    "                                #        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ee0bb-1925-42b2-819c-90ca745ba47e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf71de-038b-4b66-ac58-232c4fc4476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5273351c-a0c6-4b31-bec5-658a84b27bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_list, path_string=print_path_with_edges_as_list(G , path)\n",
    "# path_list,path_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d8867-2749-4f7b-993d-dc3287db8132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_paths_pretty([path_list], 'knowledge_graph_paths.svg', display_graph=True,data_dir=data_dir_output, scale=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14f83f5-b43d-45ce-8b0c-57a57a1a937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplets=find_all_triplets(path_graph) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d054dfc-a0d6-4a12-9905-ab264c8d9565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e52df-60c8-4d9a-87ef-1f566bf88f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d07c7-250e-4a6b-9ee9-b5e327d7942a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
