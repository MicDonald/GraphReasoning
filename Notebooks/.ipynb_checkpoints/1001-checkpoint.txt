/home/mkychsu/pool/TSMC/dataset/(The Springer International Series in Engineering and Computer Science 447) Richard F. Lyon (auth.), Tor Sverre Lande (eds.) - Neuromorphic Systems Engineering_ Neural Networks in Silicon-Springer US .pdf
neuromorphic systems  engineering  neural networks in siliconthe kluwer international series  in engineering and computer science  analog circuits and signal processing  consulting editor: mohammed ismail. ohio state university  related titles."  design of modulators for oversampled converters, feng wang, ramesh harjani,  isbn: 0-7923-8063-0  symbolic analysis in analog integrated circuit design, henrik floberg, isbn:  0-7923-9969-2  switched-current design and implementation of oversampling a/d  converters, nianxiong tan, isbn: 0-7923-9963-3  cmos wireless transceiver design, jan crols, michiel steyaert, isbn: 0-7923-9960-9  design of low-voltage, low-power operational amplifier cells, ron  hogervorst, johan h. huijsing, isbn: 0-7923-9781-9  vlsi-compatible implementations for artificial neural networks, sied  mehdi fakhraie, kenneth carless smith, isbn: 0-7923-9825-4  characterization methods for submicron mosfets, edited by hisham haddara,  isbn: 0-7923-9695-2  low-voltage low-power analog integrated circuits, edited by wouter serdijn,  isbn: 0-7923-9608-1  integrated video-frequency continuous-time filters: high-performance  realizations in bicmos, scott d. willingham, ken martin, isbn: 0-7923-9595-6  feed-forward neural networks: vector decomposition analysis, modelling and analog  implementation, anne-johan annema, isbn: 0-7923-9567-0  frequency compensation techniques low-power operational  amplifiers, ruud easchauzier, johan hu~jsmg, isbn: 0-7923-9565-4  analog signal generation for bist of mixed-signal integrated circuits,  gordon w. roberts, albert k. lu, isbn: 0-7923-9564-6  integrated fiber-optic receivers, aaron buchwald, kenneth w. martin, isbn: 0-7923-  9549-2  modeling with an analog hardware description language, h. alan  mantooth, mike fiegenbaum, isbn: 0-7923-9516-6  low-voltage cmos operational amplifiers: theory, design and implementation,  satoshi sakurai, mohammed lsmail, isbn: 0-7923-9507-7  analysis and synthesis of mos translinear circuits, remco d.. wiegerink, isbn: 0-  7923-9390-2  computer-aided design of analog circuits and systems, l. richard carley, konald  s. gyurcsik, isbn: 0-7923-9351-1  high-performance cmos continuous-time filters, ,]os6 silva-martlnez, michiel  steyaert, willy sansen, isbn: 0-7923-9339-2  symbolic analysis of analog circuits: techniques and applications, lawrence p.  huelsman, georges g. e. gielen, isbn: 0-7923-9324-4  design of low-voltage bipolar operational amplifiers, m. jeroen fonderie, johan  h. huijsing, isbn: 0-7923-9317-1  statistical modeling for computer-aided design of mos vlsi circuits,  christopher michael, mohammed lsmail, isbn: 0-7923-9299-x  selective linear-phase switched-capacitor and digital filters, hussein baher,  isbn: 0-7923-9298-1  analog cmos filters for very high frequencies, bram nauta, isbn: 0-7923-9272-8  analog vlsi neural networks, yoshiyasu takefuji, isbn: 0-7923-9273-6neuromorphic systems  engineering  neural networks in silicon  edited by  tor sverre lande  university of oslo  norway  1~8  kluwer academic publishers  boston / dordrecht / londondistributors for north, central and south america:  kluwer academic publishers  101 philip drive  assinippi park  norwell, massachusetts 02061 usa  distributors for all other countries:  kluwer academic publishers group  distribution centre  post office box 322  3300 ah dordrecht, the netherlands  library of congress cataloging-in-publication data  a c.i.p. catalogue record for this book is available  from the library of congress.  copyright Â© 1998 by kluwer academic publishers  all rights reserved. no part of this publication may be reproduced, stored in a  retrieval system or transmitted in any form or by any means, mechanical, photo-  copying, recording, or otherwise, without the prior written permission of the  publisher, kluwer academic publishers, 101 philip drive, assinippi park, norwell,  massachusetts 02061  printed on acid-free paper.  printed in the united states of americacontents  foreword ix  preface xiii  acknowledgements  part i cochlear systems  1  filter cascades as analogs of the cochlea  richard f. lyon  2  an analogue vlsi model of active cochlea  eric fragni~re, andrd van schaik and eric a. vittoz  3  a kow-power wide-dynamic-range analog vks[ cochlea  rahul sarpeshkar, richard f. lyon, carver mead  4  speech recognition fxperiments with silicon audito~ models  john lazzaro and john wawrzynek xvii  19  49  105vi neuromorphic systems engineering  part ii retinomorphic systems  5  the retinomorphic approach: pixei-parallel  adaptive amplification, filtering, and ouantization  kwabena a. boahen  6  analog vlsi excitatory feedback circuits for  attentional shifts and tracking  t.g. morris and s.p. deweerth  7  floating-gate circuits for adaptation of  saccadic eye movement accuracy  timothy k. horiuchi and christo] koch 129  151  175  part iii neuromorphic communication  8  introduction to neuromorphic communication  tot sverre lande  9  a pulsed communication/computation framework for  analog vlsi perceptive systems  alessandro mortara  10  asynchronous communication of 2d motion information  using winner-takes-all arbitration  zaven kalayjian and andreas g. andreou  11  communicating neuronal ensembles between  neuromorphic chips  kwabena a. boahen 193  201  217  229contents vii  part iv neuromorphic technology  12  introduction: from neurobiology to silicon  chris diorio  13  a low-power wide-linear-range transconductance amplifier  rahul sarpeshkar, richard f. lyon, carver mead  14  floating-gate mos synapse transistors  chris diorio, paul hasler, bradley a. minch, and carver mead  15  neuromorphic synapses for artificial dendrites  wayne c. westerman, david p. m. northmore, and john. g. elias  16  winner-take-all networks with lateral excitation  giecomo indiveri 263  267  315  339  367  part v neuromorphic learning  17  neuromorphic learning vlsi systems: a survey  gert cauwenberghs  18  analog vlsi stochastic perturbative learning architectures  ger~ cauwenberghs  19  winner-takes-all associative memory  philippe o. pouliquen, andreas g. andreou and kim strohbehn 381  409  437  index 457foreword  today's silicon technology can provide about one billion transistors on a single  chip, and new process developments will no doubt offer ten times ~s much in  the early 2000s. this enormous hardware capability offers the potential for  implementing very complex functions on single chips and for combining these  chips into highly intelligent systems.  however, the rate of progression of design techniques -- and of the associated  design tools -- needed to build such complex systems-on-chips is lagging in-  creasingly, and threatens to become a real bottleneck in the exploitation of this  fantastic technology. power dissipation becomes a limiting problem, not only  because of the increasing difficulty to evacuate the heat created on-chip, but  also because of the growing interest for very small, battery-operated, portable  systems. how can the creativity of designers be dramatically boosted to pul-  verize these obstacles?  one possible answer is to look into what life has "invented" along half a bil-  lion years of evolution. not in order to copy or directly exploit living organisms  -- this is the realm of biotechnology -- but to get inspiration from what they  use as principles, mechanisms or architectures, or even of the very functions  they carry out.  since the enormous potential of microelectronics is essentially for processing  signals, it is the central nervous system of living creatures which is of most im-  mediate interest, hence the term "neuromorphic engineering" coined by carver  mead, one of the early promoters of this fascinating approach. some may prefer  to speak of "biology-inspired" microelectronics to stress the fact that the struc-  ture of the electronic solution does not necessarily match that of its biological  source.  numerous principles found in the brain can provide inspiration to circuit  and system designers. most obvious is the huge amount of repetitive cells  operating in a massively parallel manner. this may help to solve the problem  of hierarchically designing very complex systems. applied to digital systems, it  also enables the speed of each individual cell to be reduced, in order to obtain  a proportional reduction of power consumption by reducing the supply voltage.x neuromorphic systems engineering  as a matter of fact, the brain is able to solve, in real time, very complex  tasks on massively parallel data, by means of neurons operating at a very  low local speed (of the order of one kilohertz), with the poor local precision of  analog processing. power consumption is very low since the whole human brain  dissipates no more than a few tens of watts. furthermore, the cells operate in a  collective manner: they solve a problem by elaborating a consensus on the best  solution, instead of each separately taking care of a small part of the problem.  thus collective computation in massively parallel analog vlsi circuits may  reasonably be expected to be the best engineering approach to solve perceptive  tasks such as vision or audition.  throughout the evolutionary process, life has opportunistically exploited  and retained all favorable features of new structures. this opportunism can  also inspire engineers to exploit all the inherent properties of the available  technology. this bottom-up approach to optimum solutions is ideally suited  for analog circuits: all the properties of transistors and of elementary circuits  are analyzed and characterized for later opportunistic use. along the same  idea, a search is made for hidden advantages of apparent defects or parasitic  effects. in many parts of the brain, the information coming from the various  sensors is coded and processed by means of maps of activity. the limited range  of operation of each cell in a map is locally adapted to the level of the signals,  with the result of a very wide dynamic range of sensitivity. such schemes are  very useful to design very low power circuits based on noisy and inaccurate  analog cells. learning by examples can be considered an extended form of  adaptation for solving particular problems. it is a very attractive alternative  to address the problem of programming complex machines for very complex  tasks.  another salient feature of nervous systems, the ubiquitous use of the fre-  quency or phase of pulse trains to represent analog signals, turns out to be  a very efficient way of realizing the dense communication network required  by collective computation. furthermore, such a representation facilitates the  implementation of time-domain processing by means of simple hardware oper-  ators.  the state of the art of neuromorphic systems engineering is excellently doc-  umented by the selection of contributions assembled in this edited book by tor  sverre lande. various aspects of the biology-inspired approach are described  by most of the prominent actors of this fascinating adventure. although most  of the work described is still very exploratory and fragmentary, it shows very  attractive results which definitely support the validity of such an innovative  engineering approach. some results, in particular in the domain of early vision,  are ready for practical applications. others help understand the biological  processes and are just the very first steps towards building fully operational  perception machines.  after the excitation created by the immediate rewards of a new discipline,  neuromorphic engineering is now in a phase of consolidation, by a rather limited  number of research groups. a few original products based on it have alreadyforeword xi  reached the market, but the functions they implement are very far below what  can be expected in the future. this book is intended to attract a broader  interest for this promising discipline and to advocate a more massive research  effort therein. this effort will at least turn out to be a strong action of lateral  thinking, resulting in more efficient solutions to standard problems. at most,  it might well bring a revolution in the way very high-density processes will  be used in the future to build sophisticated systems for advanced perceptive  processing.  prof. eric a. vittoz  csem, neuch~telpreface  the name of this book, neuromorphic systems engineering, emphasizes three  important aspects of an exciting new research field. the term neuromorphic  expresses relations to computational models found in biological neural systems,  which are used as inspiration for building large electronic systems in silicon.  by adequate engineering, these silicon systems are made useful to mankind.  while trying to catch the computational paradigms of biology, the strong  connection between the representation of the neural state and the neural com-  putation is unveiled. although there are quantized states in biology, the domi-  nating state variable is continuous, making our nervous system predominantly  an analog computational system with computations taking place in real and  continuous time. in contrast to our completely quantized and error free digital  systems, biology seems to carry out computational tasks in a fundamentally  different way. based on badly conditioned data, complex matching operations,  like recognizing faces, are carried out in real time, outperforming any known  digital system. knowing that these complex operations are carried out with  extremely limited computational elements with slow and noisy interconnections  makes the computations of nature even more intriguing.  another fascinating and challenging aspect of neuromorphic engineering is  the interdisciplinary nature. the rapidly growing research field of neuroscience  is combined with the rapidly growing complexity of microelectronics. the  progress in neuroscience has uncovered a fundamental new understanding of  the computational aspects of our nervous system. new measurement tech-  niques have given us detailed knowledge of the basic mechanisms of neural  computations. an increasing flow of improved biological models based on solid  physical evidence is a valuable source of inspiration. in particular models of  our early vision system and the primary auditory system have successfully been  implemented in silicon, as reported in parts i and ii of this book.  the achievements of microelectronics are evident to everybody, as we are all  affected in our everyday life. the famous moore's law of doubled performance  every second year seems to be unbreakable. the success of digital microelec-  tronic systems, utilizing millions of transistors, is indisputable, while the oldxiv neuromorphic systems engineering  art of analog circuit design is still stuck with less than a hundred transistors.  armed with the increased knowledge of our mainly analog, nervous system, we  may be able to utilize the available silicon in a novel way by building large-  scale analog systems. in doing so we may even surpass some of the practical  limitations of digital microelectronics. several systems in this book use weakly  inverted (subthreshold) transistors, reducing power consumption to a fraction  of a similar digital system. in neuromorphic systems, we also try to inherit the  defect tolerance of biology.  the intention of this book is to provide a snapshot of neuromorphic engi-  neering today. most of the material is taken from a special issue of the interna-  tional kluwer journal analog integrated circuits and signal processing, vol. 13,  may/june, 1997. significant new material is added, reflecting state-of-the-art  developments within neuromorphic engineering.  the book is organized into five parts with a total of nineteen chapters viewing  the field of neuromorphic engineering from different perspectives.  part i, "cochlear systems" starts with the chapter "filter cascades as  analogs of the cochlea" by dick lyon. lyon together with carver mead pio-  neered cochlea modeling in silicon by implementing a working basilar membrane  equivalent with a couple of hundred second-order filters cascaded. in spite of  all predictions, the implemented micropower system worked surprisingly well,  showing one of the first neuromorphic analog systems in silicon. the power of  pseudoresonance in cascaded filters is analyzed, showing a system resonance su-  perseding the resonance of each individual filter stage. the second chapter, "an  analogue vlsi model of active cochlea" attacks one of the hardest modeling  issues of the inner ear, namely automatic gain control (agc). the mechani-  cal feedback through the outer haircells of the basilar membrane increases the  tuning sharpness giving a unique system performance with only weakly reso-  nant filters. the last author, eric vittoz, is known for his pioneering work  on micropower microelectronics, the technological foundation of most neuro-  morphic silicon. in chapter three sarpeshkar et. al. present a state-of-the-art  implementation of lyon's basilar membrane model in silicon with high dynamic  range combined with low power consumption. the feasibility of pseudoreso-  nance is demonstrated and well documented. the next chapter on "speech  recognition experiments with silicon auditory models" explores the feasibil-  ity of speech recognition using three simultaneously running analog cochleae  with neural spike encoded data processing. an asynchronous communication  system is developed interfacing the chips to a computer. a classification sys-  tern for human speech is developed and evaluated. all together this system is  a strong indicator of the possibilities of neuromorphic engineering.  part ii is devoted to selected topics on retinomorphic or vision systems in sill-  con. several existing implementations of silicon retina are available in the litera-  ture, but improvements may be done. in the fifth chapter, "the retinomorphic  approach: pixel-parallel adaptive amplification, filtering and quantization"  by 'buster' boahen elegantly shows how simple vital retina functions may be  implemented using compact, current-mode circuits. the two-dimensional na-preface xv  ture of the retina rewards this simplicity by increased resolution. the sixth  chapter by morris et. al., "analog vlsi excitatory feedback circuits for at-  tentional shifts and tracking" approaches another essential property of vision,  namely tracking of moving objects. by extending the classical winner-take-m1  circuit, robust tracking systems are presented with experimental results. in  chapter seven, "floating-gate circuits for adaptation of saccadic eye move-  ment accuracy" by horiuchi et. al., the floating gate of mos-transistors is  used to permanently store the results of the adaptive action taking place in our  visual system. the feasibility of using analog storage in an adaptive control  system is discussed and experimentally demonstrated.  part iii consists of four chapters in the field of "neuromorphic commu-  nication" which is characterized by time-multiplexing of asynchronous neural  spikes (events) over a digital bus. chapter eight is a short introduction where  different approaches are discussed. the concept of weak arbitration is intro-  duced and measured against other approaches. the next chapter by mot-  tara, "a pulsed communication/computation framework for analog vlsi  perceptive systems" shows a simple solution allowing collisions of simultane-  ous events. the very essential inter-spike timing information is maintained,  but some noise is added to the signal transmitted, due to collisions. chapter  ten by kalayjian et. al. "asynchronous communication of 2d motion in-  formation using winner-take-all arbitration" proposes an arbitration scheme  virtually eliminating collisions. in chapter eleven, "communicating neuronal  ensembles between neuromorphic chips," boahen explains different tradeoffs  when implementing a full interchip communication system. a full arbitration  scheme is used and compared to traditional scanning techniques. the errors  introduced by multiplexing are characterized, documenting both the potential  and the limitations of a neuromorphic communication channel.  part iv is devoted to "neuromorphic technology." the chapter called "in-  troduction: from neurobiology to silicon" by diorio explains some of the ra-  tionales for exploring the physics of microelectronics in modeling biology. in  chapter thirteen "a low-power wide-linear-range transconductance amplifier"  by sarpeshkar et. al., the back-gate of the mos-transistors is used as input to  a differential pair. degeneration of both source and gate is applied, extending  the linear range an order of magnitude. a transconductor with 65db snr is  implemented in weak inversion and demonstrated for use in a cochlea implemen-  tation. in the next chapter called "floating-gate mos synapse transistors"  by diorio et. al., the floating-gate of a mos-transistor is utilized to program  the strength of a synaptic connection (or weight). the striking simplicity and  innovative usage of standard cmos technology favor scalability to large sys-  tems. chapter fifteen, "neuromorphic synapses for artificial dendrites," by  westerman et. al. approaches the synaptic connection in a more biological  manner, mimicking some of the dynamics found in real synapses. in chapter  sixteen, "winner-take-all networks with lateral excitation" by indiveri, the  classical winner-take-all circuit is extended to exhibit hysteresis and lateral ex-xvi neuromorphic systems engineering  citation, making the circuit more robust to noise. its behavior is demonstrated  on a 1-d silicon retina implementation.  "neuromorphic learning" is the heading of part v and starts with the in-  troduction, "neuromorphic learning vlsi systems: a survey," by cauwen-  berghs. this is an excellent review of the development of learning systems in  hardware and the bibliography serves as an excellent reference. cauwenberghs  is also the author of chapter eighteen, "analog vlsi stochastic perturbative  learning architectures," presenting three different learning algorithms taking  the stochastic nature of neural plasticity into account. the techniques apply  to general reward-based learning and give rise to scalable, robust learning ar-  chitectures in analog vlsi independent of both the structure of the network  and the specifics of the learning task. the last chapter of the book, "winner-  take-all associative memory: a hamming distance vector quantizer," by  pouliquen et. al., is a remarkably simple, mixed-mode system using a memory-  based computation to classify input patterns according to a known pattern set.  the system is shown to work reliably discriminating bitmapped characters,  even with some error correction.  although this book is far from providing complete coverage of this emerging  field, i sincerely hope that the material presented will spawn some interest and  make more people join the exciting and highly rewarding research in neuro-  morphic engineering.  tor sverre landeacknowledgements  preparing an edited book should be easy, since most of the written material  is done by somebody else. i have learned that merging material from different  word-processing systems is far from easy. thanks to my student, jan-tore  marienborg, all the individual bibliographies were merged into one conformant  .bib-file. he also helped me converting files from different word-processing  systems.  the book is completely set and printed using computer facilities at the de-  partment of informatics, university of oslo. as a staff-member of the mi-  croelectronic systems group my gratitude goes to all of you and especially  to my college, yngvar berg, for valuable corrections and suggestions. also my  ph.d. student, sigbj~rn nmss, did an excellent job correcting numerous errors.  my friend, mohammed ismail - the editor the kluwer journal "analog inte-  grated circuits and signal processing", encouraged me to be the guest-editor  of a special issue of the journal and gave me valuable advise.  the central players of this book are all the contributing authors, without  whom this book never would have been printed. i am proud of being the editor  of such aa excellent collection of outstanding work and i am grateful for all  the encouraging feedback from all of you. it is my sincere hope that this book  is something you will be proud of in the future. i know that several of the  authors contributing to this book have experienced the great inspiration of  carver mead. i hope this book will carry on his ideas and open up for a wider  audience the importance of his pioneering work.  finally i want to thank my always understanding and patient wife, elisabeth.i cochlear systemsi filter cascades as analogs  of the cochlea  richard f. lyon  foveonics inc., i0131-b bubb rd., cupertino ca 95014  dicklyon@acm.org  1.1 models of cochlear wave propagation  wave propagation in the cochlea can be modeled at various levels and for vari-  ous purposes. we are interested in making models of cochlear signal processing,  in analog or digital vlsi or in software, suitable for supporting improved hear-  ing aids, speech-recognition systems, and other engineered hearing machines.  we are also interested in developing models that can contribute to a deeper un-  derstanding of how hearing works. hence, a neuromorphic approach, in which  the functionality of the model emerges from a form that is loosely copied from  the nervous system, is appropriate.  the filter-cascade approach to modeling the cochlea is based on the obser-  vation that small segments of the cochlea act as local filters on waves propa-  gating through them. thus, a cascade of filters can emulate the whole complex  distributed hydrodynamic system. this modeling approach can include com-  pressive and adaptive aspects of the peripheral auditory nervous system as  well, using analogs of cochlear nonlinear distortion and efferent feedback. we  summarize the underpinnings, advantages, and limitations of this approach in  this paper~ so that readers can more readily understand other papers on filter-  cascade approaches and implementations.  figure 1.1 shows the filter-cascade structure that we discuss in this paper.  1.2 cochlear hydrodynamics in the liouville-green  appoximation  imagine the cochlea as a three-dimensional (3d) hydrodynamic system with  a linear or one-dimensional (1d) array of sensors attached to it. in the real4 neuromorphic systems engineering  in  (stapes) filter cascade structure  x (place) dimension  1~  from base (high cf) . . . to apex (low cf)  figure 1.1 a filter cascade. this simple structure of cascaded filter stages is a useful  analog to the hydrodynamic wave propagation system of the cochlea.  cochlea, the 3d and 1d structures follow a complicated helical path, with the  sensors being the inner hair cells (ihcs) of the organ of corti. abstractly,  we refer to the one dimension that indexes the sensors as the cochlear place  dimension. from a functional point of view, we care about the response only  as a function of the input signal and of the 1d place, so there is only one  relevant spatial dimension at the model output.  wave propagation in the cochlea depends on fluid displacement in three di-  mensions, on membrane bending and stretching, and on related 2d and 3d and  micromechanical effects within the organ of corti, which have mostly unknown  physical parameters. there have been many arguments in the cochlea-modeling  business about whether 1d, 2d, or 3d models are good enough to capture the  essence of the physics. independent of these arguments, if the results of the  model are needed at only a sequence of places along one dimension (such as  at the inner-hair-cell locations), then we can represent the results of the 2d  or 3d hydrodynamics by a 1d model system, and can do so by using transfer  functions, more economically than by modeling the fluid motion directly.  one key property of cochlear physics on which we must rely for this approach  is the unidirectionality of energy flow. under normal conditions, sound energy  enters the ear, propagates, and is absorbed without causing significant energy  to reflect and propagate back out [2]. this condition is the one that we shall  model; we discuss exceptions in section 1.9.  the method known as liouville-green (lg) or wentzel-kramers-brillouin  (wkb, or wkbj with jeffreys) give us easy insight into wave propagation in  nonuniform media such as the cochlea. this method says that, if a wave is  propagating from the input along one dimension, then we can get the response  from the input to any point by composing the relative responses from each  point to the next along that dimension, using local parameters as though the  medium were uniform.  the mathematics is most easily expressed in terms of a wave description  in which the local (i.e., at any particular place) wave-propagation properties  are characterized by a complex wavenumber. to make life simple, we considerfilter cascades 5  only one frequency at a time. to simplify the analysis further, we use complex  numbers as the values of waves, realizing that we can easily constrain wave  values to real numbers later by adding pairs of complex waves in a complex-  conjugate relationship.  in a uniform medium, a wave propagating toward increasing values of the  place dimension is given by  w(x) = a exp(i~t - ikx)  where a is the amplitude, ~o is the frequency, k is the wavenumber, t is time,  and x is place. the wavenumber depends on frequency via the physics of the  medium; we can write it as the function k(~v)--the solution of the physical  constraints known as the eikonal or the dispersion relation of the system.  we can think of the wavenumber as the spatial frequency of the wave, in  radians per meter in mks units. if k is real, then the wave described simply  progagates with no change in amplitude, with a wavelength of 2~r/k, at a veloc-  ity w/k. if k has a nonzero imaginary component, however, then the wave can  decay in amplitude as it propagates (i.e., in a passive or attenuating medium, for  a negative imaginary part) or increase as it propagates (i.e., an active amplify-  ing medium, for a positive imaginary part). zweig and colleagues [29] presented  an analysis of the 1d long-wave approximation to the cochlea with resonant  basilar membrane (bm), using the complex radian wavelength :~ (lambda-bar,  the reciprocal of the wavenumber).  by examining the ratio of waves at two places separated by a distance ax,  we see that the wave at the farther place is equal to the wave at the nearer place  multiplied by exp(-ikax), representing a frequency-dependent filter character-  izing the stretch of length ax.  in a nonuniform medium, there is no single wavenumber for a frequency, and  possibly certain regions amplify a particular frequency while others attentuate  it. under reasonable conditions, however, each point in such a medium (i.e.,  along the place dimension) can be characterized by a local wavenumber, as  though it were part of a uniform medium. the lg approximation then says  that a wave propagating an infinitesimal distance dx through that place will be  multiplied by exp(-ikdx) (and possibly also by a real-valued factor near 1, if a  constant amplitude does not correspond to a constant power as the parameters  of the medium change--but let's neglect that factor for now).  now consider wavenumber as a function of both frequency and place: k(w, x).  within the approximation of the lg method, this function completely char-  acterizes wave propagation in the nonuniform medium along the x dimension.  to see what happens between points far apart, we can break the medium into  infinitesimal segments of length dx, and can multiply together all the factors for  those segments. the factors are exponentials, and the product of exponentials  is the exponential of a sum, so the resulting product is the exponential of an  integral along the x dimension:6 neuromorphic systems engineering  ~ xb  h = exp(-i kdx)  a  this complicated-looking frequency-dependent gain and phase factor h is  the lg method's representation of the transfer function between points xa  and xb in a nonuniform medium; it is a generalization of the transfer function  exp(-ikax) that characterizes a stretch of a uniform medium.  the factor h is still just a linear filter in the usual signal-processing sense.  furthermore, we can factor this filter into a product, or cascade, of several filters  by splitting the interval of integration (from x~ to xb) into n small steps:  n ~xxj  g -- ii exp(-i k(w, x)dx)  j=l j-1  any number and size of steps leads to a factorization, but, if the steps are  small enough, then each individual filter will be well approximated from a local  wavenumber by exp(-ikax), where ax = (xb -- x,~)/n is the step size, making  it easier to tie the filters directly to a model of the underlying wave mechanics:  n  h ~ 1~ exp(-ik(w, xj)ax)  j=l  therefore, independent of the details and dimensionality of the underlying  wave mechanics, the responses of the cochlea at a sequence of places are equiv-  alent to the responses at the outputs of a sequence of cascaded filters. the  lg method constrains the design of those filters when the underlying physics  is known.  how does the filter relate to the wavenumber? for a given value of ax  in a uniform medium, the filter and the wavenumber are in 1-to-1 correspon-  dence via the complex exponential. for a given pair of places xa and xb in a  nonuniform medium, the filter is determined uniquely by the function k(w, x),  although the inverse is not necessarily true (i. e., a different k(w, x) with the  same integral on that interval, such as a spatial reversal of k(~, x), also would  be a solution).  even for nonlinear and time-varying wave mechanics, we can reasonably as-  sume that a nonlinear and time-varying filter cascade will be a useful structural  analog and a fruitful modeling approach: that of modeling local behavior with  local circuits. the approach is neuromorphic in the sense that it is based on the  form of wave propagation present in this peripheral part of the sensory nervous  system.  if the cochlea's frequency-to-place map is approximately logarithmic, and we  model equal place increments with filter stages, then the model stages will have  characteristic frequencies (or time constants) in nearly a geometric sequence.  we often assume a geometric sequence in model calculations, but the method  is more flexible and can be used to match realistic cochlear maps in which the  low-frequency region maps nearly linearly to place.filter cascades 7  1.3 power flow and active gain  the lg method goes one step further than we just described in providing  techniques to compute a slowly space-varying amplitude factor to account for  the varying relationship between wave amplitude and power in a system with  nonuniform energy-storage mechanisms. for example, if bm stiffness is vary-  ing, the proportionality between squared volume displacement and potential  energy is changing; the amplitude of a bm displacement wave needs to be ad-  justed accordingly. for our purposes, we shall typically jump up one level of  abstraction by imagining that our wave amplitudes are given in terms of de-  rived variables, such that constant amplitude corresponds to constant power.  therefore, the gain of the filters will be exactly 1 in regions that are passive and  lossless, as is typical of extremely low frequencies, relative to the characteristic  frequency (cf) of the cochlear place under consideration.  for more specific information on hydrodynamic modeling and the lg  method, relative to analog vlsi implementations and numerical methods, see  the dissertation of lloyd watts [26].  we have not yet said how the filters in a cascade should look--we have said  only that the design can be constrained by models at a lower level. we discuss  specific filters in section 1.5. in general, we expect cochlear filters to be passive  and linear for low frequencies, to provide active gain or power amplification for  frequencies near cf, and to attentuate high frequencies. therefore, filters of  the class of interest have unity gain at dc, followed by a gain somewhat greater  than unity, and a high-frequency gain less then unity.  if the model has many such filters in cascade, then the individual filter stages  do not need to have gains far from unity for the cascade to achieve an aggregate  pseudoresonant [5] response, with a high peak gain and a large high-frequency  attenuation.  the notion of a pseudoresonance differs in a fundamental way from that of  a resonance, with which engineers generally are familiar. a pseudoresonance is  a broadly tuned gain bump that results from the collective behavior of a large  number of broadly tuned (and hence low-precision) stages (or poles), or of a  distributed system. a resonance, on the other hand, becomes narrowly tuned  and needs high precision to achieve a high gain at its center. reliance on such  a collective computation is another hallmark of the neuromorphic approach.  1.4 wide-dynamic-range compression via filter  cascades  one of the most important nonlinear functions of the cochlea is the compres-  sion of a wide range of sound intensities into a narrower range of cochlear-  motion intensities at the sensor array, for frequencies near cf. studies of  cochlear mechanical response since about 1970 [19] have repeatedly demon-  strated this frequency-dependent compression in live cochleae, and its absence  in dead cochleae.8 neuromorphic systems engineering  in live cochleae, the overall input output intensity curves for frequencies near  cf have a slope of typically 0.25 to 0.5 on a log-log plot [20]. this reduced slope  is known as a compressive nonlinearity. the exact slope depends on the quality  of the experimental preparation, on the frequency and intensity range, and on  whether the response is measured at a fixed frequency or at the frequency of  greatest response, which shifts a little with level. a higher compression (slope  0.25, or 4-to-1 compression) is more typical at a fixed frequency at or above the  most sensitive frequency (cf), and a lower compression (slope 0.5, or 2-to-1  compression) is more typical at the peak response frequency, as the peak moves  to frequencies below cf at higher sound levels.  from our filtering point of view, we need level-dependent filters whose gains  decrease as the signal level increases, to model this mechanical compression.  equivalently, we expect that the imaginary part of the wave number will change  with level, even changing its sign between damping and amplification for some  combinations of frequency and place.  presumably, the dependence of wavenumber on level comes from nonlineari-  ties in the biomechanics, including the outer hair cells, which are the presumed  source of the energy needed to provide active gain. these mechanical changes  modify the way that traveling waves pick up energy, and the resulting cascaded  filters that model a set of different places are a reflection of the underlying wave  mechanics. therefore, a filter-cascade model can, in principle, exhibit a range  of behaviors similar to those of the mechanical system.  as a wave picks up energy in traveling across a range of places, each little  increment of place needs to contribute only a small amount of gain. if the  filter-cascade model has stages that model small ax regions, then each filter  will need to contribute only a small gain; as the overall gain changes, each filter  will have to change only slightly.  a power gain, or a filter gain greater than unity, is correlated in this view  with an active process that we think of as providing an active undamping--  effectively a negative viscosity. but even if we do not rely on this notion of a  literal power gain, the variable-gain filter-cascade structure provides a qualita-  tive functional model of the variable-gain behavior observed in the cochlea--it  could be adapted to fit the wave mechanics of a passive model. indeed, the ba-  sis for our first use of the filter-cascade technique [11] was a passive long-wave  analysis [29].  in our earlier model [11], we added the gain variation after the filters as  a functional afterthought, so the model did not have the right frequency-  dependent properties, such as linearity at low frequencies. because it incor-  porates gain variation directly into the cascade as filter-parameter (q) varia-  tion, the filter-cascade approach inherently achieves a reasonable constraint on  how the overall filter gain can vary with frequency and place: the different  places share most of the same cascade filters. that is, it is not possible to have  a high peak gain at one place and a low peak gain at a nearby place, even  if the cascaded filters vary substantially, because the composite responses at  nearby places share most of the same filters. this property arises because wefilter cascades 9  are modeling a wave propagation directly, again illustrating the benefit of a  neuromorphic approach.  1.5 fitting a filter cascade to the cochlea  for most models of the mechanics of cochlear wave propagation, the qualita-  tive behavior of a stage is just this: the filtering provides a gain bump for  frequencies "near" cf, and provides attenuation at higher frequencies. what  are the simplest lumped-parameter filters (i.e., small sets of poles and zeros in  laplace or z-transform spaces) that can model this qualitative behavior? how  significantly do the details of the stage filter affect the overall pseudoresonant  response of the cascade? we answer these questions using examples.  the simplest stage is a two-pole filter, which we refer to as a second-order  section (sos). the sos as commonly used in digital signal processing might  have either just two poles and no zeros in the simplest case, or might include 1  or 2 zeros in a higher-complexity alternative. we have focused on the simpler  all-pole version in recent years [27, 18], whereas our earlier work used both  poles and zeros [15, 11, 12, 16, 23].  an active-undamping approach to a physical basis for a wavenumber solu-  tion [18] led us to believe that simple two-pole filters may be not sharp enough  (have narrow enough relative bandwidth) to be realistic, and that a three-pole  filter would be a closer match. but two poles and two zeros can accomplish  the same sharpening if that is what we need to fit experimental or theoretical  data. in spite of this sharpness discrepancy relative to our particular mechan-  ical model, we see the two-pole filter as a good and useful model of cochlear  function. we should not rule out this simplest approach without a compelling  reason.  figure 1.2 shows a composite pole-zero diagram representing four alternative  designs for a single filter stage: two-pole, three-pole, two-pole/two-zero, and a  sharper two-pole/two-zero designs. the corresponding stage transfer-function  gains and group delays are shown in figures 1.3 and 1.4, respectively. the  two-pole/two-zero designs have sharper drops just beyond cf than the all-pole  designs, but then level out at some gain less than i, rather than continuing to  drop.  figure 1.5 shows the composite gains of long cascades of geometrically spaced  stages, and figure 1.6 shows the corresponding total group delays. note that  we can sharpen the two-pole response by adding either an extra pole or a pair  of zeros; the resulting cascade gains can be made similar, but adding poles  adds to the delay, whereas adding zeros reduces the delay. this dimension of  flexibility may be useful if we wish to match the model to a delay or phase  measurement.  moving the zeros closer to the poles and closer to the imaginary axis in the s  plane results in more sharpening, especially of the high side, of the overall filter.  this configuration fits the notch transfer function of a long-wave mechanical  approximation, and was the basis of our original cochlea model [ii]. we now10 neuromorphic systems engineering  o  2-pole,2-zero ~ o  x  2-pote yc'x  3-pole ~.._~ ~x  2-pole,2-zero uo sharper s-plane  figure 1.2 composite pole-zero diagram. four different filter designs are specified within  this composite diagram, so that the pole and zero positions can be compared. since complex  poles and zeros are inherently in a complex-conjugate relationship, we label several in the  top half-plane and several in the bottom half-plane to reduce clutter.  ~0  -5  -10  -15  -2( stage gain  i  o.1  o-pole  ~ sharper \,~,.x,, two-pole, .:\,\ two-zero ........  ".... ~ '\ ." ." ..'"  â¢  " '~",~ two-pol~  ~ -  three- '"~2~1'2 ~  freque'ncy (ar~itrawu'nit's) ' ' '1'.0  figure 1.3 stage transfer-functions gains. for each of the four filter designs of figure 1.2,  the magnitude of the stage transfer function is plotted.  believe that that model was too sharp, due to the unsuitability of the long-  wave approximation for modeling the real cochlea near cf. at the time, we  used too-s.harp filter models by trying to match transfer functions to iso-ratefilter cascades 11  7  3 stage group delay  i ; : !  ' " i  .... t_hr_ee_-po_l_e _ _ ~  /... \  ./" ..." i. \\ \,,  â¢ ~ ..... â¢ snarper. ~. ~,  ............. ~._ - two:pole/ -x~.,,  ........... ' .... two ~ero ~ ~  .... 0'.1 freqt~ency (~rbit~ary'unitsj " ' i.0  figure 1.4 stage group delays. for each of the four filter designs, the group delay of the  filter stage is plotted in arbitrary units.  tuning curves; that approach is clearly inappropriate, as we have explained  subsequently [13].  1.6 cascade-parallel filterbanks  our original filter model [11] was based on the longwave approximation to  cochlear mechanics, in which a significant membrane mass leads to a true local  resonance. we used a cascade of notch filters (two-pole/two-zero) to model  pressure-wave propagation, plus a resonator at each tap of the cascade to con-  vert pressure to bm displacement or velocity locally. this cascade-parallel  architecture may still be a useful way to separate the propagated variable from  the sensed variable, and possibly to simplify the required filters. for the pur-  poses of that model, however, we noticed that, by constraining the parameters  and rearranging some poles, we could easily convert the structure to a roughly  equivalent pure cascade version, saving complexity and computation [12].  figure 1.7 shows the cascade-parallel structure; contrast it with figure 1.1.  notice that the output taps of the cascade-parallel structure are still always  related by a relative transfer function, such that a pure cascade equivalent  version exists, although stability of the exactly equivalent pure cascade is not  ensured unless the parallel filters have stable inverses.12 neuromorphic systems engineering  40  20  -20  --40  -60 cascadeoain twÂ°-pÂ°lÂ°  ~'.~ : ~.i\ two-pole,  _~~- ~ i!ii\ two-zero  three-pole " ~:i!)i~ ~ three-pole i(.  \~!  shamer : ~ ~  two-pole'" -r -, 9 i  ~o-zero i~,~ )~  i i  ti  (~  .... ~ ...... i"l  o. 1 frequency (~rbitraw units) 1.0  figure 1.5 aggregate transfer-functions gains. for each of the four filter designs, the gain  of a long geometrically-spaced cascade is plotted. the scale is arbitrary, because it depends  on the density of stages per octave.  1.7 nonlinear effects  a filter cascade can have an overall strongly compressive nonlinear input-  output behavior, if the stages are weakly compressive. there are two general  forms of nonlinearity that are important to consider, and it is likely that both  operate in the real cochlea: instantaneous nonlinear distortion, and feedback  control of a peak-gain parameter. for example, the nonlinear model of kim  and his colleagues [8] is just a cascade of two-pole filters with a compressive  nonlinearity in each stage; kim's later suggestions [7] are of the parameter-  feedback form, and are motivated as a functional role for the auditory efferent  system.  an instantaneous distortion nonlinearity, such as a hyperbolic tangent that  puts a firm saturation limit on the amplitude out of each stage, leads to the gen-  eration of intermodulation products, such as the cubic-difference tone 2fl - f2.  distortion products are mostly generated where the primary components (fl  and f2 in this case) are large, and distortion products with frequencies below  the primaries are then free to propagate farther to their own place. frequencies  above and below the primaries can sometimes be detected propagating back out  of a real cochlea, but unidirectional cascades cannot model that effect.  as discussed in section 1.4, we can use feedback of a detected output level  to affect the filter parameters, implementing a less distorting amplitude com-filter cascades 13  140  120  100  ~ 80 ,~  .~60  c~ " 4o  m 20 cascade group delay ,' ,,  / \  i i ~\  / ~  ~ \  / \  three-pole ,, - - ",  two-~" '  t/ ... .: \. \  ~" f '. '\.  ~,. ." ". \  â¢ ~ .. .. "\ two-pole, tw_orz_ero ..... " ... ,  .............. . '.. .~  ......... ;hamer , ................................... ~o-pole, ~o-zero '".. '.,.  , , , , i ...... "'~'" ..........  0.1 frequency (a'rbitra~ units) 1.0  figure 1.6 aggregate group delays. for each of the four filter designs, the group delay of  the cascade is plotted in arbitrary units.  cascade/parallel structure  figure 1.i' the cascade-parallel structure. this cascade-parallel filter configuration can  model the response of the cochlea with more flexibility than the pure cascade structure has.  pression known as automatic gain control (agc). a small reduction in the pole  q of each stage in response to output energy can lead to a highly compressive  overall response. this agc is one purported function of the auditory efferent  innervation: to tell the outer hair cells to turn down their level of activity [7].  sound in one ear is even known to drive the efferent neurons to the contralateral  cochlea, perhaps to keep the gains of the two ears more synchronized than they  would be otherwise, and thus to aid the brain in binaural level comparisons [1].14 neuromorphic systems engineering  1.8 silicon versus computer models, and practical  problems  we would need a high-power programmable processor to implement a filter-  cascade model of the cochlea in real time. dedicated silicon implementations,  on the other hand, can be made with less silicon and much lower power con-  sumption [27, 12, 27]. an important cost factor in analog or digital sampled-  data implementations is that avoidance of aliasing in the nonlinear opera-  tions that follow the cochlea model requires a substantial oversampling fac-  tor. by avoiding clocks, sampling, and high-speed circuits, the continuous-  time analog approach yields by far the lowest power consumption, but requires  novel solutions to noise, matching, tuning, stability, and communication prob-  lems [3, 6, 11, 25, 24, 25, 32, 90]; see also the other papers on neuromorphic  analog cochleae in this book.  1.9 limitations  as we mentioned in section 1.7, distortion products can propagate backward  out of a real cochlea, but not out of a unidirectional filter-cascade model. this  limitation applies to other otoacoustic emissions as well, both stimulated and  spontaneous. so the filter cascade is not a suitable modeling substrate for such  effects.  the filter-cascade model is based on looking at a set of points along only  one dimension, and as such provides no direct help in our understanding the  motion of other parts of the cochlea or in the cochlea's fluid. micromechanical  models, 2d and 3d models, and other modeling approaches can help to inform  the design of the filter cascade, but the filter cascade then captures only a slice  of the more detailed models.  any small stretch of cochlear transmission line acts approximately as the  filter exp(-ikax); but does this filter, or an approximation to it, have useful  properties, such as stability or causality? the filter specification derived from  the wavenumber tells us what happens at all sine frequencies; to address sta-  bility, however~ we need to consider approximate filter models with poles and  zeros. we believe, but have not proved, that, if the mechanical wave system is  stable, then stable rational filters exist that are reasonable approximations to  the system's frequency response.  causality is more complicated, because the response at a point is not phys-  ically caused by only the action at a different point upstream, even under  the unidirectional assumption; rather, it is caused by the combined actions of  nearby points in the whole 2d or 3d motion of the medium. the resulting  filter, or approximations to it, could conceivably show precursors in response  to an impulse. so, if we design a filter with the right magnitude frequency  response, causality may force the filter to have excess delay if the ax value  is short compared to the wavelength. thus, fine division of the place dimen-  sion may make it increasingly difficult to get the phase right in low-order filter  appoximations--especially in the case of causal all-pole filters. adding zerosfilter cascades 15  helps us to cancel some of the delay of the poles, thus making it easier for us  to develop a model with reasonable phase. kates [6] has explored one class of  filter cascades using zeros to arrive at lower overall delay.  the lg method breaks down in the cochlea in the cutoff region, where the  eikonal has multiple complex solutions for k. in this region, the wavenumber  changes so rapidly that there is effectively a mode coupling phenomenon that  allows energy to couple into several of these different wave modes, which in-  terfere with each other in complex ways, and which decay more slowly with x  than does the original mode [26]. the resulting high-frequency irregularity, or  plateau, in the response gain, which is found in numerical 2d solutions and is  sometimes observed in real cochleae, is not easily modeled by cascades of simple  filters. this discrepancy is an obstacle not to the concept of a filter cascade,  but rather to the modeling details and to the desire to use simple stages. it  seems likely, however, that the high-frequency plateau has no functional im-  portance in normal hearing. complicated response patterns due to cochlear  micromechanics may lead to similar considerations, depending on one's goals  in modeling.  1.10 relation to other approaches  the most common functional approach to computational models of the cochlea  is the bandpass filterbank. in this approach, every place to be modeled has  its own filter, which is designed to match experimental data. because there is  usually no good basis for constraining a filter design using poles, an all-zero  (transveral or finite impulse response) filter is often employed. both of these  features--independent filters and lack of poles--make the implementation of  this approach computationally expensive.  filterbanks that use poles--such as the gammatone filterbank (gtf) and its  all-pole variant (apgf) [14, 23j--are becoming more widely used, because of  their efficiency and simpler parameterization. the gtf is popular, but has an  inappropriate symmetric passband; the apgf is closely related to a cascade  of two-pole filters, and is therefore much more realistic in terms of transfer  function and of the possibility of parametric nonlinearity.  an analog silicon model of the cochlea that can propagate waves bidirection-  ally has been reported by watts [27]. it uses a 2d resistive grid as a substrate  for directly solving laplace's equation for wave propagation in a 2d fluid model  of the cochlea, with second-order filters along one edge modeling the bm-fluid  interaction. this approach needs to be further developed to see whether it  leads to an overall advantage in implementing an effective cochlea model. a  potential problem is that irregularities or the inherent spatial discretization  may lead to reflections that cause instability, as has sometimes been a problem  in 2d numerical solutions of active cochlea models.16 neuromorphic systems engineering  1.11 conclusions  the filter-cascade structure for an cochlea model inherits two key advantages  from its neuromorphic roots: efficiency of implementation, and potential re-  alism. both the filter transfer functions, in terms of magnitude and delay  dispersion, and the nonlinear behaviors of the cochlea, in terms of distortion  and adaptation, are modeled realistically under the constraints imposed by the  cascade. minor problems, such as excess total delay in the finely discretized  all-pole version, are tolerable in practical applications.  analog vlsi implementations of the filter-cascade cochlea model are cur-  rently being explored at a number of laboratories around the world. the ideal  of a practical micropower real-time artificial cochlea circuit is rapidly coming  closer to reality.  acknowledgments  i thank malcolm slaney, lloyd watts, john lazzaro, rahu] sarpeshkar, and carver  mead for their contributions to the work discussed. and i thank tor sverre lande  for encouraging me to write this paper, and lyn dupr~ for skillfully editing several  drafts of the manuscript.  references  [1] j. f. brugge. an overview of central auditory processing. in a.n. popper  and r.r. fay, editors, the mammalian auditory pathway: neurophysiol-  ogy, pages 1 33. springer-verlag, 1992.  [2] e. de boer and r. mackay. reflections on reflections. j. acoust. soc.  am., 57:882-890, 1980.  [3] p. l~rth and a. b. andreou. a design framework for low power analog filter  banks. ieee transactions on circuits and systems, part i: fundamental  theory and applications, 42:966-971, 1995.  [4] p. furth and a. b. andreou. linearized differential transconductors in sub-  threshold cmos. ieee electronics letters, 31(7):545-547, march 1995.  [5] m. holmes and j. d. cole. pseudo-resonance in the cochlea. in boer e.  de and viergever m.a., editors, mechanics of hearing. martinus nijhoff  publishers, the hague, 1983.  [6] j. m. kates. a time-domain digital cochlear model. ieee trans. signal  processing, 39:2573 2592, december 1991.  [7] d. o. kim. functional roles of the inner- and outer-hair-cell subsystems  in the cochlea and brainstem. in berlin c., editor, hearing science, pages  241-261. college-hill press, san diego, 1984.  [8] d. o. kim, c. e. molnar, and r. r. pfeiffer. a system of non-linear  diferential equations modeling basilar membrane motion. j. acoust. soc.  am., 54:1517-1529, 1983.filter cascades 17  [9] j. lazzaro, j. wawrzynek, m. mahowald, m. sivilotti, and d. gillespie.  silicon auditory processors as computer peripherals. ieee journal of  neural networks, 4(3):523-528, 1993.  [10] w. liu, a. andreou, and m. goldstein. voiced-speech representation by  an analog silicon model of the auditory periphery. ieee transactions of  neural networks, 3(3):47?-487, 1992.  [11] r. f. lyon. a computational model of filtering, detection and compression  in the cochlea. in proc. ieee intl. conf. on acoust. speech and signal  proc., pages 1282-1285, paris, 1982.  [12] r. f. lyon. computational models of neural auditory processing. in proc.  ieee intl. conf. on acoust. speech and signal proc., san diego, 1984.  [13] r. f. lyon. automatic gain control in cochlear mechanics. in p. dallos  et al., editor, the mechanics and biophysics of hearing, pages 395-402.  springer-verlag, 1990.  [14] r. f. lyon. all-pole auditory filter models. in e. lewis, editor, diversity  in auditory mechanics. world scientific, in press.  [15] r. f. lyon and l. dyer. experiments with a computational model of the  cochlea. in proc. ieee intl. conf. on acoust. speech and signal proc.,  pages 1975-1978, tokyo, 1986.  [16] r. f. lyon and n. lauritzen. processing speech with the multi-serial signal  processor. in proc. ieee intl. conf. on acoust. speech and signal proc.,  tampa, 1985.  [17] r. f. lyon and c. mead. an analog electronic cochlea. ieee trans.  acoust., speech, signal processing, 36:1119-1134, july 1988.  [18] r. f. lyon and c. mead. cochlear hydrodynamics demystified. caltech  computer science technical report caltech-cs-tr-88-4, caltech, 1989.  [19] w. s. rhode. observations of the vibration of the basilar membrane in  squirrel monkeys using the msssbauer technique. j. acoust. soc. am.,  49:1218-1231, 1971.  [20] l. robles, m. a. ruggero, and n. c. rich. basilar membrane mechanics  at the base of the chinchilla cochlea, input-output functions, tuning curves  and response phases. j. acoust. soc. am., 80:1364-1374, 1986.  [21] r. sarpeshkar, lyon r. f., and c. a. mead. an analog vlsi cochlea with  new transconductance amplifiers and nonlinear gain control. in proc. ieee  intl. conf. on circuits and systems, volume 3, pages 292-295, atlanta,  may 1996.  [22] r. sarpeshkar, lyon r. f., and c. a. mead. nonvolatile correction of  q-offsets and instabilities in cochlear filters. in proc. ieee intl. conf. on  circuits and systems, volume 3, pages 329-332, atlanta, may 1996.  [23] m. slaney. lyon's cochlear model. apple technical report 13, apple  computer, cupertino, ca, 1991.18 neuromorphic systems engineering  [24] c. summerfield and r. f. lyon. asic implementation of the lyon cochlea  model. in proc. ieee intl. conf. on acoust. speech and signal proc., san  francisco, 1990.  [25] a. van schaik, e. fragni~re, and e. a. vittoz. improved silicon cochlea  using compatible lateral bipolar transistors. in david s. touretzky,  michael c. mozer, and michael e. hasselmo, editors, advances in neu-  ral information processing systems, volume 8, pages 671-677. the mit  press, 1996.  [26] l. watts. cochlear mechanics: analysis and analog vlsi. ph.d. disser-  tation, california institute of technology, 1993.  [27] l. watts, lyon r. f., and mead c. a bidirectional analog vlsi cochlear  model. in c. sequin, editor, advanced research in vlsi, proceedings of  the 1991 santa cruz conference, pages 153-163, cambridge, ma, 1991.  mit press.  [28] l. watts, d. kerns, r. f. lyon, and c. mead. improved implementation  of the silicon cochlea. ieee journal solid-state circuits, 27(5):692-700,  may 1992.  [29] g. zweig, r. lipes, and j. r. pierce. the cochlear compromise. j. acoust.  soc. am., 59:975-982, 1976.2 an analogue vlsi model of  active cochlea  eric fragni~re i, andr~ van schaik i and eric a. vittoz 2  1 mantra centre for neuromimetic systems, department of computer science,  swiss federal institute of technology (epfl), ch-i015 lausanne, switzerland  fragniere@di.epfl.ch  2 csem, centre suisse d'electronique et de microtechnique s.a.,  jaquet-droz 1, ch-2007 neuchstel, switzerland  2.1 introduction  in the last decade, analogue electronics has been almost confined to the con-  version between data of the physical world and its abstraction by numbers, in  order to process it with efficient digital computers. nature, however, could not  wait for the development of computer science and creatures developed vari-  ous strategies to interact with their environment. these interactions consist of  sensing the environment and producing an action on it under the control of a  decision. to be efficient, this decision is based on a set of representations of the  environment best suited to the usual action to be taken: the perception of the  environment. in applications where the perception/decision scheme can be ap-  plied such as pattern recognition, there is a growing interest to take inspiration  from strategies developed by nature, especially where computer algorithms still  fail to be as efficient as their natural counterparts. analogue vlsi techniques  seem best suited for an efficient implementation of the perceptive functions as  an analogue/perceptive converter: data of the physical world is converted into  relevant perceptive information rather than into a sequence of numbers allowing  its perfect restitution, like in conventional analogue/digital converters [20].  in automatic speech recognition (asr), the decision is nowadays usually  handled by efficient statistical classifiers based on hidden markov models  (hmms). however, the efficiency of hmms strongly relies on restrictive as-  sumptions on its input [3], which are generally not fulfilled by the preprocessing  stage of most asr systems. attempts to use models of the auditory pathway20 neuromorphic systems engineering  as preprocessing stages [14] still do not really improve the speech recognition,  particularly in noisy environments [4]. the main reasons are that the result-  ing classifier's input are not in accordance with its working assumptions and  that the speech model itself is not realistic. an alternative markov-like recog-  nition model best adapted to speech signals is currently being developed [29].  it models speech as a sequence of relevant spectral and amplitude transitions  only, instead of the usual sequence of statistical characteristics of stationary  segments. promising results are shown in [13], but a preprocessing stage that  could efficiently emphasise and code these transitions is still to be developed. if  an analogue/perceptive converter dedicated to transform a speech signal into  relevant transitions can be identified in the auditory pathway, it should be used  as a guideline for developing such a preprocessing stage.  in the inner ear's cochlea, the input speech signal induces mechanical vi-  bration on the basilar membrane (bm). the amplitude of these vibrations are  frequency dependent, with a most sensitive response at a characteristic fie-  quency (cf) which depends on the position along the bm. the inner hair  cells (ihcs) transform the bm movement into a release of neuro-transmitters,  activating the auditory nerve fibres. the temporal adaptation of this trans-  duction is said to be responsible for the enhancement of intensity transitions  measured on the auditory nerve in response to brief tone bursts [11]. further  in the auditory pathway, in the cochlear nucleus, the onset cells are neurons  that spike only at the onset of the tone burst [15].  the inner hair cells and the onset cells are often mentioned as processing  stages where transitions are enhanced. however, the transduction from acous-  tic pressure to mechanical vibration on the bm already presents the required  characteristics. in order to get a limited bm movement range for a wide input  dynamic range, an active process that feeds energy back onto the bm at low in-  put intensities is hypothesised. this process also explains the sharp frequency  selectivity around cf measured at a given position of the bm at low input  intensities [8]. as a consequence of this tuned adaptive gain control, intensity  transitions near the cf are enhanced.  several computer models attempt to capture this active process, with various  trade-off choices between model accuracy and computational efficiency [3, 5, 18],  but even the most efficient ones could not run in real time at low cost  and low power with enough detail. analogue vlsi models seem to permit  such performance. lyon's silicon cochlea [27] and its successive improve-  ments [32, 90] were already widely used as a basic structure for several efficient  analogue/perceptive converters [2, 15, 10].  we propose in this paper an analogue vlsi model of an active cochlea based  on the silicon cochlea [32]. in section 2.2, the model is carefully analysed. after  a brief description of the functions required to model a single section of the  cochlea, we outline its analogue vlsi implementation in order to estimate its  crucial parameters. we show that an individual stage could not support the  quality factor required to match the physiological data measured on biological  cochleae. the pseudoresonance, which yields frequency selective high gainsactive cochlea 21  despite limited quality factors in the individual stages is then presented and  analysed in order to design an appropriate pseudoresonant gain control loop.  a computer simulation performed on an amplitude gain model of the complete  cochlea with a single frequency input signal demonstrates the validity of the  concept. conditions and behaviour of sustained oscillations that may occur  in a single stage are determined analytically, while the stability analysis of  the complete quality factor control loop is outlined. section 2.3 proposes the  analogue building blocks required for a vlsi implementation of the model,  while the effect of the major technological limitations of a standard cmos  process are discussed.  2.2 the model  after several decades of controversies since the first measurements of the bm  movement, it has become accepted that the sharp frequency tuning at low input  intensities and the compressive nonlinearities at high intensities are caused by  an active process which feeds energy back onto the bm. this mechanism is  attributed to the outer hair cells (ohcs) which present motile properties in  addition to their sensitive abilities [6]. the effect of the ohcs can be seen as  a local adaptation of the bm quality factor.  2.2.1 functional model  figure 2.1 shows the proposed feedback loop that controls the bm damping  with the cochlea's appropriate output signal. similarly to [27, 32, 90], the  bm is modelled by a cascade of second-order lowpass filter stages. each stage  hk(s) = 1/(s2~-~ + s~-k/qk + 1) filters the bm displacement db,~ at the out-  put of the previous stage with a cutoff frequency fck = 1/27rrk and a quality  factor qk expressing respectively the cf and the effect of the ohcs at this  particular stage. the cf of each stage decreases exponentially from the base  (first stage) to the apex (last stage) of the cascade. each stage hk thus mod-  els a bm section located at a discrete position k along the bm. because the  ihc's stereocilia are in a viscous medium, they detect the velocity of the bm  movement relative to its supporting structure. the ihc model thus includes a  differentiator dk(s) = s~-~ which converts the bm displacement dbm into bm  velocity vb,~ [32, 90]. in addition the transmitter release in the ihc happens  only when its stereocilia bend in one direction. the ihc performs therefore  at first approximation a half-wave rectification of the bm velocity signal vb,~,  yielding the ihc output yihc o~ ivbm]. in this first approach, the temporal adap-  tation of the transmitter release [11] has been neglected, assuming for simplicity  the effect of the ohc's active process to be dominant for temporal adaptation  and transient enhancement. it is reasonable to assume that, in the auditory  pathway, the mean value of the bm velocity can be estimated at a higher level  as a mean spiking rate (msr) on the afferent nerve fibres projected from the  ihc at the corresponding bm position. as the dynamics of transmitter release  has been neglected, the instantaneous spiking rate (or spiking probability) is22 neuromorphic systems engineering  roughly proportional to the ihc output yihc, thus a measure of the mean bm  velocity y,~s~ ~ ivbm] may be available at the ohc's efferent nerve fibres. the  adaptive loop is closed by controlling the quality factor of the bm, assumed to  be the action of the ohc, using an appropriate quality factor modulation by  this measure y,~sr of the mean bm velocity.  mean spiking rate (msr)  figure 2.1 active cochlear filter cascade, with functional block diagram detailed for one  stage.  2.2.2 quality factor modulation  the effect of the ohcs is performed by controlling the quality factor qk of  each stage hk(s). on a single stage h(s)d(s), a sine wave input signal at  frequency f and with a peak amplitude x produces an output signal having a  peak amplitude y = [[h(f)d(f)[]x. at a frequency f close to the cf of the  stage, the amplitude gain nh(f)d(f)n ~ q, thus the output signal has a peak  amplitude y ~ qx. modulating the quality factor with a power a of the peak  output amplitude y  q o~ y ~ (2.1)  yields  y ~ x ~--~, (2.2)  where a < 0 produces the desired compressive input-output function.active cochlea 23  2.2.3 anmogue model  in the analogue model proposed in [27, 32, 90], the cutoff frequency f~k =  1/2~r~'k of a second-order stage hk is determined by ~-k = c,/9,~. the capac-  itances c~ of the bm block are identical for every stage and the transconduc-  tances 9m~k of its otas are controlled by the bias currents i,~ which decrease  exponentially along the cascade. the quality factor qk of any stage k is con-  trolled by the ratio between the transconductances gmqk and g,~,k of its ohc  ota and its bm otas respectively. as all otas operate in weak inversion,  their transconductance gm-r,q is proportional to their bias current i,,q. the  quality factor is given by  1 q - (2.3)  2(1- a~)'  where a -- g,,~q/21~ is a constant. gm./i.  a quality factor varying between qmin and q~ can thus be controlled by  means of a translinear loop which imposes  iq_ ier~a~ (2.4)  l ~c +~o  using normalised currents i~ = i~/io and iqmax "= -[qmax/io, equation (2.3)  can be expressed as  1 [ 2q.c~x _~_1_ ] (2.5)  q = ~ l + 2q,~axic + l j '  where 1 qrnax : (2.6) 2(1 - ~iqmox)  is obtained for ic = 0 and q,~n depends on the maximal available value of ic.  the quality factor control signal ic is made proportional to the peak ampli-  tude ?.b~ of the bm velocity analogue signal,  ic = f~vbm,  by the feedback gain  f- gmaarg,~f (2.7)  galo '  which takes into account the ihc ota transconductance g,ca, the ihc rectifier  gain ar between the dc component of the rectified bm velocity signal and it8  peak amplitude i?vbm, the dc gain g,~f/ga of the msr lowpass filter which  extracts this dc component and the normalising current i0 (figure 2.2).  the peak bm velocity l)vb,Â¢ is given by the amplitude gain of the bm stage  [ih(f)ii and its differentiator lid(f)l i for an input signal at frequency f having  a peak amplitude l~din  9vbm = ilh(f)d(f)ii?din" (2.8)24 neuromorphic systems engineering  the differentiation d(s) = s~- is simply performed by taking the difference  between the output voltages of both bm otas [32], which yields  8t  ~7- â¢ h(s)d(s) s~'r ~ + -~ + 1  ohc lihc   figure 2.2 analogue model of one stage of the active cochlear filter cascade.  2.2.4 level compression  except for the shift of variable limiting the quality factor between qm~n and  q~ax, with ic proportional to the peak amplitude l~vbm of the output signal  after stage tt(s)d(s), equation (2.5) respects the form of equation (2.1) with  a = -1. the required compressive input-output relation expressed by equao  tion (2.2) is thus obtained and the output peak amplitude ffvbm is proportional  to the square root of the input peak amplitude ~'~.active cochlea 25  the limited quality factor range implies that the square root compression is  performed only on a limited input dynamic range. under the same assumption  as in section 2.2.2, equations (2.5), (2.7), and (2.8) yield  x - 1 + ~/(1 - x) 2 + 8q~xx (2.9)  y= 4 '  where x = qmaxf~ai~ and y = qmaxf~vb,~. this function has an asymp-  tote limx-~ y = x/2 and slope qma~ at the origin x = 0. the square root  compression is thus active between the corresponding gains y/x = 1/2 and  y/x = qm~x. according to physiological data [17], an input sound level of 60  db spl corresponds to lnm bm displacement at frequencies below the cf of  the measured bm position, thus where ]lhkt] ~ 1 according to our model. on  the other hand, at frequencies close to the cf, the bm displacement equals the  bm velocity since [id~[[ ~ 1. a good correspondence between the physiologi-  cal data [17] and equation (2.9) can be obtained for a peak bm displacement  lower than 10nm (figure 2.3). at higher intensity values, a saturation that does  not stem from the quality factor control loop appears. this saturation will  correspond to the saturation of the physical devices used for the vlsi imple-  mentation of the model. at lower intensities, the model fits the physiological  data with a maximal quality factor qm~x ~ 180, allowing an amplitude gain  up to 45db at cf for low input level.  2.2.5 pseudoresonance  the quality factor required to match physiological data cannot be safely im-  plemented on a single analogue second-order filter, because the mismatch of its  internal components are likely to drive it into instability. nevertheless, ampli-  tude gains corresponding to such a high quality factor can be achieved using  the pseudoresonance of the cochlear filter cascade. the control is made locally  at every cascade stage in a narrow range of quality factors (between 1/~/~ and  2), but the accumulation of these effects on the cascade allows gains up to 45db  for low intensity input levels. the compression of an 80db input dynamic range  into the 40db output dynamic range corresponding to the physiological data  of figure 2.3 is thus possible with reasonable individual quality factors. the  frequency selectivity, however, cannot be as sharp as measured on biological  cochleae, since it is determined by the quality factor of a single stage.  the pseudoresonance results from the accumulation of the individual second-  order lowpass filter gains hk (s) all along the cascade [27]. if their cfs are close  enough to each other to allow their resonant bumps to overlap, their multiplica-  rive effect can lead to high overall gains at frequencies close to the pseudores-  onant frequency fpn. since the cascade consists of filters with decreasing cf,  this cumulative effect occurs only on a limited number of filters, for which the  individual gain is substantially greater than unity. stages with a cf much  higher than the signal frequency have no effect on this signal (gain close to26 neuromorphic systems engineering  50  40  ] 20~  10  0  -10  -2 0 ./ ,~ . ,' /  i â¢ /" .' /./,"  x " i '/ i i i  " o  20 40 ," }~ 80 i00 120  ~ ~ /  , / ; ,' /  io00x [db spl]  ~ f=-i .5kh z<cf [] f= 3kh z<cf a f= 6 khz<cf  o f=-i 0khz<cf â¢ f=-i 8khz=cf model  i~kl~l y=x/2 y=xqm ax  figure 2.3 compression curves; the boldface curve plots equation (2.9) for a quality factor  q = 180. the factor 1000 on x represents the gain between sound pressure level (spl)  and bm displacement at the input of the cochlea. the physiological data are from [17].  1), whereas the stages with a cf lower than the signal frequency rapidly at-  tenuate the signal since their individual gain drops with 12db/octave. the  pseudoresonance therefore  â¢ strongly amplifies frequencies f ~ fpr,  â¢ sharply attenuate frequencies f > fpr, and  â¢ has no effect on frequencies f < fpr.  the decreasing cfs from the cascade's base toward its apex are necessary  to allow the propagation of a signal modelling the bm travelling wave from the  input of the first stage to the stage where it accumulates to its peak amplitude  before rapidly fading away further in the cascade. in this model, the cutoff  frequency of each stage in the cascade, being defined by its cf, decreases ex-  ponentially by one octave every b stages from fco at stage 0, which is expressedactive cochlea 27  by  fc~Â¢ -- f~o2 -~/~. (2.10)  the transfer function between the input vibration at stage 0 and the bm  displacement at the output of stage k is given by the product of the transfer  function of all individual stages between the cascade input to the output of  stage k.  these transfer functions are advantageously expressed as functions of fre-  quency normalised to the cf of stage k. using equation (2.10), the transfer  function between the input of the first stage in the cascade and the output of  stage k is thus expressed by  k k  1 (2.11) gk( ) = = i-[ t=0 t=0 1-~t 2~ +2~2~-  where gt = f/fck is the frequency normalised to the cf of stage k.  the normalised pseudoresonant frequency ~tpn at the output of stage k can  be estimated by finding the maximum gpn = i igk (~gn)ll of the amplitude gain  function iigk(~)ll. the pseudoresonance involves a limited number l of basal  stages, since all stages 1 < k - l have a gain ht(gt) close to unity. therefore  the product in equation (2.11) can start with the index l = k - l if k _> l.  by a simple change of variables, the transfer function gk+a(~t) after stage  k + d can be expressed by gk (~2 a/~) using equation (2.11) in which the quality  factors q~ must be shifted to qt+a and the product must start with the index  1 = -d instead of 0. hence, provided that the stage k (or k+d ifd < 0)  is far enough from the base to allow full pscudoresonance to build up, and  with the same quality factor for all stages participating to pseudoresonance,  g(x) = gk(~ = 2 ~) expresses either the transfer function after stage k at a  frequency x octaves away from its cf, or the transfer function after stage k + bx  at the cf of stage k.  figure 2.4 shows the accumulation of individual amplitude gains t lh~ (~)ii on  a growing amount of basal stages (plot a) and the accumulated gain i iga (~)ii  for several cases of resolution b (plot b) and quality factors q (plot c). an  important point to notice is that for 4 < b < 10 and for any resonant q, the  normalised pseudoresonant frequency ~-~pr is about 0.4 octaves higher than the  normalised peak frequency ~p of the stage k alone. since gk (f~) also measures  the gain of the stage k + b log2 ~ at the cf of stage k, this implies that the  stage k + dpn with highest amplitude gain at the cf of stage k is located dpr  stages apicalward from stage k, where  dpr = b(0.4 + log~ v/1 - 1/2q 2)  is the pseudoresonant distance. for an individual quality factor q between 1.5  and 2, the pseudoresonant distance lies between 0.22b and 0.30b, while the pseu-  doresonant gain gpr easily reaches the 45db required to match physiological  data as shown in section 2.2.4.28 neuromorphic systems engineering  50  40  30  20  10  0  -10  -20  -30 q=2, b=6  -1 i  f=t pk i  0 1  f/fck [oct] c~ -~ 40~  ~0  0 -i 0  f/fck [oct]  40f)~, l>>  0 0.4 1  f/fpk [oct]  figure 2.4 pseudoresonance; (a) accumulation of amplitude gain of l individual stages  basalward from stage k ((~k k = 1-iz=k-l hi); resulting amplitude gains for several (b)  resolutions (stages per octave) and (c) quality factors.  2.2.6 quality factor control loop  in order to locally control the pseudoresonant gain gk (~) at the output of stage  k, only the quality factors of the stages participating to the pseudoresonance  have to be controlled. a natural way of doing this, is to distribute the signal  controlling the quality factor over this range.  the spatial distribution of the quality factors q(d) = qk+d on the pseu-  doresonant portion around stage k depends on the distribution of the control  signals it(d), in response to the distribution ~vb,~(d) of the peak bm velocity  signals at the output of stages k + d. similarly to the single stage presented  in section (~2.2.3), the control signals it(d) are made proportional to the peak  amplitude yvb m (d) of the differentiators outputs, but they can interact between  stages, which is expressed by the spatial convolution product  it(d) = ~ f(d- d')~zvb,,,(d'), (2.12)  d , = - c:x~  where f(d) represents the spatial distribution of the feedback gains around  any stages k in the cascade. in the single stage control loop of section 2.2.3,  f(d) = f for d = 0 and f(d) = 0 otherwise.  as mentioned previously, the spatial distribution around stage k of the cas-  cade's output responses at the cf of stage k can be expressed using the gainactive cochlea 29  function of equation (2.11) in which ql+d must be used instead of qt. since  the equivalence between stage index shift d and relative frequency ~t applies  also to the differentiators, the spatial distribution ~vb,~(d) around stage k can  be expressed by  ~vb,~(d) = iigd(d, ~)ll~di~o, (2.13)  where iigd(d, f~)ll = iigk(~2d/~)]l~2d/~ is the amplitude gain of the cascade  after stage k + d, including the differentiator dk+d, at frequency ~fck, while  ~'din0 is the peak amplitude of the cascade input signal.  hence, using equations (2.5), (2.12), and (2.13), the distribution of the qual-  ity factor q(d) on stages k + d can be calculated by  l i 2q,~ax-i ] (2.14)  q(d) = ~ 1+ 2qmaz~dmogol(d) + 1 '  where the open loop gain distribution gol(d) = e~,_-~ f(d-d')iigd(d', ~t)i]  is the spatial convolution product of the feedback gain distribution function  f(d) and the cascade amplitude gain distribution llgd (d, ~t)ii.  since iigd(d)ll--and, thus, gol(d)--depends on the quality factors distri-  bution q(d), equation (2.14) has the form of a nonlinear system of equations  q(d) = q(d, ~d~o, q(-l), ...q(m)). the size of the system to solve is given  by the range -l > d > m for which ~d~ogol(d) substantially modulates the  quality factors q(d).  the amplitude gains iigd(d)i i with identical quality factors q(d) = q vd  can be approximated using the normalised gain gd (d) o( q-pc i i gd (d + dpr)ii  which has its maximum at d = 0 (figure 2.5, plot a). the modulation of  the quality factors by control signals ic(d) having the same distribution as the  normalised gain gd(d) and amplitudes varying on the expected 40db output  dynamic range (figure 2.5, plot b) yields its maximal effect at a quality factor  modulation distance dqm close to the pseudoresonance distance dpr. figure 2.5  (plot c) shows that on a b = 6 stages per octave cascade, the amplitude gains  of the stages k + 1 and k + 2 are most attenuated. this quality factor best  modulation distance dqm between 1 and 2 is similar to the pseudoresonance  distance dpr lying in this case between 0.22b = 1.32 and 0.30b = 1.8.  this means that the stage k + dpr has the best response fzvbm(dpr ) to an  input signal at the cf of stage k, whereas the same stage k + dqm is the most  sensitive to a control signals i~(d) having the same distribution as the peak bm  velocity signals ~vb,~(d), but shifted by --dpr in order to have its maximum  i~(0) at stage k. since the most efficient quality factor adaptation is performed  for the highest open loop gain, the feedback gain's spatial distribution f(d)  should have its maximum at d = --dqm. in conclusion, the output of any  stage in the cascade must control the quality factor of a stage located at a basal  distance --dqm corresponding to a cf increase between one sixth to one third  octave (figure 2.6).  as mentioned in section 2.2.3, the feedback gain distribution f(d) models the  ihcs and the msr blocks. the distribution of f(d) can thus model interactions30 neuromorphic systems engineering  1.8  -2  1.6  ~-,~  ~' 0 1.4  i~  -6 1.2  -8  -2 -1 0 1 2 0.~ -2  d/b i b)  -1 0 1 2  d/b  c) 45  -~ -~0  45 d=l  ~1=2,  ~\ \\\\ 2  ,=1 d=2  \/\\\\\\\\ ~ d=2  ~-/j - 10 ~  -~ -,0[\\\\\\\\\\ 2  f/fck[oct]  figure 2.5 most sensitive stage to quality factor modulation; (a) normalised gain function  for several quality factors; (b) resulting quality factors for several output level; (c) gain curves  after stages/~ + d.  between stages that may occur at higher levels of auditory processing, such as  lateral inhibition, diffusion and nonlinear spatial filtering. as for the basal shift  -dqm, its biological counterpart is plausible, since afferent fibres of the ihcs  are paired with the afferent fibres of the ohcs at one seventh to one third  octave higher cf [6]. the operating point of the ohc is likely to provide theactive cochlea 31  auditory brainstem with information relative to the signal level, which has been  lost by the gain adaptation.  &gdd  --0 0 0  -d qm i  sta~e k t d 0--0~ dpr  i  figure 2.6 cascade quality factor control loop; the feedback gain distribution f(d) shifts  basalward at a distance dqm ~ dpr the output signal with highest peak amplitude occuring  at stage k + dpr.  2.2.7 closed loop amplitude gain  in order to validate the quality factor control on a pseudoresonant cascade,  digital simulation cannot be avoided to perform the closed loop analysis of  such a nonlinear system. the computer model we propose analyses the dynamic  behaviour of the peak amplitude at the outputs of the cascade when excited  by an input signal at a given frequency.  the system to solve is given by equation (2.14). the model uses the simplest  feedback gain distribution f(d) = f for d = --df and f(d) = 0 otherwise,  where df is an integer such that 1/6 _< df/b <_ 1/3, according to our conclusion  about the best quality factor modulation distance dqm. the term goÂ¢(d)  in equation (2.14) equals in this case fiigd(d + d~,f~)t [ and it depends on32 neuromorphic systems engineering  the quality factor distribution q(d), since [igd(d Ã· df, ~)[i is the product of  amplitude gain functions of stages with the different quality factors that follow  the distribution q(d). an iterative loop must estimate the distribution q(d)  on a range of stages for which q(d) is substantially modulated by the feedback  loop.  this iterative loop can advantageously be driven by the averaging performed  in the msr blocks. the msr is estimated by averaging the ihc output v~u~ of  every stage in the cascade according to a first order lowpass filtering 1/(s~-~ + 1).  using equation (2.12), the quality factor control signal i~(d) can be described  in the time domain identically for every stage by  5i~(d,t) _ f~/~b,~(d + dr,e) -- i~(d,t),  za 5t  where the differentiator's output distribution ~/~b~(d, t) = i[gd(d, ~, t)[ll)din0(t)  is controlled in time by the input peak amplitude vdi,~o(t) and in space and time  by the distribution at time t of the amplitude gain [igd(d, ~, t)[i of the cascade.  figure 2.7 shows the simulation result of such a closed loop analysis, per-  formed on a b -- 6 stages per octave cascade with a quality factor limited at  q,~ =- 1.8. in plot a, the amplitude gain after stage k Ã· d at the cf of stage  k is represented as function of the stage shift d relative to stage k. the index  d equivalently represents the frequency in b-th of an octave relative to the cf  of stage k. the peak amplitude gain adapts from 40db at -80db input level  to nearly -10db at 0db input signal, yielding the 50db dynamic compression  also measured on biological cochleae. the shift of the peak frequency ~pr  from one third an octave to minus half an octave relative to the cf as well  as the frequency selectivity decrease for an increasing input level are also in  accordance with physiological data. plot b, in addition to the level adaptation,  shows the level transition enhancement expected to improve the detection of  relevant features in the speech signal.  2.2.8 stability  the closed loop analysis has been performed on the amplitude gains of the  cascade stages with a single frequency input signal. this analysis, however  does not take into account the phase of the control loop which may cause the  system to become unstable.  instability can occur locally in a single stage k if in equation (2.6) aiqmax > 1  leads to a negative value of qmax. in this case, according to equation (2.5), the  quality factor q of this stage becomes negative with ic -- 0, that is, in absence of  any input signal. as a result, the real part of the poles of the transfer function  hk (~t) becomes positive. any perturbation would cause its output signal vdbm  to oscillate with an amplitude growing exponentially. thanks to the quality  factor control loop, as the amplitude of vdbm grows, the control signal ic grows  proportionally until it ~yields a quality factor q -- -~ for which vdb~ stabilises  to a peak amplitude vos~. this amplitude is directly related to the controlactive cochlea 33  40  30  20  ~ 10  l9  -10  -20 a)  f  -3b b=6,qmax=l .i  j  -2b -b df b  -2o  ~ -40  x  -60  -80 yidb]  i  i  i  ..... i ~ ..... d=df  rxld_bi.,' ..... :  o ~ ~'o ~'s ~'o ~'s ~'o ~'~ 4'o ~s ~o  t'  figure 2.7 closed loop analysis; (a) amplitude gain for an input signal at the cf of stage  k with input levels between -80db and odb; (b) time response of the output level for a input  level increasing then decreasing between -80db and odb. the time t ~ is normalised to the  time constant t a of the msr lowpass filter.  signal icosc = -1/2q,~a= yielding q = -oc. the resulting output oscillation  at the output of the second-order stage hk+d~- has therefore a peak amplitude  ~osc -- c~iqrnax -- i  f  this oscillation should however not propagate apically in the cascade. since  its frequency equals the cf of the oscillating stage, it is rapidly attenuated by  the sharp cutoff at the following stages.34 neuromorphic systems engineering  this sustained oscillation caused by a misfunction of the ohcs was observed  in the biological cochlea as an otoacoustic emission, which strongly suggested  that an active process performs the automatic gain control in the cochlea.  the stability of the quality factor control loop itself has to be verified. in  order to estimate the open loop gain, the nonlinear feedback loop can be lin-  earised around a given operating point. we assume for this analysis that all  stages k to k + df participating in the control loop have the same quality factor  q(ic). for notation simplification, let us use x for the input of stage k, y for the  output of the differentiator k + df, and c for the signal controlling the quality  factors of the stages k to k+df. using the feedback loop gain c/y = f/(s'~a+l),  the linearisation of y = ddf 1]dd~o hd(c)x yields (see appendix a)  df  :  d=0  where the linearised feedback gain _~(c) is given by  2- ddhd(c)  =  (1 + c) 2 1 + s~'a  and r(y) expresses a piecewise-linear full-wave rectification r(y) = lyl or half-  wave rectification r(y) = y if (y > 0) and r(y) = 0 otherwise.  ~-fdf hd as shown in figure 2.8, the open loop gain ~ddf lid=0 features a nonlin-  ear element (the rectifier) which generates harmonics. these harmonics, as well  as the fundamental frequency of the input signal are to be filtered by the msr  filter which must therefore have a time constant ta large enough to remove the  lowest frequencies in the input signal. the stability of the loop will depend on  the amplitude and the phase of the oscillation remaining at the output of the  msr lowpass filter, which we will address in section 2.3.4.  2.3 vlsi implementation  in this section we detail the analogue vlsi implementation of the the major  building blocks of the proposed model. all circuits are designed for a standard  cmos technology, including the bipolar transistors implemented as compatible  lateral bipolar transistors (clbt) [32, 20].  2.3.1 bm second-order stage with differentiator  the second-order stage cascade modelling the bm displacement as well as the  differentiators converting its outputs into the bm velocity all along the cascade  are the same as described in [32] (figure 2.9).  the crucial point of this circuit is the control of its quality factor. according  to equation (2.3), the quality factor of any stage Â£ is driven by the ratio of the  ohc ota's bias current iqk and the bias currents i,k of the bm otas which  determine the cutoff frequency of the stage ~. the modulation of iqk/i~ allowsactive cochlea 35  a) hk h hk+l t  q(c) i~ c t_  f  s~a + _-, io. 0 1  ~ hk(c) i "ihkÃ·  <c>i  b) i d k+df i  zddhd(c)  s~a+l  figure 2.8 linearisation of the quality factor control loop; (a) portion of the cascade  including; an individual quality factor control loop; (b) linearisation of (a) for small sig;nal  variation.  therefore to control the quality factor of any stage in the cascade independently  of its cf.  because of the limited slew rate of the bm otas due to their load capaci-  tance c~, large signal instability may occur when the ota's outputs saturate  to their bias current. according to the analysis shown in [9], large signal tran-  sients always recovers if iq/i~ < ilsi, where ils1 lies between 1 and 2 for any  common mode voltage at the ota input between the positive voltage supply  v+ and the ground v_. using the definition of ~ in equation (2.3), the second-  order section recovers from large signal instability if gmq/2gm~- < c~ilsi. on  the other hand, small signal stability is ensured if g,~q/2g,~- < 1. therefore,  with ~ilsi > 1, large signal transients always recover as long as the small  signal stability is ensured. if both bm otas and ohc ota have the same  linear input voltage range i~-,q/g,~,q in weak inversion, this condition is never  respected. the bm otas are thus degenerated in order to have a linear inputaq& 'dia,t!aaadsaa v/lo dho aqa pun sv&o inh a~t~ jo o i pun ~i ~uaaana  sn!q at d su!:~naaua~ s/lh~id atla su!pniau! doo i anau!isuna a ~ xq palioaauoa aq  una osnas xun aq Â£duapuadapu! ao~anj x~!innb oqa ~unninpotu ~i/bi o!ana aq&  ioaauoa aoaav, t Â£a~.[vno g'ug  â¢ p~ sjo~,s!suej~, at n ,~q po~e  -jaua~ap o jr sv.]_ 0 ~] ali~ j.o ~!ed iep, uo~ajj!p atp, :~a~,l! j ssedt~o i jap~o-puo~a s 6"e ajn~!-i  til~ mq^ a  -as!saad o:~ uo!~nlidso inu~!s  asan i a,t~ satoiin aaaau 'doo i ~avqpaaj aoaa~j x~ivnb ara xq anita s~ra oa pa?~m~[  s~ ran~ pue '(i + u)/~ig saravaa ~i uar~ suunaao xa~i~q~asu~ ieu~s ii~ms ara  a~qa sa!idm! qanm 'i ueqa aaa~aa$ sÂ£~l~ s~ u aoaag adois or& "[06 'gg] sv&o  ~a aqa jo a!~d i~auoao~p oql aa~aaua~ap aeql p& saoas~su~aa poaaouuoa-apo~p  aqa 3o ([og] aas 'aoas~su~al 80~ jo iapom aha) aoaag adois aqa s~ u aaaq~  , g ~ i+u  su~pla~x 'v&o dho oqa uvqa aosa~ i sam n (i + u) o~u~a o~vlloa  dnin~nidn~t sin~ksxs oihdhoiaiohfi~n 9~active cochlea 37  circuit proposed in figure 2.10 generates iq and iv from the currents iqmax,  ic + i0 and iv according to equation (2.4). the currents iq,~ax and ic + i0  are imposed at the collectors of the respective clbts, whereas iv is imposed  by the base voltage v~ picked on the resistive line controlling the exponential  decrease of the cf along the cascade. the common voltage vq on the emitters  of the clbts tq and tq,~ is controlled by the mos transistor te which sinks  these emitters with a current that allows their respective collectors to sink the  required currents iq and iq~a~.  an important point to analyse is the effect on the quality factor of the  dispersion of the translinear loop input currents. according to equation (2.6),  for a m~imal quality factor qmax ~ 0 at every stage, the condition aie~ < 1  must be respected for every stage. allowing variations of a ~ = ~iqmax within  the range ~xa ~ leads to a nominal m~imal quality factor  1 + ~max ~ - , (2.~) 2~amax  where ~ ~ Â¢~ ensures q~(a' + Â¢~a') > 0. however a negative variation  within the same range can be shown to reduce q~ by up to a factor two:  ~a~(~'--z~a~a') = ~ ;~'~ ~maxk 2"  the variations ~ of ~ depend on the matching of n, iqma~ and io all along  the cascade, which may result in important variations of ~'. equation (2.15)  shows that ~ 33% variation of ~ allows a nominal q~ = 2, with a worst case  ~ (n + ~)~. q~ = 1 while ensuring q~a~ > 0 in any case, with i0 = ~  i~, let iqmax{, i c +10{ ,  v~  %  resistive line  i i ,te  figure 2.10 translinear loop; the emitter current of transistors tq and tqm is sinked by  the transistor t e.38 neuromorphic systems engineering  2.3.3 ihc rectifier  the function of the rectifier is to generate a dc component from the bm  velocity signal. the bm velocity is available at the level of the bm second-order  filter as the voltage difference vvbm. therefore an ota suffices to generate a  current ivb,~ ---- g,~avvbm. this current is then half-wave rectified (hwr) by a  single diode at the output of this ihc ota (figure 2.2). full-wave rectification  (fwr) can be performed by connecting its output to the output of a rectifier  generating the opposite half wave of the current iihc. the two hwrs differ  thus by the inversion of their ota input (figure 2.11a).  the delicate points to master in this circuit are related to its need to rectify  a very small current (few hundreds pa) varying at frequencies up to the highest  cf of the cascade, i.e,. 5khz. the current must be as small as possible because  it will determine the large time constant ~-a (about 10ms) of the msr lowpass  filter together with its capacitance ca, which must in turn be small enough  (few pf) not to waste chip area.  the time to switch from a negative current --i~b,~ to a positive one i,b,~  depends on the charge accumulated on the parasitic capacitance cr~ at the  output of the ihc ota. this switching time tsw can be estimated roughly by  c.~ av.~  - --, (2.16) tsw ivbm  where av,~ is the excursion of the ihc ota output voltage.  in order to limit av,~, the conductance at this node must be large enough  for both positive and negative current ivb,~. this means that a negative wave  must be sinked from the node vn through a transistor tn whereas the positive  one is sourced through the transistor tp on the node vr. the voltages v,~ and  vp must be chosen carefully in order to minimise the leakage current flowing  between their respective nodes when i,b,~ = 0. the transistor tn and tp are  n and p type respectively in order to avoid their gate capacitance at the ihc  ota output due to the drain-gate connection of the transistors. the bulk of  transistors tp and tn are connected to the voltage supply v+ and to ground  v_ = 0 respectively.  using the ekv model of the mos transistor [20], the drain currents ip and  in in the respective transistors tp and tn as function of v,~ for given vp and  vn are represented in figure 2.11b on a logarithmic scale i -- ln(i/idon). all  voltages are normalised to the thermal voltage ut = kt/q (26mv at 300k).  the reference current idon ---- is~ exp(--vton/nn) depends on the technologi-  cal parameters of the transistor tn, whereas the term a = ln(idop/idon) --  v+(np -- 1)/np expresses those of transistor tp relative to transistor tn. the  voltage excursion av,~ at the output of the ihc ota for a current varying  from i~bm to --i~b,~ can be estimated by  ivbrn avm = 2utln iv---~, (2.17)  where ipo is the leakage current.active cochlea 39  vvbrr  a) lihc{7  v' i=ln ~1~ ~lihc  n ^ don ,~  nn "~oe. avm _~'  /  b) i~o- ~  o ~  a_)h~ i~p i"p irn li~,~ ,,p v m  figure 2.11 ihc rectifier; (a) a hwr is represented in the grey frame whereas the com-  plete structure implements a fwr; (b) forward and reverse currents in the transistors t n  and tp are represented on a log scale as function of the ihc ota output voltage normalised  to u t.  allowing a leakage current less than 1% of ivb m and a switching time shorter  than a quarter period at highest cf (5khz) and assuming a capacitance cm =  0.1pf, the ihc ota output ~vbm current must be larger than 480pa.  the voltages vn and vp determine the value of the ihc ota output voltage  v,~0 when ivb,~ = ip -- in = 0. the diagram in figure 2.11b shows that this40 neuromorphic systems engineering  point is given by 2vmo = v~/n~ + vp/np - a, or  v,~0-- ~ ~+--+ v+ + in + -- , (2.18)  rtp np ~ isp np nn j  where is~,p, vton,p and nn,p are respectively the specific currents, the nor-  malised threshold voltages vto,~,p and the slope factors of the transistors t~,p.  the same diagram shows that a nominal output current ivb m at least k  times larger than the leakage current ipo imposes  2 v n vp izvbrn vto p vto n np -- 1 < in -- + + + v+ - 21nk. (2.19)  rt n np isnxsp ~p 71 n ftp  the rectified current i~hc can be sourced from node vp, but the opposite wave  of l[vbm can also be sinked from the node v~. the rectifier's output voltage  vp,~ must respect the condition (2.19) where the voltage v~,p on the opposite  node is imposed. the ihc ota output node voltage at operating point v,~0 is  then determined using equation (2.18). in order to ensure the saturation mode  of both transistors tp and t~, their respective drain-source voltage v~ - vr~0  and v,~o - vp must be larger than 3 to 4 ut.  with nn -= np= 1.5, v+ = 5v, vto~ = 0.8v, vto p : 1v, isnisp =- (80na) 2,  is,~ ,~ 3isp (tp and t~ are minimal size transistor) and a maximal leakage  current k = 100 times smaller than a nominal current ivbrn ~- 480pa, the right  hand side of the condition (2.19) equals 2.36v/ut. using the node vp as the  rectifier output with the node v~ at v+ imposes vp > 1.46v and vmo > 3.07v,  whereas a grounded node vp requires v~ < 3.54v and v,~0 < 2.09v.  in both case the node which sinks/sources the half wave which is not further  used is at a fixed voltage, whereas the node which sources/sinks the rectifier  output must have a resistance low enough to minimise its voltage variations  within a few ut.  2.3.4 msr lowpass filter  in order to estimate the mean value of the bm velocity, the msr low pass  filter separates the dc component [ihc from the harmonics generated by the  rectification of the bm velocity signal ivbm = g,~avvb,~.  with a rectifier's input signal at frequency 1/2~r~-k, the frequency component  with highest amplitude (the fundament~tl for hwr and the first harmonic for  fwr) at the output of a msr first order lowpass filter 1/(s~'a + 1) has an  attenuation  ah1 = --~/l+(za/tk) 2 hwr  af2 = ~v/1 + 4(~-a/tk) 2 fwr  with respect to its dc component. this attenuation corresponds to a sig-  nal/noise ratio (snr), since the dc component is the information carrying  signal and the remaining oscillations are to be considered as an additive noise.active cochlea 41  this snr is thus larger using a fwr than using a hwr. the snr improve-  ment af2/ah1 lies between 2.35 for ~'a << t~ and 4.71 for 7a >> t~ and it equals  3.72 for ~-a = ~-~.  the minimal value of the time constant ~-~ of the msr is determined by the  minimal snr a,~in ensuring the stability of the quality factor control loop as  mentioned in section 2.2.8:  /~i "2 ~2 7kv~-~irnin -- 1 hwr  ta _> ~ /4 a2 4 v~mi, - 1 fwr (2.20)  if a snr lower than 2/7~ or 3/2 is allowed, the msr average is not even  necessary using a hwr or a fwr respectively. for any snr larger than 3/2,  the required msr time constant is always more than 3~ times smaller using a  fwr instead of a hwr. at the price of doubling the area of the rectifier, the  msr capacitance ca (thus its area) can be reduced by a factor 10 for a given  msr transconductance ga. hence, the rectifier-msr block is in any case at  least 5 time smaller using a fwr rather than a hwr.  the msr lowpass filtering is implemented by injecting the rectifier output  current i~hc into a capacitance ca in parallel with a conductance ga. the  voltage va on ga or its current i~ is proportional to the mean bm velocity signal  (figure 2.2). it will thus be used to generate the current ic to be injected into  the translinear loop controlling the quality factor as described in section (2.3.2).  the conductance ga can be implemented with a single mos transistor (fig-  ure 2.12a) or with an ota (figure 2.12b). in both cases the value of the  linearised conductance ga is determined by the dc current ia flowing through  it, thus in weak inversion  { i~ ( ~ / ~os  g~(ia)= ~ 1-~ ota ' 2nut i b  where ib is the bi~ current of the msr ota g~ and the dc current i~ cor-  responds to the mean rectifier output current [ihc which is proportional to the  mean bm velocity. therefore the msr time constant ~ = ca/ga depends  on the mean bm velocity. with a single mos, the time constant grows to  very large values when i~ becomes small, which means that when the mean  bm velocity falls close to 0 the msr memorises its previous value: to avoid  this problem, the normalising current i0 injected into the translinear loop to  limit the quality factor to q~ (section 2.3.2) can be injected before the  mrs with a ratio i~/io = [ih~/i~. the variation of t~ will be limited be-  tween 7ama~ = canut/i~ and 7,min = tamaz/(1 + [ihcmaz/i~). however,  using an ota to implement the conductance g~, the injection of i~ at the in-  put of the msr is no longer required and the time constant varies between  w~mÂ¢, = c~2nut/i~ and ~m,~ = z~i~/(1 - [yh~/i~), which tends to very  large values for bm velocities yielding mean rectifier output currents [ih~ close  to the saturation current ib of the msr ota.42 neuromorphic systems engineering  we assume that the maximal bm velocity yields a rectifier output current  having its mean value [ihcmax = 0.531b, which corresponds to a msr ota  output current ia(va) 10% lower than its value gava with a transconductance  ga linearised around ia = 0. in this case, the msr time constant dynamic  range tama=/~-a,~n equals about 1.4. if the single mos transistor implements  the conductance ga, the time constant dynamic range ~-a,~a~/~-a,~n is equivalent  to l+ic,~a=/io or, using equation (2.5), (1-1/2qmaz)/(1-1/2q,~n). therefore  a quality factor varying between 1/x/~ and 2 imposes a time constant dynamic  range of 2.56.  the advantage of the single mos transistor resides in the possibility to  mirror the current ia = [~h~ + i~ to the msr output current i~ + i0 with a  current gain mr = gmf/ga = ic/fihc. the area of two otas can therefore be  reduced to the area of two single mos transistors.  nevertheless, at the price of using the area of two otas instead of a current  mirror, the area of the capacitance can be reduced by a factor 2 for the same  time constant ~-a, since the transconductance of an ota is twice as small as  the transconductance of a single mos transistor when biased with the same  smallest possible current i~ = ib. if the capacitance uses much more area than  two otas, the implementation of the msr with 2 otas would even be smaller  than with a single current mirror.  the degeneration by a diode-connected transistor at the source of the mir-  ror's reference transistor or the ota's differential pair transistors increases  once more the transconductance by a factor n + 1 at the price of two or four  additional transistors respectively, since in this case the mirror's output tran-  sistor or the the feedback ota's differential pair must also be degenerated to  keep the transconductance ratio gmf/ga independent of the mean bm velocity.   +,01 0 . lk  ~ ga-ii-i-i[-gmf ~ ~1 r. ~ oaj_ ,oÃ·1  figure 2.12 msr lowpass filter; implementation using (a) a single mos transistor and  (b) an ota for the conductance 9a.  according to equation (2.20), the msr time constant ta must be large  enough to sufficiently attenuate the oscillations at the rectifier's output for  the lowest cf in the cascade. using degenerated otas with a capacitance  ca = 10pf, a 200hz cutoff frequency can be obtained with a bias current  ib = 1.76na. with a maximal mean rectifier output current [~hcma~ = 0.53ib  as assumed above, the expected 40db dynamic range of the bm velocity al-active cochlea 43  lows to estimate a minimal mean current [ihc,~ = 9pa at the output of the  rectifier, which reaches the limits of the leakage current of mos transistors.  since iih~ is produced by the rectification of the ihc ota output current  l,bm, the minimal peak value f,b,~,~n of the ihc ota output current, assumed  to be a sine wave, amounts to 30pa for a hwr and 15pa for a fwr. in sec-  tion 2.3.3 we showed that the current ivb m must be larger than a few hundreds  pa (480pa in our example) to ensure a switching time short enough to allow  frequencies up to 5khz to be rectified. the leakage current we allowed was 100  times smaller, thus only 7 and 3.5 times smaller than the respective minimal  peak ihc ota output currents estimated above. with f.b,~ = 7ipo = 30pf,  equations (2.16) and (2.17) give a switching time allowing a maximal frequency  of 740hz to be rectified, instead of the required 5khz calculated in the same  conditions with f,b,~ = 480pa. the ihc ota has its minimal output current  imposed by the high frequency limit of the rectifier given by the highest cf in  the cascade, whereas the maximal mean rectifier output current is imposed by  the large msr time constant required by the lowest cf in the cascade. there-  fore, the currents biasing the ihc and msr otas could advantageously be  graded similarly to the bias current it defining the cf of the bm second-order  stage. another solution could consist of mirroring the rectifier output current  into the msr with an attenuation large enough to make possible the large msr  time constant together with fast rectifier switching.  the minimal cutoff frequency of the msr lowpass filter is anyway limited  by implementation constraints such as reasonable size and currents larger than  the leakage currents. in order to sufficiently attenuate the ac component of  the rectifier output signal to ensure the quality factor control loop stability  expressed in section 2.2.8, the difficulty to create a large time constant with a  relatively large current and a reasonable size capacitor can be circumvented in  our application. the purpose of the msr lowpass filter is to measure the mean  value of the bm velocity signal, rectified by the ihc. since the bm sections are  modelled by second-order stages, the bm velocities measured at adjacent stages  have a phase difference close to ~r/2 near their cfs for which they also have  the largest amplitude. therefore 4 half-wave-rectified or 2 full-wave-rectified  sine waves shifted by a quarter period can be used to generate a signal having  its remaining oscillations attenuated sufficiently to implement the msr with a  higher time constant. this "double-wave rectification" can increase the snr  of the mean bm velocity signal similarly to the improvement brought by the  fwr compared to the hwr.  the design of the msr lowpass filter well illustrates the difficulties which  generally occur in implementing vlsi analogue systems processing low fre-  quency signals typically found in biological models.  2.3.5 feedback gain distribution  according to equation (2.12), the current icÃ·i0 at the output of the msr of the  stage k must control the quality factor of the cascade through the distributed44 neuromorphic systems engineering  feedback gain f(d) having its maximal value at d = -dr. the simplest distri-  bution f(d) = f at d = -df and f(d) = 0 otherwise is easily implemented  by connecting the output node sourcing ic + i0 in the msr of stage k to the  collector of the clbt transistor tc in the translinear loop of stage k - dr.  the disadvantage of this fixed connection resides in its limited flexibility,  since df has been shown in section 2.2.6 to lie between b/6 and b/3 for the  most efficient quality factor control loop. the number of stages per octave b  can be adjusted by the cfs of the first and the last cascade stage through the  resistive line [32], but it should not extend too far outside the limits 3df and  6dr.  2.4 conclusions  a model of an active cochlea allowing 40db dynamic compression has been pro-  posed. this model was specially developed for an analogue vlsi implementa-  tion and this constraint highlighted strong functional and structural analogies  with its biological counterpart. these analogies support the validity of the  model.  a careful analysis of the model supported by realistic computer simulations  has been presented. together with the available exploratory research already  done on the topic, we hope this work will facilitate a sound implementation of  the first analogue vlsi active cochlea known to date.  the model is based on the original lyon and mead's second-order low pass  filter cascade which still proves to be a very resourceful concept. hopefully,  the additional features we added can also be used as a framework general  enough to progress in further modelling of the auditory pathway. for instance,  neural processing performed at higher levels of the auditory brainstem could  be included in the feedback block of the proposed quality factor control loop.  as preprocessing element for an automatic speech recognition system, the  frequency selective transient enhancement due to the quality factor control  is expected to better model the relevant features of the speech signal. an  interface is therefore required on top of the model to provide a classifier with  the appropriate sequence of acoustic vectors.  finally, the possibility of faithfully modelling the cochlea with low power  analogue vlsi systems also opens the way to a new generation of hearing aids  and cochlear implants.  appendix: a  the quality factor control loop  dr  y = ddf h hd(c)x  d=o  is linearised by  _ by dy /x~ ~x a~ + ~cc a~active cochlea 45  df  : dd~ 1--[ hd(c)ax  d=0  df  Ã·  d=o  since 1 1 hd(c) _  pd(c) s2~+~ +1'  the partial differentiation of the product becomes  ~ ~o 1 df 1 df ~ pd(c)  = pd(c)--d=op~-~d=oii e pd(c)  and, remembering that dd = 8td,  ~ ~qd(c)  ~cpd(c) = dd q~(c)  since qd(c) = q(c) vd was ~sumed, and using equation (2.5),  ~q~(~) ~ ~ qma~  _ ~(~) (~ + ~)~  can be removed from the sum  df  e d=o pd(c) 2 1 df  qrnaz (c + 1) 2 e ddhd(c).  d=0  fr(y) depends linearly on the rectified finally, since the control signal c - sra+l  output signal r(y), ac f ~r(~)a allows to close the loop  -- s~-~+l 5y y  df  : ii  d=o  with a linearised feedback gain  2 1 df  /~ = c -- qm.-----~- ed=0 ddhd(c)  (c + 1) 2 8t a ~- 1  and where r(ay) = ~yy) ay for a rectifier r(y) which implements lyl or y if  (y > 0) and 0 otherwise, since in this case ~yy) is a step or a sign function  respectively and the operating point y = 0 is imposed by the differentiator  dd f â¢46 neuromorphic systems engineering  references  [1] h. bourlard and n. morgan. connectionist speech recognition: a hybrid  approach. kluwer academic publishers, boston, mass, 1994.  [2] e. fragni~re, a. van schaik, and e. a. vittoz. linear predictive cod-  ing of the speech using an analogue cochlear model. in proceedings eu-  rospeech'95, volume 1, pages 119-123, madrid, spain, september 1995.  [3] c. gigu~re and p. c. woodland. speech analysis using a nonlinear cochlear  model with feedback regulation. in martin cooke, steve beet, and mal~  colin crawford, editors, visual representation of speech signal, chapter 25,  pages 257-264. wiley, 1993.  [4] t. hirahara and h. iwamida. auditory spectrograms in hmm phoneme  recognition. in proceedings icslp'90, pages 381-384, 1990.  [5] t. hirahara and t. komakine. a computationnal cochlear nonlinear pre-  processing model with adaptive q circuits. in proceedings icassp'89,  volume 1, pages 496-499, glasgow, u.k., may 1989.  [6] d. o. kim. active and nonlinear cochlear biomechanics and the role of  outer-hair-cell subsystem in the mammalian auditory system. hearing  research, 22:105-114, 1986.  [7] j. lazzaro, j. wawrzynek, , and a. kramer. systems technologies for  silicon auditory models. ieee micro, 14(3):7-15, june 1994.  [8] r. f. lyon and c. mead. an analog electronic cochlea. ieee transo  acoust., speech, signal processing, 36:1119-1134, july 1988.  [9] c. a. mead. analog vlsi and neural systems, chapter 11, pages 179-192.  addison-wesley, reading, ma, 1989.  [10] c. a. mead, arreguit x., and j. lazzaro. analog vlsi model of binau-  ral hearing. ieee transactions on neural networks, 2(2):230-236, march  1991.  [11] r. meddis. simulation of mechanical to neural transduction in the auditory  receptor. journal of the acoustical society of america, 79(3):702-711,  march 1986.  [12] n. morgan, h. bourlard, s. greenberg, and h. hermansky. stochastic  perceptual auditory event-based models for speech recognition. in 1994  international conference on spoken language processing, volume 4, pages  1943-1946, yokohama, japan, 1994.  [13] n. morgan, s-l wu, and h. bourlard. digit recognition with stochastic  perceptual speech models. in proceedings eurospeech'95, volume 1, pages  771-774, madrid, spain, september 1995.  [14] r. d. patterson, j. holdsworth, and m. allerhand. auditory models as  preprocessors for speech recognition. in m. e. h schouten, editor, the  auditory processing of speech: from auditory periphery to words, pages  67-89. mouton de gruyler, berlin, 1992.active cochlea 47  [15] w. s. rhode and s. greenberg. physiology of the cochlear nuclei. in  arthur n. popper and richard r. fay, editors, the mammalian audi-  tory pathway: neurophysiology, springer handbook of auditory research,  chapter 3, pages 94-152. springer-verlag, new york, 1992.  [16] m. a. ruggero. response to sound of the basilar membrane of the mam-  malian cochlea. current opinion in neurobiology, 2:449-456, 1992.  [17] m. p. sellick, r. patuzzi, and b. m. johnstone. measurement of basilar  membrane motion using the moessbauer technique. journal of the acous-  tical society of america, 72:131-141, 1982.  [18] h. w strube. a computationally efficient basilar-membrane model. acus-  tica, 58:207-214, 1985.  [19] a. van schaik, e. fragni~re, and e. a. vittoz. improved silicon cochlea  using compatible lateral bipolar transistors. in david s. touretzky,  michael c. mozer, and michael e. hasselmo, editors, advances in neu-  ral information processing systems, volume 8, pages 671-677. the mit  press, 1996.  [20] e. a. vittoz. analog vlsi signal processing: why, where and how? ana-  log integrated circuit and signal processing and journal of vlsi signal  processing, 8:27-44, july 1994. published jointly.  [21] l. watts, d. kerns, r. f. lyon, and c. mead. improved implementation  of the silicon cochlea. ieee journal solid-state circuits, 27(5):692-700,  may 1992.a low-power  wide-dynamic-range analog vlsi  cochlea  rahul sarpeshkar i, richard f. lyon 2, and carver mead 3  i department of biological computation,  bell laboratories, murray hill, nj 07974  ra h ul~physics, bell-la bs.corn  2foveonics inc., i0131-b bubb rd., cupertino ca 95014  3physics of computation laboratory, california institute of technology  3.1 introduction  the dynamic range of operation of a system is measured by the ratio of the  intensities of the largest and smallest inputs to the system. typically, the  dynamic range is quoted in the logarithmic units of decibel (db), with 10db  corresponding to 1 order of magnitude. the largest input that a system can  handle is limited by nonlinearities that cause appreciable distortion or failure  at the output(s). the smallest input that a system can handle is limited by  the system's input-referred noise floor.  at the same given bandwidth of operation, a low-current system typically  has a higher noise floor than does a high-current system: the low-current  system averages over fewer electrons per unit time than does the high-current  system, and, consequently, has higher levels of shot or thermal noise [23]. thus,  it is harder to attain a wide dynamic range in low-current systems than in high-  current systems. a low-voltage system does not have as wide a dynamic range  as a high-voltage system because of a reduction in the maximum voltage of  operation. 1  low-power systems have low-current or low-voltage levels; consequently, it is  harder to attain a wide dynamic range in low-power systems than in high-power  systems. the biological cochlea is impressive in its design because it attains an  extremely wide dynamic range of 120db (at 3khz), although its power dissipa-50 neuromorphic systems engineering  tion is only about 14#w. the power dissipation in the biological cochlea has  been estimated from impedance calculations to be about 0.4#w/mmÃ35mm  = 14~w [4].  the dynamic range of the cochlea at various input frequencies has been  measured by psychophysical and physiological experiments [7]. the biological  cochlea has a wide dynamic range because it has an adaptive traveling-wave  amplifier architecture, and also because it uses a low-noise electromechanical  technology.  the electronic cochlea models the traveling-wave amplifier architecture of the  biological cochlea as a cascade of second-order filters with corner frequencies  that decrease exponentially from 20khz to 20hz (the audio frequency range) [7].  the exponential taper is important in creating a cochlea that is roughly scale  invariant at any time scale; it is easily implemented in subthreshold cmos, or  in bipolar technology.  prior cochlear designs have paid little or no attention to dynamic range.  the reports do not give their dynamic ranges [1, 13, 7, 32, 90]. however, we  know that low-power cochlear designs that pay no attention to noise or gain  control, like our own initial designs, have a dynamic range of about 30db to 40  db (lmv to 70mv rms) at the small-signal peak frequency (bf) of a typical  cochlear stage. the lower limit of the dynamic range is determined by the  input signal level that results in an output signal-to-noise ratio (snr) of 1.  the upper limit of the dynamic range is determined by the input- signal level  that causes a total harmonic distortion (thd) of about 4%. typically, the  upper limit is a strong function of the linear range of the transconductance  amplifiers used in the cochlear filter.  a single follower-integrator filter in one of our recent designs [24] had a  dynamic range of 65db (0.55mv-1000mv rms) because of the use of a wide-  linear-range transconductance amplifier (wlr) [11]. however, even if the first  filter in a cochlea has a wide dynamic range, the dynamic range at the output  of a typical cochlear stage is reduced by the accumulation and amplification  of noise and distortion from stages preceding it. nevertheless, the constant  reduction in the bandwidth of the cochlear stages along the cascade ensures  that the total noise or distortion eventually becomes invariant with the location  of the cochlear stage: noise or distortion accumulates along the cascade, but it  is also reduced constantly by filtering. however, the asymptotic noise is high  enough that, in our design [24], the dynamic range for a cochlear stage with a  bf input was only about 46 db (5mv to 1000mv rms). in that design, the use  of nonlinear gain control helped to decrease the small-signal q with increasing  input amplitude, and thus mitigated the effects of distortion; however, the  design's filter topology was not low-noise, and the nature of the nonlinear gain-  control circuit was such that the circuit increased the noise further. thus, the  effects of noise accumulation and amplification limited our ability to attain a  wide dynamic range.  in this paper we describe a cochlea that attains a dynamic range of 61db at  the bf of a typical cochlear stage by using four techniques:wide-dynamic-range analog vlsi cochlea 51  1. the previously described wlr  2. a low-noise second-order filter topology  3. dynamic gain control (agc)  4. the architecture of overlapping cochlear cascades  in addition, we use three techniques that ensure the presence of a robust in-  frastructure in the cochlea:  1. automatic offset-compensation circuitry in each cochlear filter prevents  offset accumulation along the cochlea.  2. cascode circuitry in the wlrs increase the latter's dc gain, and prevent  low-frequency signal attenuation in the cochlea.  . translinear bipolar biasing circuits provide qs that are approximately in-  variant with corner frequency, and allow better matching. bipolar biasing  circuits were first used in cochlear designs by [32].  we shall discuss all of these preceding techniques in this paper.  the organization of this paper is as follows: in section 3.2 we discuss the  architecture and properties of a single cochlear stage. in section 3.3 we discuss  the architecture and properties of the cochlea. in section 3.4 we compare analog  and digital cochlear implementations with respect to power and area consump-  tion. in section 3.5, we discuss the relationship between our electronic cochlea  and the biological cochlea. in section 3.6, we discuss possible applications of  the electronic cochlea for cochlear implants. in section 3.7, we summarize our  contributions.  3.2 the single cochlear stage  figure 3.1 shows a schematic for a singe cochlear stage. the arrows indicate the  direction of information flow (input to output). the second-order filter (sos)  is composed of two wlr amplifiers, two capacitors, and offset-compensation  circuitry (lpf and ocr). the corner frequency 1/~- and quality factor q of  the filter are proportional to ~ and v/~/i2, respectively, where i1 and i2  are the bias currents of the wlr amplifiers. the tau-and-q control circuit  controls the values of the currents i1 and i2 such that the value of 1/~- depends  on only the bias voltage vt, and the small-signal value of q depends only on  the bias voltage vq. an agc correction current ia attenuates the small-signal  value of q at large-signal levels in a graded fashion.  the inner-hair-cell circuit (ihc) rectifies, differentiates, and transduces the  input voltage to a current ihr. the voltage va controls the value of an internal  amplifier bias current in the ihc. the voltage vhr controls the transduction  gain of the ihc. the peak detector (pd) extracts the peak value of ihr as a  dc current ipk. the current ipk becomes the agc correction- current input52 neuromorphic systems engineering  vin Â© )  vhr  vpt ~r sos  vrf 0 vot o  vof 0  i i~  ~_ .......  vto i tau & q  vqo i control  > vout  figure 3.1 schematic for a cochlear stage. a single cochlear stage is composed of a filter  (sos) with offset-adaptation circuitry (lpf and ocr), an inner-hair-cell and peak-detector  circuit (ihc and pd), and a tau-and-(~ control circuit.  (ia) to the tau-and-q control circuit. the bias voltage vpt determines the  time constant of the peak detector, and thus the response time of the agc.  the peak detector is designed such that it can respond to increases in input  intensity within one cycle of a sinusoidal input at v~ ; its response to decreases  in input intensity is much slower and is determined by vpt.  the offset-compensation circuit is composed of a lowpass filter (lpf) whose  time constant is determined by vot. the lpf extracts the dc voltage of the  filter's intermediate node, and compares this voltage with a global reference  voltage vrf in the offset-correction block (ocr). the ocr applies a correction  current to the intermediate node to restore that node's voltage to a value near  vrf. the dc voltage of the output node is then mso near vrf, because the  systematic offset voltage of a wlr amplifier is a small negative voltage. the  maximal correction current of the ocr scales with the bias current i1; the bias  voltage vof controls the scaling ratio. since the restoration is performed at  every cochlear stage, the output voltage of each stage is near vrf, and offset  does not accumulate across the cochlea. if there were no offset adaptation, a  systematic offset voltage in any one stage would accumulate across the whole  cochlea.oq~ u! s~oojja aou~t.o~d~o-obt.s~a~d pi.oa~ o~l 'ot.~a ~:i ~ q~!ax '~u!~nuo~ 02~  sao~a!m ia!-iaid oq~l "[i i] s~o~s!su~a~ ~] oq~ m.a uot.0,~z!~out.[ dmnq jo onb.tuqoo~  aq~ q~nosq~ pu~ 'sao~st.su~a~ ~d oq~ ~!a uot.~aoua~ap a~ lo 9nbt.uqoo~ iaaou  aq~ q~no~q~ pauap!m :totd~n j s! a~u~a a~aut.i aq& "ao~l!ldm~ oq~ jo a~u~s :tvati.q  aq~ uop!~ o~ xi~uonbasuo~ pu~ a~um~npuo~su~/t~ :talj.tidlli~ ~omo i o~ 'o~ at d jo  p~a~su! 'iia~ owl ash am !s:o~s!su~:~ 3a aq~ jo sham oq~ a~ -a pu~ +a s~ndu~  aqÂ£ '~aghdm~ a~u~npuo~su~ aq~ jo ~n~o oq~ smoqs ~'~ a~n$~ "xgopq ~  aq~sop ii~qs om os 'pag~pom xdq~is uaaq ~q '~aaa~oq '~aiq~o~ ~no m ash a~  ~q~ h~ oq~ jo uots~aa oqÂ£ "[ii] hmop ~o~ u~ poq~sop uoaq s~q h~ aq~  ~/5raa aqÂ£ tz'~  â¢ a~ms z~olqaoo ozt.~,ua u~ lo sat.~,zadozd ilmoao aq~ ssnaslp a~  '9'g'~ uÂ°daa9 ui "s~aÂ°iq gd pu~ dhi aq; ,~ s;lnazp oq; aqlzasap am '~'~'~ uo n  -aas ~i ";lnazia ioz;uoa ~-pu~-nm ~ouilsu~z; oq; ;uasazd am ~'g'9 ~odaa9  u i "j~olodo~ ~o~i~ oq~ amm~xo am 'e'~'s uo~oas ui "~n3~o uodmd~p~-~as~o  aq~ aq~aosap am 'g'g'g uodoo s u i '~n~3 h]~ aq] ssnos~p am i'g'g uodoas  ui "i'g aan~d u~ s~noa~o aq~ jo q~a lo suma p aq~ oq~osap mou ii~qs a~  â¢ ~oq~o qa~o uo ~uopuodap di~a~ a~ salm~udp a~lioa-~nd~no dv pu~  du oq~ '~ aq~ uo a~uan~ul pilm ~ oa~q soop a~ioa d~ oq~ ph~ 'o~ioa d~  oq~ slomo[ s~ ~h191~ ao~is 'ioao~o h "0 aq~ ul suod~ilpso ao sodhlq~sul ploa~  am '~a~qpaaj u~q~ ~oq~ 'p~ojpaoj sl d~oiodo~ io~uoa-ul~ aq~ aauls  "~i!ndj!d uoneldepe-l~sjjo ~q~ ujojj s~ndu] lu~sajd~j .~o~ pue io~ sg~elloa  gql "hi s~ ~uajjn3 se~q ~ql "~Â°i luajjn~ ~q~ s~ indlno aql pue '-~ pue +~ ~je ~a~j~id  -me aq~ o~ s~ndu[ ~qi 'ja[i[id~v a~ue~npuÂ°~suejl a~ue~-jeau[q-ap[m aqi z'e a~n~d  ~Â°a +ao o -a  0  l  ' -  no o ~a  ~nÂ°l < 1  ~ i~ -Â° ~a iÂ° a  Â£:l ~  t  f ? ~ ~ao~ ~  i ~Â°, o~ ci~  ,  ~ vx~ihdod iÂ£qa doqvnv xdnvtt-di~vnaci-~(iipa54 neuromorphic systems engineering  ocr  vof ? ~ ~i 0 ~ ~  ~ ~ lpf ~  ivrf ]., i ./~+1 i  ,~ql r , ~ v~ ; v /~r /~ ~1~ ~ ~ ~ ~ ~ ~'~'~ ~, ~o ~ ~ ~,~ ~o~,  ~; ;~ ~,  it a  v 1  figure 3.3 the offset-adaptation circuit. the input to the circuit is the output of the  1 and 1 first amplifier of the sos, v1. the outputs vo~ vo~ connect to the corresponding inputs  of the first amplifier of the sos.  differential pair. the cp and cn transistors function as cascode transistors;  they ensure that the dc gain of the amplifier is high such that there is no  significant low-frequency attenuation in the cochlea (see section 3.3.1 for fur-  ther details.). we use cp and cn transistors on both arms of the amplifier to  avoid any systematic offsets. the cp and cn transistors do not alter the noise  performance of the amplifier, since their noise currents contribute little to the  output current of the amplifier. the pfet m transistors implement current  mirrors. the bias current of the amplifier ib is provided by bipolar transis-  tors in the tau-and-q biasing circuit. the extra mirror necessary to convert  npn bipolar collector currents to pfet bias currents does not alter the noise  performance of the amplifier. the offset-correction circuit adds two correction  currents at the vot and vow. nodes. in the filter of figure 3.1, only the left  amplifier has correction-current inputs from the ocr.wide-dynamic-range analog vlsi cochlea 55  3.2.2 the offset-adaptation circuit  figure 3.3 shows the offset-adaptation circuit. the lpf is a simple 5-transistor  nfet ota-c filter operated in the subthreshold regime. the dc value of the  v1 input is extracted by the lpf and is compared with vrf in the pfet  differential pair. the currents in the arms of the differential pair are steered  via mirrors to the voz and vo~ inputs of the left wlr in figure 3.1, such that  the sign of the feedback is negative. the cascoding of the offset-correction  currents prevents the offset-correction circuitry from degrading the dc gain of  the amplifier. the vb gate voltage of the left wlr, as revealed in figure 3.2,  is also the vb gate voltage in the offset-adaptation circuit, such that the offset-  correction current scales with the bias current of the wlr. the scaling ratio  is controlled by the source control vof.  the value of vof, which determines the loop gain of the offset-adaptation  loop, and the value of vot determine the closed-loop time constant of the offset-  adaptation loop. a high loop gain speeds up the closed-loop response such that  the influence of low-frequency inputs is adapted away. thus, there is a tradeoff  between strong offset control (high loop gain), which implies some ringing and  overshoot as well, and coupling of low-frequency inputs into the cochlea. since  our lowest-frequency input is 100hz, we have been able to maintain a good  control of the dc offset in the cochlea without attenuating any frequencies of  interest.  note that the offset-correction circuitry can correct offsets to only a reso-  lution that is limited by its own offsets. since we use subthreshold circuitry  with small linear ranges in the ocr and lpf, these offsets are on the order  of 5mv to 15mv; they are small compared with the 1v linear range of the  wlr. the offset-correction scheme would have been less effective if we had  used other wlrs to sense and correct the offset of the filter wlrs. in the  latter case, since the offset of a wlr scales with its linear range, the resolution  of the offset-correction circuitry would typically have been a significant fraction  of 1v.  note that our offset-compensation circuitry does not require any fioating  gates and the assosciated need for high voltages, and high-voltage circuits.  further, unlike in some floating-gate circuits, the time constant of the offset-  adaptation circuit may be tuned to be in the 10ms-lsec. range; such values  ensure that the offset adaptation across an entire cochlea is not excessively  slow.  3.2.3 the second-order filter  figure 3.4 shows representations of our second-order filter. the block-diagram  form of part (b) is convienient for doing noise calculations. for the purposes  of doing noise calculations we list 12 algebraic relationships:  i1  g,~1- vl" (3.1)56 neuromorphic systems engineering  vin  Â©> (a)  ! ,~ : ,~ ", 2  (b)  vnl v, n2  figure 3.4 the second-order filter circuit. (a) the input is v~n, the output is y~, and  the bias currents of the first and second amplifiers are i1 and i2. (b) the block-diagram  equivalent of the filter is useful for making noise calculations. the voltages vnl and v~2  represent the input-referred noise sources of the first and second amplifier respectively.  i2 (3.2)  g~ - vl"  c1  ~'1 -- â¢ (3.3)  gml  c2 w2 = (3.4)  gm2 "  ~ = ~. (3.5)  ci vl  i~ - (3.6)  t1  c2yl ie = (3.7)  ~2  e = ff~. (3.s)  t ~ ~ t~ q. (3.9)  ~ = ~e. (3.~o)wide-dynamic-range analog vlsi cochlea 57  cr = ~. (3.11)  c = ~. (3.12)  the currents i1 and i2 are the bias currents of the amplifiers, vl is the linear  range of these amplifiers, and the transconductance gmi of amplifier i is given  by gmi = ii/vl. the noise source vn~, shown in figure 3.4(b), represents the  input-referred noise per unit bandwidth of amplifier i. from [11], we know that  2 = gqy~/ii, vni  where n is the effective number of shot-noise sources in the amplifier, and q is  the charge on the electron. for our amplifiers, n is typically 4.8, whereas the  amplifiers reported in [11] have n = 5.3.  from figure 3.4(b) and the preceding algebraic relationships, it may be  shown that the output noise per unit bandwidth v,~o is given by  v,~l + v,~2 ('~s/q) --__  vnÂ° t282 + ~'s/q + 1'  2 ~ ~,~ + i~-~s:/q~l ~ vn2 (3.13) v~o ~  ]~s ~ + ~s/q + 1~ ~ "  in eq. (3.13) we have used the fact that the noise sources v~ and vn2 are  uncorrelated, so there are no cross terms of the form v~v~. ~om the algebraic  relationships of eqs. (3.1) through (3.12), by substituting s = ~ and ~ = 2r f,  and by using the normalized frequency x = ~, we can show that  (nqvl/qc~)(1 +~)  =  ~ f ~  \ nqvl ) x: v~o(z)dx = ih(x)l ~ (1 + ~)dx, (3.14)  where h(x) represents the normalized transfer function of a second-order filter,  1 g(x) = 1 - x ~ + jx/q'  and j is the square root of -1. to get the total noise over all frequencies, we  integrate the lhs and rhs of eq. (3.14) from 0 to ~. it can be shown by  contour integration that  ~ ih(x)12dx ~ ~q,  ~z~lh(z)l~dz = ~ ~.58 neuromorphic systems engineering  2 the total output noise over all frequencies < v,~ o > is then given by  ~ > _ 1 (nqvl~(tc 0+_ 1  2clcr -' yno  = (nqv~(i+~),  ~ 4c~ /  :  note that the total output noise is independent of q for this topology: the  noise per unit bandwidth scales like l/q, and the integration of the noise over  all bandwidths scales like q, so that the total output noise is independent of q.  these relationships are reminiscent of an lcr circuit where the total output  current noise depends on only the inductance, the total output voltage noise  depends on only the capacitance, and neither of these noises depends on r;  only the noise per unit bandwidth and the q of the lcr circuit are influenced  by r. in fact, it can be shown that this topology has a transfer function and  noise properties that are similar to those of an lcr circuit if we make the  identifications  ~-1 = l/r,  ~-2 = rc.  for a given value of c (the geometric mean of c1 and c2), the total output  noise is minimized if cr = 1--that is, if c1 = c2.  figure 3.5 shows the noise spectral density versus frequency and the total  integrated noise at the output over all frequencies. the data were obtained  from a test chip that contained a second-order filter with amplifiers identical  to those in our previous report [11]. the lines are fits to theory. as we expect,  the total integrated noise is quite constant with q. the parameters used in the  fits were n = 5.3, vl = 1v, cr = 1.43, q = 1.6 Ã 10 -19, and c = 697ff. the  values of n and vl were obtained from measurements on our transconductance  amplifiers [11]. the value of cr was obtained from least-squares fits to the data.  we obtained the value of c by having the noise measurements be consistent  with values of c expected from the layout.  in filter topologies that have been used in prior cochlear designs e.g. [7]  or [24], the noise per unit bandwidth increases with q: the q is obtained  through the addition of positive-feedback currents. these currents contribute  additional shot noise and thus increase the noise per unit bandwidth; in these  topologies, the integrated noise over all frequencies also increases with q, so  both factors contribute to the increase in noise with q. although we have  performed extensive experimental and theoretical analysis of noise in these filter  topologies as well, we shall present only key findings here: for the topology  presented in [7], at qs near 2.5, the rms noise power at the output is 9 times  higher than it is for our topology. for qs near 0.707, the rms noise power at  the output is about 0.8 times lower than it is for our topology. for qs near10 ~  "i-  #  ~: 10 -s  r~ -~  i~  ~.~  .~  o  10:0~  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  g (a) o--q= 2.60  x -- q = 0.96  * -- q = 0.50  10 2 ~ ~ ~ , ~,11  10 3  frequency (hz) 10 4  >  e  o  z  0 wide-dynamic-range analog vlsi cochlea 59  o o (b)  Â¢, o o ~ 0 0  0:s i 1:2 1.'4 1:6  sqrt(q)  figure 3.5 noise in the second-order filter circuit. (a) the noise spectrum changes  shape as the (~ of the filter is changed. (b) the total output noise integrated over all  frequencies is approximately invariant with q for this filter.  1.5, which is where we typically operate, the rms noise power at the output  is about 2 times higher than it is for our topology. the effects of increased  noise per unit bandwidth in a single second-order filter are greatly amplified in  a cochlear cascade. factors of 2 in noise reduction in a single stage can make a  significant reduction in the output noise of a cochlear cascade. thus, using our  filter topology contributes significantly to reducing noise in a cochlear cascade.350  300  ~" 250  e  > 200  150  1Â°Â°ols 2.s 1 1.5 2  quality factor (q) 60 neuromorphic systems engineering  figure 3.6 maximum undistorted signal in the filter. the input amplitude at which  the total harmonic distortion at the output is attenuated by 30db with respect to the  fundamental is plotted versus q. the fundamental frequency is at the bf of the filter. the  line is an empirical fit.  although the noise properties of the filter of figure 3.4 are superior to those  of other second-order topologies, this filter's distortion at large amplitudes is  significantly greater, especially for qs greater than 1.0: distortion arises when  there are large differential voltages across the transconductance-amplifier inputs  in a filter. the feedback to the first amplifier of figure 3.4 arises from v2, rather  than from v1, in contrast to the topology of [7]. consequently, the accumulation  of phase shift from two amplifiers, as opposed to that from one amplifier used  in earlier topologies, causes greater differential voltages and greater distortion  in the first amplifier. also, the transfer function of the intermediate node v1 is  such that the magnitude of voltage at this node is greater than that in other  topologies for qs greater than 1.0. consequently, the differential voltage across  the second amplifier is larger, and the distortion from the second amplifier is  also greater.  it is instructive to find the largest input signal at the bf of a filter for which  the total harmonic distortion (thd) is about 3%-5%. the amplitude of this  signal, v,~, is a good measure of the upper limit of dynamic range for a filter,  in the same way that the input-referred noise is a good measure of the lower  limit of dynamic range. figure 3.6 shows the rms amplitude v,~ at a bf of  140hz for the filter of figure 3.4. we observe that, as the q increases, the  distortion increases, and the value of v,~ falls. the data were obtained for a  thd level of 3.3% (30db attenuation in intensity). the data were empiricallywide-dynamic-range analog vlsi cochlea 61  12  i~  v~ i1  fii~ure 3.7 translinear tau-and-(~ biasing circuit. the voltage vt sets the ~- of the  filter, and the voltage vq sets the small-signal q. the current ia is a placeholder for a  gain-control-correction current. the currents i1 and i2 are the bias currents of the first  and second amplifiers of the filter.  fit by the equation  vmx(q) = 128- 1611n ~ .  the preceding discussion illustrates why an agc is essential for attaining  a wide dynamic range with our filter topology: the noise properties of the  topology are favorable for sensing signals at small amplitudes, and with high  qs. however, when the signal levels are large, if the distortion is to be kept  under control, the qs must be attenuated. the agc ensures that the qs are  large when the signal is small, and are small when the signal is large.  3.2.4 the tau-and-q control circuit  in figure 3.7, we make the following definitions:  it = ise vt/2ut, (3.15)  qo = e yq/2ur, (3.16)  where ut ---- kt/q is the thermal voltage, and is is the bipolar preexponential  constant. the current ia is a place holder for an agc correction current from  the ihc and peak-detector circuit, and i1 and i2 are output currents that bias  the first and second amplifiers of figure 3.4, respectively. a simple translinear  analysis and the solution of the quadratic equation reveal that, if we define ~  to be a normalized agc correction current, according to  ( ia ) (3.17) ~= 2i~/qo'62 neuromorphic systems engineering  then  q = iv~ 2 ,  independent of the value of ia, the translinear circuit always ensures that  ~ = i~. (3.18)  thus, it is an effective tau-and-q biasing circuit for the filter in figure 3.4,  since it ensures that the agc affects the q but not the corner frequency of the  filter. if we let  0 = arctan 7, (3.19)  then trigonometric manipulations of eq. (3.17) reveal that  if there is no agc correction current, then 0 = 0 ~nd q = q0. in the limit of  an infinite agc correction current, 0/2 = ~/4 and q = 0.  figure 3.8(a) shows the corner frequency of the filter in figure 3.4 ~s ~  function of the bias voltage vt. as we expect from eq. (3.15) and (3.18), and  from the equations of the filter (eqs. 3.1 to (3.12)), the corner frequency is  an exponential function of the bi~ voltage vt. the exponential preconstant  yields a thermal voltage of 26.7mv, which is fairly close to the expected thermm  voltage of 26mv at a room temperature of 300k.  figure 3.8(b) shows the q of the filter in the absence of any agc correction  current. as we expect from eq. (3.16) and eq. (3.20) with ~ = 0 (no agc  current), the q is an exponential function of the bi~ voltage vq. the exp~  nential preconstant yields a thermal voltage of 26.3mv, which is fairly close to  the expected thermal voltage of 26mv at a room temperature of 300k.  3.2.5 the inner hair cell and peak-detector circuits  figure 3.9 shows the ihc and pd circuits. the amplifier in the ihc is a  simple 5-transistor nfet ota with a fairly high gain (500 to 1000). the bias  current of the ota is determined by the voltage va. the bias current should  be sufficiently high that the dynamics of the node vh are much faster than the  dynamics of the node vn, for all input frequencies and amplitudes of interest.  since the ota is connected in a follower configuration, the voltage v,~ is very  nearly a copy of v~n, except for very weak signals, where the bipolar transistor  ba or the mos transistor pa are not sufficiently turned on. in practice, the  signals or noise at the cochlear output taps are sufficiently high that ba orwide-dynamic-range analog vlsi cochlea 63  '-i-  e  o 10 s  10 4  10 3  10 2  0:9 ~ 1:1 1:2  v t (v) 1.3  i0 ~  v  _10 Â°  >.,  ..~_  o'  ,o:~ (b)  v t = 0.950v  -4b -2~ ~ 2b  vq (my) 40  figure 3.8 tau-and-q control circuit characteristics. (a) the corner frequency has an  exponential dependence on the voltage vt. (b) the quality factor q has an exponential  dependence on the voltage vq.  pa may be assumed always to be sufficiently turned on. when v~,~ or v,~ are  rising, the capacitor chr is discharged primarily by the bipolar transistor ba.  when vm or vn are falling, the capacitor chr is charged primarily by the mos  transistor pa. thus, during the phases of the signal when the derivative of the  signal is negative, the current ihr is an amplified copy of cgrdv~n/dt. the  amplification factor is given by exp(vnn/ut). thus, the ihc differentiates,64 neuromorphic systems engineering  vin  o inner hair cell  ,  i  vhr peak detector  1  zt  j  figure 3.9 the ihc and pd circuits. the inner hair cell transduces its input vm to a  current ]h~ that is then fed to the peak detector. the output of the peak detector ]pk is  mirrored to the tau-and-q control circuit as a gain-control-correction current.  rectifies, amplifies, and transforms the input voltage y/n into an output current  ihr.  the output current ihr is fed into the peak detector. the peak detector  consists of a slow source follower, composed of pf, pt, and cpt, and the  feedback transistor pi. the transistor po outputs a copy of the current in  pi as ipk. the source follower can follow descending signals in v i rapidly  because of the exponential dependence of the current of pf on its gate voltage.  however, the voltage vpt is set near vdd so that the current source formed by  the transistor pt is slow in charging the capacitor cpt; consequently, during  ascending signals in vi, the voltage v8 is slow to respond. because of the  fedback nature of the circuit, and the asymmetry in the time constants of the  source follower, v8 will equilibrate at a value such that the average current  through pi is slightly below the peak value of iur. as ihr alternately reaches  its peak and moves below that peak, the voltage v~ will undergo large swings  due to the high gain of the input node of the peak detector. in contrast, the  voltage v~ will have only small variations from its dc value; they constitute  the ripple of the peak detector.  figure 3.10 shows the waveforms v~n, vn, vh, v~, and v~. the labeled  voltages in the figure indicate the dc voltage v~lues that correspond to the  horizontal location of the arrow. as we expect, v~n and vn are very nearly equal  to each other. the voltage vh undergoes abrupt transitions during changes inwide-dynamic-range analog vlsi cochlea 65  0  0 ~ vi n ~ 3.0~,  ~-b4.6\  -~-~f4.6y  vs ~ ~-~ 4.3\  o 5  time (msecs)  figure 3.10 the ihc and pd circuit waveforms. the waveforms for the voltages vin-vs  illustrate the operation of the circuits of figure 3.9.  the sign of the input derivative; these changes correspond to a transition from  ba being turned off to pa being turned on or vice versa. the voltages v~, and  vs in the peak detector undergo rapid downward transitions that are phase  locked to the downward-going zero crossings of the input waveform where the  peak value of ihr occurs. the upward transitions in vf and vs are slow because  of the sluggishness of the current-source transistor pt. the data were taken  with v~n being a 102mv rms input at lkhz, with va = 1.0v, with vpt =  4.039v, vdd = 5.0v, and with vhr ---- 100mv. typically, we operate vpt near  4.25v, which results in no discernible ripple in vs, but these data were taken  specifically to illustrate better the workings of the peak detector. the transistor  pt was fabricated as a poly2 transistor. thus, at the same current level, the  bias voltages on vpt are higher than those corresponding to bias voltages on  a polyl transistor.  from the preceding discussion, we expect that the value of ipk will be near  the peak value of chrdv~/dt amplified by the factor of exp(vhr/ut). thus,  if the input amplitude were given by  yin = ain sin (2~rf~nt),  then the value of ipk would be given by  ipk = 2zc f~nchrame y~r/ur. (3.21)66 neuromorphic systems engineering  <  0 10 -7 14 , , , , ,  input frequency = 10khz _j  12 vhr = loomv ~-  lo i  8  6  4  2  0  ~ 0:2 0:4 0:6 0:8  input rms amplitude (v)  10 "~  14 .... ~  input rms amplitude = 1v ~ 12 ./-  10  8  6  4  2  0  % 2obo 4obo 6o~o 8o~o lo~oo 12000  frequency (hz)  figure 3.11 ihc and pd amplitude and frequency characteristics. (a) the current i;/~  has a linear dependence on the input rms amplitude. (b) the current _f;k has a linear  dependence on the the input frequency.  in conformance with eq. (3.21), figure 3.11 shows that the response of  ipk is linear with the amplitude and with the frequency of the input. the  data were taken for va = 1.0v, and vpt = 4.3v. the experimental slopes for  figure 3.11(a) and figure 3.11(b) yielded values for chr ~ 335ff and chr =  313ff, respectively. however, the linear fits to the data reveal that an offset in  amplitude of about 77.5mv rms in the case of figure 3.11(a), and an offset in  frequency of about 276hz in the case of figure 3.11(b), needs to be subtracted  from a~ or f~, respectively. these offsets imply that there is a minimumwide-dynamic-range analog vlsi cochlea 67  10 -8  <  ~- 10-9 -.~  0  ,-~  i~  i~.  ~0-~o  0 input rms amplitude = 1 o0 mv /  o  0.~2 0.~ 0.56 0.~8 0:~ 0.~2  vhr(v)  figure 3.12 dependence of [p/Â¢ on vhr. the current ipk has an exponential dependence  on the voltage vhr.  amount of input current ih~- that is required for the peak detector to output a  current ipk. through experimentation, we have found that this minimum value  scales approximately linearly with frequency such that the offset for ain always  lies somewhere in the 50 to 100mv rms region (for a vhr of about 100mv). at  this time, we do not have a good explanation of what causes these offsets; we  suspect that they are due to the short-channel length and small early voltage  of transistor pi.  figure 3.12 shows that the relationship between ipk and vhr is described by  an exponential, as eq. (3.21) predicts. the thermal voltage ut was determined  to be around 29.9mv. this voltage is somewhat higher than the 26mv expected  from theory. the data were taken with vpt  4.30v, and va = 1.15v.  the current ipk is mirrored by the bipolar transistors bp and bo in fig-  ure 3.9 to function as the agc correction current ia in figure 3.7. from  eqs. 3.1 to (3.12), we know that i~ is given by 2zcfccvl, where fc -- 1/~- is the  corner frequency (cf) of the filter. thus, ~ in eq. (3.17) is given by  ia  2~ / qo"  = qÂ°ev"/u~" \ fc ] 2~j 2vl j " (3.22)68 neuromorphic systems engineering  10 0  >  v  .~ 10 4  -~  .~_ __ c~  e  .Â¢~ 10-2  {/3  e r,,,  ~ ~0 -3  10:0,  100  >  ~ 10 4  .-~  .~_ _ ~.  e 10-2 <  (f}  e r~  ~ 10 .3 c~  o without agc ....... . .......  ~ (a)  q = ~.9s  q=2.17  v t = 0.95v ~  vq = 8my b,b,"  cf = 132hz o q= 2.22  102 103  frequency (hz)  with agc  q=u.5z ..... "__ ~ â¢ ........  q=1.56 ~ - ~--"~q,,, 960mv  ~ ~_-,~,~ ~,,or~v ~.~ ~.~.~.*~-- "% ~. ~omv  ,%,,., 160my  ~ ~  _- _,.,,.=~,-@'~ ~ 0 40my q=2.22 c=ooo~ ~x~  vhr = 65my "~  vpt = 4,25v o~  cf: 147hz~ ll2hz ~ 5mv  loi'o, ...... i~ ....... io 3  frequency (hz)  figure 3.13 frequency-response characteristics of a stage. (a) without an agc, it is  impossible to obtain smooth and continuous data beyond an input rrns amplitude of 250rnv.  (b) with an agc, it is easy to obtain smooth and continuous data up to and beyond a 960mv  rms input amplitude.  thus, the voltage vhr serves to strengthen or weaken the normalized agc  correction factor ~.wide-dynamic-range analog vlsi cochlea 69  3.2.6 the properties of an entire cochlear stage  figure 3.13 shows the frequency-response characteristics of the filter of fig-  ure 3.4 for different input amplitudes. in the absence of an agc, large input  amplitudes generate large amounts of distortion in the filter; thus, in fig-  ure 3.13(a), it was impossible to obtain smooth frequency responses beyond an  input rms amplitude of 250mv. in contrast, in figure 3.13(b), we could easily  obtain smooth frequency responses up to (and even beyond) input amplitudes  of 1v rms, because of the presence of an agc. if the frequency-response curves  are fitted with the transfer function of a second-order section,  1  h(s) = ~.2s 2 + ts/q + 1' (3.23)  then we find that the cf (l/t) is reduced with input amplitude, and the q  is reduced by the agc as well. in figure 3.13(b), the cf is reduced from  147hz at small signals to ll2hz at large signals; the q is reduced from 2.22  at small signals to about 0.52 at large signals. these numbers are valid for  vhr = 65mv, and vpt : 4.25v. given that we designed eq. (3.18) in our  translinear biasing circuit to keep the cf constant, it may seem surprising  that the cf changed with input amplitude. however, we must realize that we  are fitting the frequency-response curves of a nonlinear system with a linear  approximation given by eq. (3.23) at each rms input amplitude. according to  eqs.(3.22) and (3.17), for the same input rms amplitude, the "q" is lower at  high frequencies than at low frequencies. the frequency dependence of the q  results in disproportionately more attenuation of the input at high frequencies  than at low frequencies, such that the cf, as measured by the amplitude curves,  appears to shift.  if we plot the cf90--that is to say the frequency at which the the second-  order filter has a phase lag of 90 degrees--versus rms input amplitude, then  the data of figure 3.14 reveal that cfgo is approximately constant. at low  input amplitudes, the agc has no effect, because ipk provides no correction  until the input amplitude is above a certain threshold, as we discussed in sec-  tion 3.2.5. even if there were no offset, the agc correction in this regime would  be small. thus, the system is linear at low amplitudes. consequently, at these  amplitudes, the cf9o is identical with 1/~- and with the cf measured by gain  curves. since the agc is designed not to affect the parameter % the cfgo re-  mains approximately invariant with input amplitude, even at high amplitudes  where the agc is active. in fact, figure 3.14 shows that a strong agc (higher  values of vhr) improves the constancy of the cfgo with amplitude, because  it prevents static nonlinear shifts in cf9o that increase at high qs. the cf9o  is the frequency near which center-surround schemes, e.g., those that perform  spectral extraction on cochlear outputs for use in implants [14], generate their  maximum output. thus, the fact that the cfgo is approximately invariant with  amplitude makes our agc cochlea attractive as a front end for center-surround  postprocessing.70 neuromorphic systems engineering  g  n  o z 1.2  1  0.8  0.6  0.4  0.2  0, o* t  o -- v h~ = 3mv  x--v h~ =32mv  * -- v hr = 65rnv  0:2 0:4 0:s  input rms amplitude (v)  figure 3.14 cf9o characteristics. the frequency at which the phase lag of the filter is  90 degrees is relatively invariant with input rrns amplitude.  figure 3.15(a) shows data for q versus a~ measured for three different  values of vhr for the second-order filter. from eqs. (3.17), (3.19), and (3.20)  we would expect the curves to be fit by a fnnction of the form  q = qotan (~ arctan(ga~)) 2 " (3.24)  however, from the discussion of section 3.2.5, we know that a~ in eq. (3.24)  should be replaced by 0 below some threshold value a0, and by am - ao above  this threshold value. the fits in figure 3.15(a) are functional fits to eq. (3.24)  with the free parameter g, and the additional free parameter a0. for v14r =  3mv, 32mv, and 65mv, we found g -- 1.93, 2.5, 4, and a0 = 0.164, 0.095, 0.06,  respectively; q0 was 2.05 for all curves, ain and a0 are in units of v. we took  data by measuring the gain of the filter at fi,~ = fc = cfgo = 1/% which, for a  second-order filter, is q. figure 3.15(b) plots the output amplitude, q(a~) Ã  a~n, rather than q(a~n), at this frequency. we observe that, before the agc  turns on (a~ < a0), the relationship between the input and output amplitudes  is linear. after the agc turns on (a~ > ao), the relationship between the  output and input amplitudes is compressive, although not as compressive as  theory would predict. since a0 is large for small values of vhr, the range of  linearity is large for small values of vhr.  figure 3.15 suggests that, at large amplitudes, the static nonlinearities in  the filter increase the q slightly. since the q of the filter is given by ~//~, we  deduce that the static nonlinearity is causing ~-2 to increase faster with ain than  ~-1; this deduction is in accordance with the intuition that the second amplifierwide-dynamic-range analog vlsi cochlea 71  2.2  1.8  cy 1.6  v  ~, ~.4  ~1.2  >, ~ .~_  -~ 0.8 &  0.6  0.4  0-2 0 0:2 (a)  o -- v hr = 3mv ~ ,o x -- v hr = 32mv  0:4 0:6 0:8  input rms amplitude (v)  0.6  "-" 0.5 >  ~.~  (d  ~ 0.4  .~_  ~.  e 0.3 <  i/}  e c~ 0.2  ~ 0.~ 0  o~ ' ' ' ' 0 o~ ~~0 0 u u ~  oÂ°~ ~  ~ (b) o.-v.~ =~mv  1 x -- v hr = 32my  f " * -- v hr 65my  0:2 0:4 0:6 0:8  input rms amplitude (v)  fil~ure 3.15 q-adaptation characteristics. (a) the q adaptation due to the agc is well  fit by theory, except at large input rms amplitudes, and for strong agc corrections (large  vhr). (b) the same data as in (a) except that we plot the output rms amplitude, instead  of the q).  in figure 3.4 is subject to greater differential voltages, and, consequently, to  more saturation and slowing than the first amplifier. one way to avoid, or even  to reverse, the nonlinear shift toward higher qs is to have the linear range of  the first amplifier be smaller than the linear range of the second amplifier.  the nature of eq. (3.24) is such that, independent of the value of g,  q(a~n)a~,~ is a monotonically increasing function of a~,~. this property guar-72 neuromorphic systems engineering  10 0  >  "~10 -~ -.~ .~  ~.  e 10-~ <  $  0  > v  ,_  e <  0 10 -3  10 .4  10  10 0  10 -1  10 -2 wztheutagc  ........ (a) ; :;f _ ooooÂ°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°o x--2f  o * -- 3f o o o o xxxxxxllx  x â¢ ~x  x x x ~ x ~ ~x  x ~ x x  xxx~ â¢ x x  ~ ~ x  ~ ~  10 2  frequency (hz)  ~fith acÂ£  0 00v,uu,~ uo (~0 ...... oooooo (b)  0000  0 0  xxxxxx x o o  x x xx x  x x ~ x x  x xx ~ ~ x x  â¢ x  ~ ~( x  x  ~4 ~ x  ~ ~ t~ t~ inpu! rms a, mpli,tude, =.25.0,rn.v  10 8  o--if  x -- 2f  *-- 3f  ........ ~ input, rms .ampli.tud.e =.96.0.rny  10-103 10 2 10 8  frequency (hz)  figure 3.16 distortion characteristics of the filter. (a) without an agc, the distortion  is already fairly high at a 250my input rms amplitude. (b) with a strong agc (vhr =  65mv), the distortion is comparable to that in (a) at only a 1v rms input amplitude.  antees that the input-output curve at the bf of a cochlear stage is always  monotonically increasing, as confirmed by the data of figure 3.15.  figure 3.16 shows that the harmonic-distortion levels at 1v rms with an agc  are comparable with the harmonic-distortion levels at 250mv rms without an  agc. the agc data were taken with vh~- = 65mv. figure 3.17 shows that  a strong agc (large value of vhr) reduces harmonic distortion due to the  lowering of q.wide-dynamic-range analog vlsi cochlea 73  >  v  e <  e  0 10 0  101  10 .2  i0 -~  10 -4  10 `5 000000000000000 0 0 0 0 0 0 0 0 0 0 0 0 0 0  x xxx xxx  ~i~ ~ ~ ~ ~  xx:~c f o--lf x -- 2f x x ~ * -- 3f  x ~ x~ vhr = 3my  ~ (a)  o12 oi~ o16 ols  input rms amplitude (v)  10 0  10 4 oooooooooooooÂ°o  0 >  .~'Â°~ xxxxx~)~<x ~  ~-<f: 10~ xx~ ~  ~ ~ ~ ff ~ 10-3 x ~ f~  o 104  ~0-~ 0:~ 00000000000000  xxxxxx xxxxxxxx  o--if  (b) x--2,  * -- 3f  vhr = 65mv  o14 oi~ o18  input rms amplitude (v)  figure 3.17 distortion characteristics at bf. (a) with a weak agc (tv'.~r = 3my), the  distortion levels are significant at a 250mv input rms amplitude. (b) with a strong agc  (vhr - 65mv), the distortion levels are smaller than are those in (a) even at a iv input  rms amplitude.â¢ oanc~aacgttaav sopvas~a-$u!dd~laaao ano jo uo!:~anpoa~u! alia 0:~ai.:~oiii  ii!m uo!ssnas~.p aria asn~aoq uo!a~nuaaav xauanbaaj-~o i jo uo!ssnas!p v qa!~ m~  -aq ii~tls aat '~alqaoa aq:~ jo so!aaadoad aria ssnas!p ii~qs a~ 'uo!aaas s!r a u i  v3"ihdod 3h.l =10 s31.1.~3dord Â£'Â£  â¢ anita mnvq!i!nba uv  o~ 'apu:gldmv ~nd~no aql snq~ puv '~ aq~ sajo~saa aojnos ~uajjno ~o~a~ap-~ad  aq~ jo ~u!~a~q~ an!~d~d~ ~ols aq~ 'xii~n~ua~ ~ 'auo~ pno i ~u~pa~azd aq~ o~  uoi~md~p~ lq posn~ ~ ~o i ~ o~ onp o~aopom s ! ouo~ ~jos oq~ o~ osnodsoa  i~!~!u~ aq~ '~jos o~ pno i mojj suo!~!su~ auo~ aq~ uaq~ "anita mn[aqil!nba u~ o~  'opna~idm~ ~nd~no oq~ snq~ puv '~ aqa soaoasaa qa~q~ 'aaanos ~uaaana aoaaoaap  -~od aqa jo ~u~aeqa oa!a!a~d~a ~ols oqa xq paaaaaaoa s~ uodmd~p~aaao oq~  â¢ olaxa ouo u~qa!~ Â£iau~lsu~ ad~p~aoao oa aoaaalap ~ad oqa sasn~a auo[su~aa  or& 'andano aqa ~ uoos s[ asuodsaa aua!suvaa ~ 'ouoa aqa jo loguo aqa av  â¢ auoa aaaa~nb ~ oa ra~suaau~ u ! so~aaaap Â£iadnaq~ uaqa pu~ 'al~q~ ~ aoj sas~saad  'aauai~s jo po~aad v aa~jv rluappns uo suan~ ~vqa aaai~ aqa jo aa aqa av auoa  oand ~ s~ sninm~as aqÂ£ :uo~amd~pv 0 ~o sa~mvuÂ£p aq~ saa~a~snli ~ gi'g oan$~d  -s~.depe ddv aq~ jo u!e~ aq~ se asuodsaj ~,nd~no aq:l u! dnpl!n q lenpe~ e  sasne~ anita ~jos ~ o~ pno i e ~ojj aura a~s aq~ jo &~sua~u~ u~ uo~npaj aql "~n~jq~l~nba  o~ uo~ejo~saj ~ xq pa~olloj 'apxd ~sj~j aq~ uo ~ua~suej~ jagq e sashed a~ual~s jo po~jad ~  ~q papa~ajd 38 ~e auo~ pno i e jo ~asuo aql "uo~e~depe-~ jo sd~eu~ b['Â£ a~n~  (spuo~as) auj!Â±  s~ ~ s'~ ~ so 0  ~.nd~.no  zhot, i. = ~uanbs, j..-i ),ndul  ao~'t~ : j~a  ~.ndul <  <  &  ~nih~hnidnh sinhj.sas oihdhoinoh~ihn wlwide-dynamic-range analog vlsi cochlea 75  3.3.1 low-frequency attenuation  if the open-loop gains of the amplifiers in figure 3.4 are a1 and a2, then we can  show that we obtain the low-frequency gain of the filter of figure 3.4 by simply  replacing ~ks and ~-2s with 1/ai and 1/a2 in the transfer function. thus, from  eq. (3.23) the low-frequency gain h0 is given by  1  h0 = 1 1 + ~ + aia2  ai  a1+1"  although h0 is very close to 1, it is not exactly 1. a low-frequency input that  travels through m stages of a cochlea will suffer a net attenuation, hm, given  by  (ai   s-s  = '  ~ e-m/a1 ,  where the exponential approximation is valid if a1 is a large number. now,  ~1 = 2voivl, where vl is the linear range of the amplifier and 7o is its effective  early voltage at the output of the amplifier [11]. in the 1.2/~m ~well orbit  ~osi8 process used to fabricate our circuits, 7o is arouud 20v for wide-linear-  range amplifiers that do not have e~eode transistors at the outputl v~ = 17,  and a1 is about 40. thus, we can expect an attenuation of almost one e-fold  across a 39-stage cochlea built with c~codeless amplifiers.  figure 3.19 shows the low-frequency attenuation of a ,10hz 50mv input to  a 39-stage cochlea tuned from 900h~ to 100~ for different values of the pa-  rameter a1. for these experiments, we operated the cochlea with a very low  ~ (ve = -80mv) so that we could focuss on just the effects of low-frequency  attenuation. we varied the value of -41 by varying the bias of the casÂ¢ode tran-  sistors vcn and vcp , in the amplifier of figure 3.2. we explored the effects  of turning off the cascode transistors by biasing them as switches. thus, to  turn off the cn cascode transistors, we would set vcn to 5v; to turn off the  cp cascode transistors we would set vcp to 0v. figure 3.19 shows the low-  frequency attenuation for the four cases of both cascodes on, both cascodes off,  only n cascodes off, or only p cascodes off. we observe from the data that the  p cascodes are more helpful in reducing low-frequency attenuation than are the  n cascodes, because the pfets in our process have a lower early voltage than  do the nfets. with both cascodes on, a 39-stage cochlea has a net attenuation  that is less than 0.8. we normally operate the cochlea with both cascodes on,  with vcn = 1.2v, and with vcp = 3.8v. these bias values permit operation  of our amplifiers over the entire frequency range of the cochlea without any  saturation effects for input rms amplitudes that exceed 1v rms.  the attenuation of the gain of signals at other frequencies is by the same  factor hm. in contrast, the output noise (or distortion) at a cochlear tap is76 neuromorphic systems engineering  0.9  ~ 0.8  ~: 0.7  .9.o  ~ 0.6 o ~,.  ._o 0.5 ,g  Â¢" 0.4 $ 4-~  '< 0.3  0.2 0 , , ,  ~ao oo o ~ oooo  xxx~" . o oo Â° Â°oo oo o oÂ° ooo o o o oo ooo oo o-  x ~ ooo  x x ~++  xv ~++++++.++++_~ ^.. ~w -r ~t++++ ~ ~ ++__  axxxu ~ t++++  ~xxxx ~ ++  xxxxxxxxxxx~ ~  o -~ n ~nd p c~scodes ~ x  + -~ n cascode o~, p c~scode on xxxx  * -- n c~scode on, p c~scod~ o~ x x  x -~ both c~scodes off x  ~ ~b ~b ~0  cochlear tap number  figure 3.19 low-frequency attenuation in the cochlea. the low-frequency attenuation  for various conditions of open-loop amplifier gain are shown.  v k  vc*jt  figure 3.20 noise accumulation in the cochlea. the noise at the output tap of a cochlea,  your is due to the accumulation, amplification, and filtering of noise from taps preceding  that tap.  accumulated through addition over successive stages, as shown in figure 3.20.  the noise that is added at the input is attenuated by the same amount as the  signal, but the amounts of noise that are added at stages successively closer  to the output tap of interest are attenuated by successively smaller amounts.  thus, the output snr is degraded by low-frequency attenuation.  to limit the degradation of the snr of the cochlea through low-frequency  attenuation, and noise-and-distortion accumulation, we use the architecture  of overlapping cascades shown in figure 3.21. rather than having one large  cochlea, we use a few small cochleas whose frequency ranges overlap by one  octave. all such cochleas process the input in parallel. the filters in the  overlapping octave serve to mimic the effects of the infinite cascade prior to the  stages of interest; the outputs of these filters are not used. since most of thewide-dynamic-range analog vlsi cochlea 77  (a) regular cochlear cascade  in  cochlear output taps  (b) overlapping cochlear cascades  one octave  one octave  .  one octave  input cochlear output taps  figure 3.21 architecture of overlapping cascades. (a) in a regular cochlear cascade, the  input is fed serially to all stages. (b) in an overlapping cochlear cascade, the input is fed in  parallel to tiny cochlear cascades whose corner frequencies overlap by 1 octave.  effect of the infinite cascade occurs within an octave of the corner frequency  of a cochlear stage, we do not sacrifice much in the way of cochlear modeling,  but we do gain significantly in limiting our snr degradation. in general, the  amount of overlap between cochleas, and the number of stages per cochlea can  be varied to suit the nature of the cochlear application.  although the thermal noise in an infinite cascade converges to an equilibrium  where noise accumulation is matched by noise filtering, the 1/f noise in an  infinite cascade does not converge and continues to grow in the cascade. the  1/f noise is significant for only those high-frequency cochlear stages that have  amplifiers with large bias currents [11]. the overlapp':ng-cascades architecture  helps to limit the accumulation of 1/f noise.  a cochlear cascade that is composed of all-pole second-order filters overes-  timates the group delay of the biological cochlea. the overlapping-cascades  architecture also helps to reduce the group delay of the silicon cochlea.78 neuromorphic systems engineering  the architecture of overlapping cascades may be viewed as a hybrid of an  architecture that has many parallel filters in a filter bank and of one that has  one filter cascade with all the filters in serial.  the cochlea that we discuss in this paper was built out of three 39-stage over-  lapping cochlear cascades: the low-frequency cochlear cascade was tuned to  operate in the 100hz to 900hz region. the mid-frequency cochlear cascade was  tuned to operate in the 450hz to 4050hz region. the high-frequeny cochlear  cascade was tuned to operate in the 2000hz to 18,000hz region. thus, each of  the cochlear cascades had about 11.2 filters per octave, ensuring a fairly sharp  cochlear rolloff slope. the qs of the cochleas were tuned to be approximately  1.5. the voltage gradients in vt corresponding to the three frequency gradients  of the low-frequency, mid-frequency, and high-frequency cochlear cascades were  1.040 to 0.9v, 1.130 to 0.990v, and 1.210 to 1.070v respectively. the value  of vq that was suitable for operating all three cochleas was -52mv. for the  remainder of the paper, we shall focuss on the operation of the low-frequency  cochlear cascade, which we shall call the cochlea. the operation of the other  cochlear cascades follows by straightforward generalization. the other parame-  ters that we used for operating our cascades were vol = 4.93v, vhr = 120mv,  vpt = 4.25v, vcn "~ 1.2v, ycp : 3.8v, vot : 0.3v, vrf : 3.0v, and the  dc value of vm = 3v. to conserve power, we operated va at 0.995v in the  low-frequency cochlea, at 1.05v in the mid-frequency cochlea, and at 1.15v in  the high-frequency cochlea. it is possible to reduce the power dissipation even  further by having a tilt in the values of va in each cochlea. through exper-  imentation, we found that vq = -44mv, -52mv, and -65mv yielded the best  performance for the low-frequency, mid-frequency, and high-frequency cochleas,  respectively. we could also speed up the gain adaptation in the mid-frequency  and high-frequency cochleas by setting vpt in the 4.10v to 4.15v range. we  used standard shift-register and clocking circuitry to multiplex the outputs  from the different cochlear taps onto a common output tap.  3.3.2 offset adaptation  figure 3.22 shows the dc output voltage across the cochlea as we scan from  tap 1 to tap 39. in the absence of any offset adaptation (vof ---- 4.76v), each  cochlear stage has a systematic negative offset of about 42mv; by 39 stages the  dc output voltage has dropped from 3v to 1v. as we strengthen the offset  adaptation by raising the value of vof, the offset degradation improves. at  4.96v, there is little offset accumulation, and there is an almost flat dc response  across the whole cochlea. typically, we operate the cochlea at vo~ = 4.93v  and tolerate some offset in return for reduced ringing in the offset-adaptation  loop, and for a lower adaptation corner frequency.  3.3.3 frequency response  figure 3.23/a ) shows the frequency response of the cochlea at different input  amplitudes ranging from 5mv to 1000mv rms at cochlear tap 30. the adapta-wide-dynamic-range analog vlsi cochlea 79  >  v  0 >  0 3  2.8  2.6  2.4  2.2  2  1.8  1.6  1.4  1.2  1  tap i vof = 4.76v i  i i  i  t~ 39  figure 3.22 offset adaptation in the cochlea. as the loop gain of the offset-adaptation  loop, controlled by vof, is increased, the offset accumulation across the taps of the cochlea  is reduced.  tion in q with increasing input amplitude is evident. figure 3.23(b) plots the  gain versus frequency such that the curve with the highest gain corresponds  to the lowest input amplitude of 5mv. the gain adapts from about 30 for the  5mv rms case to about 0.7 at 1000mv rms. figure 3.24 shows that the output  is approximately linear in the input at frequencies before the bf, is compres-  sive at the bf, and is even more compressive after the bf. these compression  characteristics are seen in the biological cochlea as well [8]; they arise because  of the accumulated effects of gain adaptation over several cochlear stages.  figure 3.25(a) illustrates that the harmonic distortion is greatest about one  octave before the bf. this effect occurs because the second-harmonic distortion  is amplified by the high gain at the bf when the input frequency is 1 octave  before the bf. when the input frequency is at the bf, the second-harmonic  distortion drops sharply because 1 octave after the bf there is great attenu-  ation. these effects imply that nonlinearities in the cochlea cause masking in  the perception of harmonic frequencies; that is, the threshold for the detection  of a 2f tone is higher in the presence of a if tone than in the absence of one.  psychophysical experiments reveal this effect in humans as well [2].  figure 3.25(b) illustrates the growth and filtering of harmonic distortion  as the signal travels through the cochlea. the input is a 1v rms signal with  frequency 162hz that corresponds to the bf at tap 30. as the signal travels  from tap 15 to tap 30, the second-harmonic distortion builds until it is at its  peak value about 1 octave before tap 30 (tap 20). after tap 20, however, it is80 neuromorphic systems engineering  10 0  > v 10 -1  .-1 ,~ ~. 10 .2  e <  ~ 10.3 n-  ~- 10-4  o  10i501 i doomv '~ ............ 640mv 320my ~ ~'~  , ~ o m v ~j////~, t, ~ ~ o , \  .o.v ~.../// i v.~ = ] ~o~  20mv" // i  ~o~ ~ ~  stay ~ ~  (a)  102 103  frequency (hz)  102  100  (~n 10 -2  10 .4 input amplitudes  5my->1000mv  (b)  10-~0 102 103  frequency (hz)  figure 3.23 frequency-response curves of the cochlea. (a) the frequency response for  various input rms amplitudes is shown. (b) the same data as in (a) except that we plot the  gain, instead of the ouput rms amplitude.  gradually filtered away because the second-harmonic frequency begins to fall  in the cutoff region of the cochlear filters. by the time that the signal is at tap  30, there is only a small amount of distortion left. thus, the sharp cochlear  rolloff ensures that each tap does not suffer much distortion at its bf.  figure 3.26 illustrates that, at the bf, the output amplitude and harmonic  distortion barely change with amplitude for amplitudes beyond about 40mv or  50mv. the second harmonic is 25db smaller than the first harmonic for a widewide-dynamic-range analog vlsi cochlea 81  >  v  e <  e  o 10 0  i0 -~  10 -2 o -- at bf (162hz) .~1~_ ~=~ x--beforebf(4ohz) o o oo oo o i~  * -- after bf (278hz) 0 o0 0 x  x 0 x  x  x  x o x  x  x  ~ ~(~ ~ ~( ~(~y~i  1 0 3  -10-4 10-3 10-2 10-1 10 0  input rms amplitude (v)  figure 3.24 compression characteristics of the cochlea. the compression at a cochlear  tap occurs primarily at and beyond the bf, whereas the response at frequencies below the  bf is linear.  range of input amplitudes. the reduction in harmonic distortion is due to the  accumulated effects of the action of the agc at each cochlear stage, and to  the sharp cochlear rolloff. note that, in the corresponding harmonic-distortion  plots for a single cochlear stage (figure 3.17(b)), the second harmonic distortion  at bf is only down a factor of 8 at 1v rms, and there is continued growth of  all harmonics with input amplitude.  although the cf as measured by amplitude curves (figure 3.13(b)), shifts,  the cf9o as measured by phase curves (figure 3.14) does not change apprecia-  bly. these findings for a single cochlear stage are echoed in the cochlea as well:  at cochlear tap 30, as figure 3.27 shows, the phase curves have a relatively  invariant cf, although the gain curves (shown in figure 3.23(b)) shift with  amplitude. the kinks and rising parts of the phase curves of figure 3.27 are  due to parasitic capacitances in the cochlear filters.  3.3.4 noise, dynamic range, and snr  figure 3.20 illustrates that the noise at the output of a cochlear tap has contri-  butions from the input-referred noise of each cochlear filter preceding that tap.  to evaluate the total noise at the output we need to evaluate the noise per unit  bandwidth of each of these sources, to evaluate the transfer function from each  source to the output, to accumulate the contributions from the various sources,  and then to integrate over all frequencies.82 neuromorphic systems engineering  100  ~-. 10-1 >  ,~  ~ 10 .2  .i..a ._  __ ~3.  e 10-3 <  ~/3  e i~ 10 -4  ~..  ~ 10 -s o  10i601 .~% i~o Â°  ~,.~ ~ ~ o  ,-  , xoo t  ~x o  â¢ x o  ~ %  ~ o  (a) x o  ~ x ~  bf ~ ~  ~ . ....... ,  102  frequency (hz)  (b)  >  0 > o--lf  x -- 2f  * -- 3f  tap 30  10 filts./oct.  103  ~ tapl 5  ~ tapl 8  ~ tap 20  tap 21  ~ tap 22  ~ tap 23  tap 30  0 041 0.~2 0.~3 0.~4 0.05  time (seconds)  figure 3.25 harmonic distortion characteristics of the cochlea. (a) the harmonic dis-  tortion is most pronounced 1 octave before the bf, but is sharply attenuated at the bf.  (b) the dual effect in space reveals that harmonic distortion is most pronounced i octave  before tap 30 (at tap 20), but is filtered away by tap 30.  if there are noct filters per octave, then the frequency ratio r between the ~-  of any filter and the t of the filter just to the right of that one is given by  r = 2 -1/yÂ°c~, (3.25)  ~_ e--1/(yoct/ln(2)) ,  : c--1/nnat.wide-dynamic-range analog vlsi cochlea 83  >  v  e <  e  0 10 0  10 qi  10 -2  10 -3  10 .4 0 o0 00000~0 0 0 0 0 0 o0000c(d 0 00000~  x x xx x~<  x xxx ~ ~  x x ~ ,~*~  .~ x xxx)4~< x xx)oo~  -.:-  ~ ~ ~ ~."  bf = 162hz  o--if  x-- 2f  *-- 3f  10 -5  10 -3 10 .2 10 -1 100  input rms amplitude (v)  figure 3.26 harmonic distortion at bf in the cochlea. the total harmonic distortion at  bf is at least -30db for all input rms amplitudes.  -500  ~ -1000  i~  e3 -1500 ~.~  t- -2000 q-  -2500 ~ input rms amplitude  5mv -> 1v  ~ ~ ~ ~ ~ ~ ~1 ~ , ~ i ~ ~ ~1 "300010 102 103  frequency (hz)  figure 3.27 phase characteristics of the cochlea. because the agc corrects q, but  not ]/~-, there is relatively little shift in the phase curves with input rms amplitude. the  discontinuous parts of the phase curves are due to parasitic effects.  thus if x = w~- is the normalized frequency corresponding to the output tap of  interest, then the filtering effects of the filters preceding the output filter are  represented by h(x), h(rx), h(r2x),...as shown in figure 3.20. similarly, if84 neuromorphic systems engineering  10 4  ~_ 10 -2  o_  >  ~ 10 .3 .~_  0 z  ~ 10 -4.  --  >  .~ 10 -s  q.  4,~  0 j tap 31  10i60~ ....... 1'~2 ....... 1"(~ 3 ....... 10 4  frequency (hz)  figure 3.28 typical noise spectrum of a cochlear tap. the secondary peaks at the high  frequencies are at multiples of the primary peak frequency and are due to nonlinearities.  i were the bias current at the output cochlear tap (corresponding to ~ at  each tap), the bias current of the kth preceding filter would be given by i/r k.  from eqs. (3.12) and (3.14), the normalized input-referred noise per unit  bandwidth is given by  v~(x)dx = \ 2~-~ /  it is then easy to show that the input-referred noise per unit bandwidth for the  kth filter preceding the output tap is given by  4(x) x (nq.l k  ( x2r2   = \ 27~qc ] ~ + ~]dx'  since x --~ rkx and i -~ i/r k. if there are m preceding taps, then the total  output noise per unit bandwidth v~out(x)dx is given by  nqvl m x2r2 n n  27~qc e r~(~ + ~) 1-[ ih(rkx)12dx"  o o (3.26)  we obtain the total output noise at the cochlear tap of interest by integrating  eq. (3.26) over all x from 0 to ~. although the expression for the output noise  at a cochlear tap can be written down, it is hard to solve in closed form. but  it can be measured easily with a sr780 spectrum analyzer. figure 3.28 shows  what the noise spectrum of a cochlear tap looks like at tap 31 of our cochlea.widf~dynamic-range analog vlsi cochlea 85  >  e  o  >  e <  o 104  10 -2  10 "a  10 .4  10 .5 minimum detectabjejf~put  ...............  0.907mv rms input  snr = 1.18 / \ /~,~..~-signal + noise [ a) /~ (7zmv rm$)  \  ~ /noise  10 -6  10 102 103  frequency (hz)  100 [v[axit33um undistorted input  10 "i  10 -2  10 .3  10 .4 1v rrns input  thd = 3.87% (b) 315.8my  11.85mv  i.i im' 2.82mv nv  10-;0 102 103  frequency (hz)  figure 3.29 dynamic range of a cochlear tap. (a) the spectra of tap 30 when there  is no input present, and when a bf signal that is just above the threshold of audibility is  present, are shown. the minimum detectable input at bf was found to be 0.875mv. (b)  the total harmonic distortion from all harmonics for a iv rms input at bf was less than  4%. the maximum undistorted input is thus iv.  it has a form predicted by eq. (3.26) except for the second and third harmonic  peaks; these peaks are due to nonlinearities in the filters.  figure 3.29 illustrates that the dynamic range at the output of tap 30 of our  cochlea is greater than 60 db at the bf of that tap (162hz): figure 3.29(a)  shows the noise spectrum of the background noise at tap 30 which yields a total  integrated noise of 50mv rms. when a bf sinusoidal signal (162hz) of 0.907mv86 neuromorphic systems engineering  rms magnitude is applied to the input of the cochlea, it is amplified up by a fac-  tor of 57.1 to 51.smv. thus, the rms power of the signal and noise at tap 30 is  about 72mv rms (v/(51.82 + 502)). now, at an output snr ratio of 1, we would  expect the signal and noise to have an rms power of 50x/~ = 70.7mv rms. the  fact that the rms power is 72mv means that our minimum detectable signal,  which corresponds to an output snr of 1, is actually below 0.907inv. in fact,  since the system is linear at small input amplitudes, the minimum detectable  signal is 50mv/57.1 = 0.875mv. figure 3.29(b) shows that the harmonic dis-  tortion at a 1v rms input is about v/(11.852 + 2.822 + 1.112 + 0.2452)/315.8  = 3.87%. this value is less than 4% which is commonly used as a measure of  the upper limit of dynamic range of measuring-amplifier systems. thus, at bf,  we can process input signals over a ratio of 1000/0.875 = 1143 in amplitude,  or 1.306 x 106 in intensity. this range of intensity corresponds to a dynamic  range of 101og10(1.306 x 106) = 61.1db.  at large signals, the snr at bf improves for two reasons: the signal ampli-  tude gets larger--though not in a linear fashion, because of the agc and the  noise amplitude drops, because of the lowering of q. figure 3.30(a) illustrates  this effect for a 1v input and for a 0.9mv input. figure 3.30(b) shows a plot  of the signal amplitude and the noise amplitude for various input levels. the  signal amplitude was evaluated as the square root of the power at the bf in  spectral plots like those in figure 3.30(a); the power at the harmonic peaks  was ignored, although the power at these peaks also is due to the signal. we  evaluated the noise power by integrating the power over all frequencies in the  noise spectrum. the noise spectrum was obtained by removing all signal and  harmonic peaks in the spectrum. we interpolated the noise spectrum in the  regions where we removed the peaks. the noise amplitude was the square root  of the noise power.  figure 3.31 shows a plot of the snr (signal power/noise power) as a function  of input amplitude. as the input rms amplitude changes by a factor of about  61db in intensity (0.9mv to 1v rms), the snr changes by a factor of about  31db (1 to 1241).  figure 3.32 shows how our agc cochlea extends the dynamic range of a  hypothetical linear low-q cochlea. the linear low-q cochlea can be viewed  as being representative of just the passive basilar membrane, with no outer  hair cells [7]. thus, we call our agc cochlea with amplification (high-q) an  active cochlea, and the linear low-q cochlea a passive cochlea. some silicon  cochleas have been built with a passive cochlea acting as a front end to a bank  of bandpass filters [13].  suppose that the passive cochlea has the same gain, and the same low q,  as the active cochlea at the largest input levels of 1v rms. both cochleas  will then have the same low-q noise floor of 8.96mv at 1v. since the passive  cochlea maintains the same 0.315 gain at all intensities, its minimum detectable  signal is given by 8.96mv/0.315 = 28.4mv. the active cochlea has a high q  at small input levels such that it amplifies the input signal and the noise. at  bf, however, it amplifies the signal significantly more than the noise. in fact,wide-dynamic-range analog vlsi cochlea 87  100  10 "1  10 "2  10 -3  10 -4  10 "5  10i601 (a)  ~ 1v input "1-  >  v  o  = ~ ~ ~ = = ~ =  10 2  frequency (hz) 10 3  >  e <  e  c) 100  10 -1  10 .2  10 -:0 -4 ::: snicgi::l (bf at tap 30)  (b) o o o  r x x x o o  x x o o o o  x  x  x x  10 .3 10 .2 10 -1  input rms amplitude (v) 00  figure 3.30 signal-and-noise amplitude characteristics. (a) the output spectrum of tap  30 for a 1v rms and 0.gmv rms input at 13f shows the adaptation in q and consequent  reduction in noise. (b) the output rms amplitude of the signal and of the noise at different  input rms amplitudes are shown.  its minimum detectable signal occurs when a 0.82mv input at bf has been  amplified up by a factor of 59 to be just above the noise floor, which has now  increased to 48.4mv. 2 thus, the active cochlea extends the dynamic range  of the passive cochlea by having the minimum detectable signal decrease by  201og10(28.4/0.82 ) db = 31db! it is known that outer hair cells in the biological  cochlea extend the lower end of our dynamic range of hearing by about 40db.88 neuromorphic systems engineering  10 4  i:~ 10 3  g3 "5 z,  e ~0 ~ .~  f- ~'~  ._  c~  n ~0 ~  o tap 30 at bf  0  o o  o  o  0  0 0 0  ~Â°~Â°o-' +o -~ +o -+ ~o-' +o Â°  input rms amplitude (v)  figure 3.31 snr amplitude characteristics. the output snr improves by about 30db  (1 to 1241) as the signal changes intensity by about 60db (0.gmv to lv)  figure 3.33(a) shows the noise spectra of various taps from tap 1 to tap  37. the corner frequency of the noise spectra successively decrease from tap  1 to tap 37, while the noise per unit bandwidth successively increases. the  peak height increases and converges to an asymptotic limit as we traverse the  cochlea. note that, because of offsets in the cochlea, there is an abrupt re-  duction in corner frequency between taps 21 and 25. this abrupt reduction in  bandwidth lowers the noise below the theoretical value that we would expect  for taps close to and beyond this region. the total integrated noise over all  frequencies is shown in figure 3.33(b). the noise increases due to accumulation  and amplification as we traverse the filters of the cochlea. however, the suc-  cessive lowpass filtering limits this growth, until, in the asymptotic limit, there  is an equilibrium between noise accmlmlation and noise filtering, and the noise  ceases to grow. the discontinuities in the curve around tap 21 to tap 25 are  due to the abrupt reductions in bandwidth around this region. the eventual  convergence of the noise is due to the exponential taper of the cochlea: the  exponential taper results in an accumulation of noise terms with coei~icients  that are determined by the terms of a geometric series with geometric ratio r  (eq. (3.26)). since r < 1, the geometric series converges. note that, as we  increase the number of filters per octave, by eq. (3.25), we increase r, and the  noise increases. there is thus a tradeoff between the sharpness of the rolloff  slope of the cochlea, which increases with noct, and noise reduction. the noise  is also extremely sensitive to the value of q because of the sensitive dependence  of the h(rkx) terms of eq. (3.26) on q. we used q = 1.5, and noct = 11.2wide-dynamic-range analog vlsi cochlea 89  >  e <  e  o 100  10 1  10 -2  10;0_ 4 o -- l~e'ai ~,~ cochiea  x -- hypothetical linear low-q (passive) cochlea  0 0 o ~ 0  0  0  o Ã  o  high-q noise floor (48.4mv) x  l low-q noise floor "~.9~'-mv"  i extension in dynamic dynamic range of  range ~ i ~ passive cochlea  ~ v ~ ~  i  o.82mv [ 28.4mv 1v  10 3 10 "2 10 "1 100  input rms amplitude (v)  figure 3.32 extension of dynamic range. a hypothetical low-(~ cochlea that is com-  pletely linear would have a dynamic range of only 30db due to the uniformly low gain of  0.315 at all amplitudes; such a cochlea is analogous to the passive biological cochlea with  no outer hair cells. our agc cochlea has a dynamic range of o0db because faint signals at  0.82mv are amplified by a factor of 59 to be just above the noise floor of 48.4mv, whereas  loud signals at iv rms amplitude are attenuated by a factor of 0.315 to prevent distortion;  such a cochlea is analogous to the active cochlea with outer hair cells.  as a good compromise between not having too much noise at the output of the  cochlear taps, and not having broad filters with shallow rolloff slopes.  figure 3.34(a) shows the minimum detectable signal and maximum undis-  torted input at the bf of each tap in the cochlea. the minimum detectable  signal was measured as described earlier in this section. the maximum undis-  torted input was measured by finding the input rms value at which the second  harmonic (by far the dominant harmonic) was attenuated by 25 db when com-  pared with the first harmonic. we observe that the maximum undistorted input  is nearly constant at 1v rms, except for the first few taps, where the action  of the strong agcs at each tap have not accumulated sufficiently to reduce  the distortion. 3 figure 3.34(b) shows the dynamic range at various taps. the  dynamic range varies from about 59db to 64db. the early taps have little ac-  cumulation of noise or gain, in contrast with the late taps, which have large  accumulation of noise and gain. the effect of gain regulation in the agc causes  the accumulation of noise and of gain to be approximately in balance, so the  dynamic range does not suffer a huge variation across taps. however, as we10 -1  (a)  ~ 10_2  ~1.  > 10 "a  e  .~ 10-4  13. ~/3  .~ 10 -s  o z  10 -6  10 37 90 neuromorphic systems engineering  101 102 103 104  frequency (hz)  >  o z  -~  >  $  i~_  (2)  n  ~- 100  10 -1  10-~i  10 -31 0 (b)  0  0  0 0  0 0 0 0  0 0 0 0  o 0000  lb 2b 3~ 40  tap number  figure 3.33 noise accumulation across cochlear taps. (a) the noise spectra at var-  ious cochlear taps are shown. (b) the total output noise integrated over all frequencies  asymptotically converges due to the exponential taper of the cochlea. the discontinuities in  the curve are due to the discontinuous reduction in bandwidth, in turn due to chip offsets,  between taps 21 and 25 in (a)  would expect, figure 3.35 shows that the maximum output snr at bf (the  snr at 1v rms input) falls as we traverse the cochlea. it is maximum at tap1  (6.74 x 104 or 48.3db) where there is the least noise, and minimum at tap 37  where there is the most noise (649 or 28.1db). the discontinuities, due to thewide-dynamic-range analog vlsi cochlea 91  100  o  >  ~ 10 4  n  ..~  â¢ n_ 10 -a  ~_  <  ffl  ~ 10.3  $  ~_  c- __  10 ,4 0  (a) ~~,~ - ~ ,~ ~ ~  0 -- max. undistorted input  x -- min. detectable input  x x xx  xxxxxx x  lb 2b 3~ 4o  tap number  v  e 65  64 (b)  63 o  62  61  60  59  o  58  57  5e o 0 0  0 o  o  1~ 2b 3~  tap number o  0  0  o  40  figure 3.34 dynamic range across cochlear taps. (a) the minimum detectable input  and maximum undistorted input at various cochlear taps are shown. (b) the dynamic range  at bf at various cochlear taps are shown.  cf offset of the cochlea around taps 21 to 25, are evident in figure 3.34 and  figure 3.35.  3.3.5 spatial characteristics  figure 3.36(a) shows the spatial response of various taps of the cochlea to  a 162hz input, for various amplitudes. to understand the similarity of  figure 3.36(a) to figure 3.23(a), we can view the cochlea as performing a  frequency-to-place transformation with -log(f)-~x [7]. even the harmonic-  distortion plot of figure 3.36(b) is quite similar to that of figure 3.25(a). the  most severe distortion occurs at a place that corresponds to a corner frequency92 neuromorphic systems engineering  10 5 ,  :o  ~ 10 4  r~  z  $  ~_ g  o 10 3  ~4 (~  10 o  o  o  o  o  o  o o  o  lb 2b 3b 4o  tap number  figure 3.35 snr across cochlear taps. the maximum output signal-to-noise ratio pro-  gressively decreases as we travel down the cochlea due to the accumulation of noise. the  numbers represent the ratio of the signal power to the noise power, that is 105 corresponds  to 50db.  that is 1 octave higher than the corner frequency at the best place (bp). fig-  ure 3.37 shows the shift in bp for two different frequency inputs to the cochlea.  3.3.6 dynamics of cain and offset adaptation  figure 3.38(a) shows the attack response of cochlear tap 30 to the abrupt onset  of a tone at the tap's bf (162hz). after a transient at the first cycle, the  envelope of the response adapts quickly to the new intensity, corresponding to  the quick onset adaptation of the peak detector. the offset correction has a  slower rate of adaptation and continues to adapt with some ringing even after  the envelope adaptation is complete.  figure 3.38(b) shows the release response of cochlear tap 30 to an abrupt  decrease in the intensity of the bf tone. the adaptation of the envelope is  much slower than that shown in figure 3.38(a) because of the slow adaptation  of the peak detector to inputs of decreasing intensity. the dc offset adaptation  continues to have a rate of adaptation that is slower than the rate of envelope  adaptation.  3.3.7 the mid-frequency and high-frequency cochleas  so far, we have dwelled almost entirely on the properties of the low-frequency  cochlea; the properties of the other cochleas are similar. figure 3.39 shows  the variation in q versus corner frequency due to bias-current differences in awide-dynamic-range analog vlsi cochlea 93  10 o  > v  "~ 10 4 ._ __ o_  e <  e tw 10-2  $  i~.  10 < ~j/  20my j /  lomv j  smv (a)  ~ ~ 3~  tap number 162hzinput  4o  > v  e .<  e  o 10 o ~Â°~'"uÂ°Â°Â°Â°Â°~oÂ°ooooooo~ooooooo _"l l~2.z~opo~  10-11 xxx x:::::::~::::::xxxx xxooÂ°ioo:2rms  10_2 (x y4y~ ~:~ ~... x â¢ xxx:~v oo !~ ~ ~y~ y~ ~ .^  0-- if ~ x o  10 .3 x -- 2f i â¢ x o  â¢ -- 3f ~ ~  10 .4 x â¢ x x  â¢ i ~o-Â° (b) ,,',  8p  ~Â°~ 1~ ~ 3~ 40  tap number  figure 3.36 spatial-response characteristics. (a) the spatial response at various input  amplitudes is remarkably similar to the frequency response at various input amplitudes be-  cause of the cochlear frequency-to-place transformation (log(f) --+ x). (b) the harmonic  distortion is filtered sharply at the best place (bp); it is at its worst at a place that has a  corner frequency that is 1 octave above that of the best-place corner frequency.  cochlear filter. there is a variation in q as we go from subthreshold behavior  at low frequencies to above-threshold behavior at high frequencies. however,  our high-frequency circuits operate in moderate inversion (near the graded  transition from subthreshold to above threshold), and thus the change in q94 neuromorphic systems engineering  100  >  ~ 10 q  .-~  ..~  ~_~  â¢ -- 10-2  o_  e <  ~ 10_3  ~- 10 4 -~  o  10 -s oo-332hz  x--162hz oooooo Â°  ~ xÃ o Â° x ~x  o o x x o  0 o0 vxxx ~00 ~vvx x^ 0 rr~xxxxx~^^  o  o  15 5mvrmsinput  xxxxxxx x  x  x  x  x  x  x  o :  o0000  0 0  0 o0  0 o'  3~ 40  tap number  figure 3.37 the frequency-to-place transformation. the best place for high frequencies  occurs earlier than that for low frequencies.  is not significant. figure 3.40 shows that, consequently the "sounds of si-  lence", that is, the noise spectra at the various taps in the low, mid, and  high-frequency cochleas are similar in shape across the entire frequency range  (100hz to 10khz).  3.4 analog versus digital  the total resting current consumption of all three of our cochlear cascades  was measured to be 95#a. playing microphone speech through our cochleas  increased the power consumption to about 99~a. thus, the total power con-  sumption of our cochlea is about 100#a Ã 5v = 0.stow. our area consumption  was 1.6mmÃ1.6mmÃ3 = 7.7 mme in a 1.2#m process. the pitch of a single  cochlear stage, including all scanning circuitry and with a conservatively large  number of power buses (to prevent unwanted coupling through the supplies),  was 102 #mÃ 444 pm.  the high-frequency cochlea consumes more than 3/4 of this power. we can  easily cut our power dissipation to 0.2mw by having a tilt on the va voltages,  although we did not implement this tilt on our current design. if only telephone  bandwidth is required, we can do away with the high-frequency cochlea and cut  our power dissipation to 0.125mw. if we implement the tilt on the va voltages  and do not use the high-frequency cochlea, then our power consumption reduces  to 50#w.  we next compare the power and area consumption of our analog cochlea, an  asic digital cochlea, and a noncustom microprocessor (ttp) cochlea. we begin  by describing the design of the asic digital cochlea.wide-dynamic-range analog vlsi cochlea 95  >  0 > a'ltack respc~s~e  (a) input  (bf tone)  output  (tap 30)  0:5 ;  time (seconds)  >  0 ~e~ease__eespo~e  ~ input  (bf tone)  (b)  output  (tap 30)  i r ~ i  0 5 1 time (sec165nds) 2 2.5  figure 3.38 agc and offset adaptation. (a) at the onset of a loud input tone after  a period of silence, there is a brief output transient followed by quick adaptation of the  envelope. the offset adaptation occurs in parallel with the envelope adaptation, which  happens on a much slower time scale. (b) the reduction in the intensity of a loud input  tone causes a gradual adaptation in the envelope of the signal. the offset adaptation is still  slower than the envelope adaptation, but the time scales are more comparable.  3.4.1 the asic digital cochlea  figure 3.41 shows a block-level schematic of a digital cochlea, similar to our  analog cochlea, and described in [12]. second-order recursive digital filters with96 neuromorphic systems engineering  1.5  v  ~ 1  Â¢~0.5 000 o0 0 o0 o0 0000000000000000~ .......  ?o 1 ...... i'~ 2 ...... i~ 3 ...... i~' ...... io '  corner frequency (hz)  figure 3.39 the (~ across cochlear filters. the ~) across various cochlear taps is fairly  well matched.  i0 -i ........ , ................. , ........ , .......  ~, 10 -2  "1-  ~- 10-3 >  ~ ~o -~  ~1.  ._m  o 10-5  z  frequency (hz) 1Â°;oÂ° ...... i~' ...... i~ 2 ...... i6 3 ...... i6' ...... i0 5  figure 3.40 the sounds of silence. the noise spectra at various cochlear taps from the  low, mid, and high-frequency cochleas are fairly similar in shape.  tapering filter coefficients model the basilar membrane. half-wave rectification  circuits (hwr) perform msb lookup to model the inner hair cells. automatic-wide-dynamic-range analog vlsi cochlea 97  vn-1  cross-  talk r vo  figure 3.41 the asic digital cochlea  gain-control circuits (agc) with cross talk model the olivocochlear efferent  system. the multiscale agc is modeled over 4 time scales.  this is a custom cochlea, designed to be as efficient in power and area con-  sumption as possible. a digital input, clocked at 50 khz, forms the input to  the cochlea; that frequency is slightly over the nyquist frequency of 36khz for  the highest-frequency location of the cochlea, and is necessary to obtain robust  behavior with the filtering and nonlinear operations in the cochlea. it is possi-  ble to implement a multirate sampling system, but calculations show that the  bandwidth needed to implement 95 stages of the cochlea from 18khz to 100hz  (as in the analog cochlea) is equivalent to the bandwidth needed to implement  17 stages at 18khz. thus, a multirate system can help only by a factor of 5.6.  if the overhead in circuitry and complexity needed for a multirate system is  factored in, there may be no advantage whatsoever. thus, we shall confine  ourselves to a system with only one rate of sampling. note that we need only  95 stages in the digital cochlea (as opposed to 117 stages), since we do not  need the redundancy of the overlapping-cascades architecture. to handle the  input dynamic range of 60db, (i.e., 10 bits), it is necessary to do fixed-point  operations at a precision of approximately 24 bits; otherwise, overflow errors  and round-off-error accumulation can seriously jeopardize the computation.  the system shown in figure 3.41 is implemented most efficiently with a  bit-serial representation, where the bits are processed serially, and each filter,  hwr, and agc block is reused 95 times to compute the effect of the entire  cascade. the reuse of circuitry results in tremendous savings in area and power,  and makes a digital cochlear implementation feasible on a single chip. there  is, of course, overhead in the storage that is necessary to implement these  computations.  the proposed asic digital cochlea was never built. however, we can esti-  mate what its power dissipation would have been. the clock rate is 50 khz  Ã 95 stages Ã 24 bits = ll4.0mhz. the power supply would need to be about  2.0 v to attain a ll4.0mhz clock rate. let's assume that the technology is 0.598 neuromorphic systems engineering  table 3.1 cochleas  analog asic digital dec a  tech. 1.2pm 0.5 #m 0.5#m  vdd 5v 2v 3.3v  power 0.5row 150row 50w  area 7.tram 2 25 mm 2 299 mm ~  #m. the number of gates needed for the computation is roughly 40 (number  of gates for 1 multiply operation, including storage overhead) Ã 24 (number of  bits) Ã 7 (3 multiplies in filter and 4 in the agc) -- 6720 gates + ram and  rom. the 13 add operations comprising 5 adds in the filters and 4 Ã 2 adds  in the agc are treated as being essentially free in fixed-point computations.  the gate.hz = 6720 Ã x ll4mhz = 0.77 x 1012 gate hz. the gate capacitance  = (0.5 #m x 0.5) #m x 10 (transistors per gate) x 2 ff (cap. per unit area)  = 50 ff. the switching energy per gate = 50 ff x (2.0) 2 -- 2.0 x 10-13j. the  power dissipation is therefore 0.77 x 1012 gate.hz x 2.0 x 10 -13 = 0.154w,  which we shall round down to 0.15w. the area we would need to build this  chip is estimated to be 5 mm x 5 mm (in 0.5 #m tech.) -- 25 mm z.  3.4.2 #p cochlea  in flops, we need about 50 khz (bandwidth) x 95 (number of stages) x 20  (7 multiplies and 13 adds) = 95 mflops to implement our cochlea. note that  adds cannot be treated as free in floating-point operations. on the specfp92  ear program, the dec 21164 running on an alpha server 8200 5/300 does  about 1275 times better than a vax 11/780. the vax 11/780 is specified at 0.1  mflops. thus, the dec a is capable of 1275 Ã 0.1 = 127.5 mflops which  is enough for our computation, the dec a consumes 50 w and has an area  of 16.5 mm x 18.1 mm = 299 mm 2.  3.4.3 comparison of analog and digital cochleas  table 3.1 compares the power and area consumption of the various cochleas.  note that our analog cochlea would be more efficient in area by about a factor  of 2 to 4 if it were also implemented in a 0.5#m technology like the digital  designs. however, we have not scaled down the analog numbers; we have just  shown them for our current 1.2#m technology.  the analog implementations are more efficient in power than are custom  digital implementations by a factor of 300, and than are noncustom #p imple-  mentations by a factor of i x 105. the analog cochlea can run on lab batterieswide-dynamic-range analog vlsi cochlea 99  for more than a year (with 100#a current consumption), whereas the best dig-  ital cochlea would be able to run for only less than 1 day (with 75ma current  consumption).  the area comparisons show that, even in an inferior technology (1.2 #m vs.  0.5 #m), the analog cochlea is about 3 times more efficient than is the custom  asic cochlea, and is about 40 times more efficient than is the microprocessor  implementation.  the cochlear comparisons were generous to digital implementations: we  used a better technology (0.5 #m versus 1.2 #m), operated with a power-saving  supply voltage (2.0 v versus 5.0 v), used an efficient bit-serial implementation,  did not include the cost of the 10-bit or 13-bit a/d converter, and were more  conservative in our cost estimates. nevertheless, the analog implementations  were two to five orders of magnitude more efficient than the digital implementa-  tions. to compete with digital systems, the analog systems had to be designed  with wide-dynamic-range circuitry, and had to compensate for their offsets.  in fact, most of the analog cochlea's resources in area were expended in filter  linearization, low-noise transduction, and offset-compensation circuitry. most  of the analog cochlea's resources in power were expended in low-noise sensing  circuitry. the number of devices needed to do the actual computation was  nevertheless so small that 117 stages could be implemented easily on one chip,  with room to spare.  by contrast, the digital cochlea's resources in area and power were not pri-  marily consumed in maintaining precision, although extra bits were necessary  to prevent overflow and roundoff errors. rather, the actual computation was so  expensive in digital that only one stage of the cochlear cascade was feasible on  a single chip. that stage had to be reused 95 times in succession, at a fast rate  of ii4mhz, to finish the computation in real time. in other words, the analog  implementation was slow per computational stage, cheap, and completely par-  allel. the digital implementation was fast per computational stage, expensive,  and fully serial. we might wonder--if the digital implementation were slow  and fully parallel just like the analog one, would the comparisons in efficiency  seem less drastic? the answer is yes for power consumption because it could be  reduced by turning down the power-supply voltage and clock frequency. the  answer is no for area consumption, because it would be 95 times worse. in this  particular case, however, the size of the chip required for the parallel digital  implementation would be totally unfeasible. in other words, there is no free  lunch: the inefficiency of using a transistor as a switch will always show up  somewhere.  3.5 the biological cochlea  the biological cochlea is far more complex than is our electronic cochlea, and it  is surprising that we can replicate much of its functionality with just our simple  circuits. our aim is not to replicate its functions exactly, as computer modeling  attempts to do, but rather to exploit its clever computational ideas to build100 neuromorphic systems engineering  more efficient electronic architectures for artificial hearing. such architectures  may enable the design of superior hearing aids, cochlear implants, or speech-  recognition front ends. in addition, as we shall show in section 3.5.1, the  synthesis of an artificial cochlea can help us to improve our understanding of  how the biological cochlea works.  the functions of the biological cochlea that we can replicate are:  1. the frequency-to-place transformation, as implemented by the amplifica-  tion and propagation of traveling waves  . a compressive nonlinearity at and beyond the bf of a cochlear tap. like  the biological cochlea, our response is linear for frequencies well below  the bf. our compression is achieved through an agc. in the biological  cochlea, it is still a matter of debate as to how much of the compression  arises from a dynamic agc and how much from a static nonlinearity.  we have reported on cochleas where the compression arises solely from a  static nonlinearity as well [24].  3. an asymmetric attack and release response to transient inputs.  4. the extension of dynamic range due to active amplification. our dynamic  range is extended from 30rib to about 60db. in the biological cochlea, it is  believed that amplification by outer hair cells extends the dynamic range  of the cochlea by about 40db.  . the broadening of the pattern of excitation as the input intensity is in-  creased. the dual effect, which we can also model, is the broadening of  the frequency-response curves as the input intensity is increased.  . the shift of the peak frequency towards lower frequencies as the input  intensity is increased. the dual effect, which we can also model, is the  shift of the peak place of excitation toward the input of the cochlea as  the intensity is increased.  7. a sharp cochlear roll-off slope.  . masking of adjacent frequencies and harmonics due to the effects of the  agc and nonlinearity, respectively. however, our dominant harmonic is  the second harmonic. in the biological cochlea, the dominant harmonic  is the third harmonic.  3.5.1 3yaveling-wave architectures versus bandpass filters  why did nature choose a traveling-wave architecture that is well modeled by  a filter cascade instead of a bank of bandpass filters? we suggest that nature  chose wisely, for the following three reasons:  1. to adapt to input intensities over a 120db dynamic range, a filter bank  would require a tremendous change in the q of each filter. to compresswide-dynamic-range analog vlsi cochlea 101  120db in input intensity to about 40db in output intensity the filter qs  must change by 80db; a dynamic-range problem in the input is merely  transformed into a dynamic-range problem in a parameter. in contrast,  in a filter cascade, due to the exponential nature of gain accumulation,  enormous changes in the overall gain for an input can be accomplished  by small distributed changes in the q of several filters.  . large changes in the q of a filter are accompanied by large changes in  the filter's window of temporal integration. thus, in filter banks, faint  inputs would be sensed with poor temporal resolution, and loud inputs  would be sensed with good temporal resolution. in contrast, in a filter  cascade, the shifts in temporal resolution with intensity change only in a  logarithmic fashion with intensity, as opposed to in a linear fashion as in  the filter bank.  . a sharp rolloff slope in a filter is extremely useful in limiting distortion,  and in enhancing spectral contrasts. a sharp rolloff slope arises naturally  in the cochlear filter cascade. to accomplish such a rolloff slope in a  filter bank requires very high-order filters, and consequently an enormous  amount of circuitry at each tap. in contrast, in the filter cascade, the  burden of creating a high-order rolloff is shared collectively, so only one  new filter needs to be added for each new desired corner frequency.  there are two problems that need to be addressed in a filter cascade:  1. a filter cascade is prone to noise accumulation and amplification. the  solution to this problem is either to have an exponential taper in the filter  time constants such that the output noise converges (the solution found  at high cfs in the biological cochlea), or to limit the length of the cascade  (the solution at low cfs in the biological cochlea). the exponential taper  also results in elegant scale-invariant properties.  . the overall gain is quite sensitive to the value of each filter's q. the  solution to this problem is to have gain control regulate the value of the  q's in the cascade. if the gain control is sufficiently strong, then the  collective adaptation in q across many filters will compress a wide input  dynamic range into a narrow output dynamic range.  3.6 applications to cochlear implants  front-end modules in current cochlear implant devices make use of parallel  banks of independent bandpass filters. for example, the front-end module of a  state-of-the-art commercial multichannel cochlear implant devices consists of 20  fourth-order bandpass filters with center frequencies between 250hz and 10khz.  the filters are implemented using switched-capacitor techniques. the total  power dissipation of such implementations is on the order of several milliwatts,  and the dynamic range is only 35 to 40 db.102 neuromorphic systems engineering  our neuromorphic approach mimics several aspects of the biological cochlea,  as described in section 3.5. in addition, our dynamic range exceeds 60db. our  power dissipation for a 117-stage cochlea with a roll-off slope corresponding to  a high-order filter (10th order to 16th order) is 0.5row. if we use fewer stages  and fewer filters per octave to correspond to current values in implant front  ends, we could, we estimate, cut our power dissipation to 50 #w. this power  dissipation is about 20-100 times lower than that in current front ends.  thus, in terms of biological realism, dynamic range, and power we can do  much better than current implant front ends. previously [14], we described  how a nonlinear center-surround operation on the outputs of the cochlear taps  can convert cochlear lowpass information into bandpass information without  degrading the temporal resolution at that tap. a neuromorphic front-end mod-  ule like ours satisfies the fundamental requirements of future cochlear-implant  speech processors [3].  3.7 conclusions  we described a lit-stage 100hz-to-10khz cochlea that attained a dynamic  range of 61db while dissipating 0.5row of power. the wide dynamic range  was attained through the use of a wide-linear-range transconductance ampli-  fier, of a low-noise filter topology, of dynamic gain control (agc), and of an  overlapping-cascades architecture. an infrastructure of automatic offset adap-  tation, small amounts of low-frequency attenuation, and scale-invariant bic-  mos circuit techniques provided robust operation. the low power, wide dy-  namic range, and biological realism suit our cochlea to be used as a front end  for cochlear implants. the design of our electronic cochlea suggests why nature  preferred an active traveling-wave mechanism over a bank of bandpass filters  as a front end for hearing.  notes  1. we are assuming that the supply voltage limits the range of operation of the system.  if there is some other voltage that limits the range of operation of the system, then power is  wasted through an unnecessarily high supply voltage. we choose not to operate the system  in this nonoptimal situation.  2. these numbers (gain of 59, noise of 48.4mv, and minimum detectable signal of 0.82mv)  are slightly different from the numbers that we quoted earlier (gain of 57.1, noise of 50mv,  and minimum detectable signal of 0.875mv) because of the interpolation procedures used  in our data processing algorithm, and because of the different times at which the data were  collected.  3. we were able even to apply a 1.4v input rms signal and to keep the distortion under  25 db (due to the strong agc), but we refrained from doing so because the input signal then  would be just at the edge of our dc operating range; operating the cochlea at this extreme  is possible, but we chose not to so as to leave a safety margin.  references  [1] n. bhadambkar. a variable resolution nonlinear silicon cochlea. technical  report csl-tr-93-558, stanford university, 1993.wide-dynamic-range analog vlsi cochlea 103  [2] t. d. clack, j. erdreich, and r. w. knighton. aural harmonics: the  monoaural phase effects at 1500hz, 2000hz, and 2500hz observed in tone-  on-tone masking when fl = 1000hz. journal of the acoustical society of  america, 52(2):536-541, 1972.  [3] g. clark. cochlear implants: future research directions. annals of otology,  rhinology, and laryngology, 104(9):22-27, 1995.  [4] b. m. johnstone. enesis of the cochlear endolymphatic potential. current  topics in bioenergetics, 2:335-352, 1967.  [5] w. liu. an analog cochlear model: signal representation and vlsi real-  ization. phd thesis, john hopkins university, baltimore, maryland, 1992.  [6] c. a. mead. analog vlsi and neural systems, pages 179-192, 279-302.  addison-wesley, reading, ma, 1989.  [7] b. c. j. moore. an introduction to the psychology of hearing, pages 47-83.  academic press limited, london, 3 edition, 1989.  [8] m. a. ruggero. response to sound of the basilar membrane of the mam-  malian cochlea. current opinion in neurobiology, 2:449-456, 1992.  [9] r. sarpeshkar, t. delbrfick, and c. mead. white noise in mos transistors  and resistors. ieee circuits and devices, 9(6):23-29, november 1993.  [10] r. sarpeshkar, lyon r. f., and c. a. mead. an analog vlsi cochlea with  new transconductance amplifiers and nonlinear gain control. in proc. ieee  intl. conf. on circuits and systems, volume 3, pages 292-295, atlanta,  may 1996.  [11] r. sarpeshkar, r. f. lyon, and c. a. mead. a low-power wide-linear-  range transconductance amplifier. analog integrated circuits and signal  processing, 13:123-151, may 1997. published jointly.  [12] c. summerfield and r. f. lyon. asic implementation of the lyon cochlea  model. in proc. ieee intl. conf. on acoust. speech and signal proc., san  francisco, 1990.  [13] a. van schaik, e. pragni~re, and e. a. vittoz. improved silicon cochlea  using compatible lateral bipolar transistors. in david s. touretzky,  michael c. mozer, and michael e. hasselmo, editors, advances in neu-  ral information processing systems, volume 8, pages 671-677. the mit  press, 1996.  [14] r. j. w. wang, r. sarpeshkar, m. jabri, and c. mead. a low-power  analog front-end module for cochlear implants. in xvi world congress  on otorhinolaryngology, sydney, march 1997.  [15] l. watts, d. kerns, r. f. lyon, and c. mead. improved implementation  of the silicon cochlea. ieee journal solid-state circuits, 27(5):692-700,  may 1992.speech recognition  experiments with silicon  auditory models  john lazzaro and john wawrzynek  cs division,  university of california at berkeley,  berkeley, ca 94720-1776.  l~zzaro~Â¢s.berkeley.edu  4.1 introduction  neurophysiologists and psychoacousticans have made fundamental advances in  understanding biological audition. computational models of auditory process-  ing, which allow the quantitative assessment of proposed theories of auditory  processing, play an important role in the advancement of auditory science.  in addition to serving a scientific function, computational models of audition  may find practical application in engineering systems. human performance in  many auditory tasks still exceeds the performance of artificial systems, and the  specific characteristics of biological auditory processing may play an important  role in this difference. current engineering applications of auditory models  under study include speech recognition [9, 13, 31], sound separation [8], and  masking models for mpeg-audio encoding [7].  computation time is a major limitation in the engineering application of au-  ditory models. for example, the complete sound separation system described  in [4] operates at approximately 4000 times real time, running under unix on  a sun sparcstation 1. for most engineering applications, auditory models  must process input in real time; for many of these applications, an auditory  model implementation also needs to be low-cost and low-power. examples of  these applications include robust pitch-tracking systems for musical instrument  applications, and robust feature extraction for battery operated speech recog-  nizers.106 neuromorphic systems engineering  one implementation approach for auditory models in these products is to  design low-power special-purpose digital signal processing systems, as described  in [5]. however, in many of these potential products, the input takes an ana-  log form: a voltage signal from a microphone or a guitar pickup. for these  applications, an alternative architecture is a special-purpose analog to digital  converter, that computes auditory model representations directly on the analog  signal before digitization.  analog circuits that compute auditory representations have been imple-  mented and characterized by several research groups - these working research  prototypes include several generation of cochlear models [25, 27, 32, 90], period-  icity models [5, 26], spectral-shape models [16, 32], and binaural models [1, 4].  a prime benefit of these circuit structures is very low power consumption:  the circuit techniques used in most of these prototypes were originally developed  for wristwatch and pacemaker applications. for example, a recent publication  on cochlear design techniques reports a 51-channel cochlear filterbank that  consumes only 11 microwatts at 5 volts [90]. voltage and process scaling, and  advances in circuit design, could reduce power consumption even further.  if auditory models offer a performance advantage over standard signal pro-  cessing techniques in an application, and a compact implementation that only  consumes a few milliwatts of power is needed, a hybrid system that couples a  special-purpose analog to digital converter with a low-power digital processor  may be a competitive alternative to a full-digital implementation. however,  even if auditory models only offer comparable performance to standard tech-  niques for an application, an analog auditory model implementation may be  the best choice for front-end processing, if the system requires microwatt op-  eration (for example, size limitations dictate a lithium watch battery power  source). for such micropower systems to become a reality, micropower im-  plementations of pattern-recognition functions must also be available. recent  implementations of micropower pattern classification systems [6] and hidden  markov model state decoders [22] are examples of progress in this area.  standard analog performance measurements (s/n ratio, dynamic range,  ect.) aren't sufficient for determining the suitability of analog implementa-  tions of non-linear, multi-stage auditory models for a particular application.  this paper documents a more direct approach to evaluating analog auditory  models: we have integrated a multi-representation analog auditory model with  a speech recognition system, and measured the performance of the system on  a speaker-independent, telephone-quality 13-word recognition task.  the structure of the paper is as follows. we begin with a brief description of  our multi-representation auditory model hardware implementation. we then  describe in detail the specific auditory representations we use in our speech  recognition experiments, and the techniques we use for generating a feature  vector suitable for speech recognition systems. next, we assess word recognition  performance of the system, and compare the results with state-of-the-art feature  extraction systems. the chapter concludes with discussion and suggestions for  further research.silicon auditory models 107  4.2 system description  we have designed a special-purpose analog-to-digital converter chip, that per-  forms several stages of auditory pre-processing in the analog domain before  digitization [15, 22]. configurable parameters control the behavior of each  stage of signal processing. figure 4.1 shows a block diagram of a system that  uses three copies of this converter chip: by configuring each chip differently,  the system produces three different auditory representations in response to an  analog input.  multi-converter system  sound input i  i Â°Â° o~ ~o~o0~o~ ~  oo 000ooo000o  ...........  ~fl ~fl~ ~, ~? ~  figure 4.1 block diagram of the multi-converter system.  this system acts as a real-time audio input device to a sun workstation:  a pre-amplified microphone input can be connected directly to the converters  for a low-latency, real-time display of spontaneous speech. alternatively, the  system can receive analog input from the 8 khz sampling rate, 8-bit mu-law  audio output of the workstation, for controlled experiments: all experiments  reported in this paper were done using this method of sound presentation. the  dynamic range of the converter chip is 40 to 60 db, depending on the signal  processing configuration in use: input sensitivity is 1 mv (peak), and maximum  recommended signal amplitude is 1 v (peak).  figure 4.2 shows the analog signal path of the auditory pre-processor in the  converter chip. processing begins with a silicon cochlea circuit [27]. a silicon  cochlea is an' analog circuit implementation of the differential equations that de-  scribe the traveling wave motion of physiological cochleas. the cochlea design  used in this chip maps a linear, one-dimensional partial-differential equation  into circuits, as a cascade of continuous-time filter sections with exponentially  decreasing time constants. the second-order filter sections have a low-pass  response, with a slight resonant peak before cutoff. the cascade acts as a  discrete-space, continuous-time finite-element approximation of the partial dif-  ferential equation.  like wavelet filterbanks, the silicon cochlea outputs balance temporal and  spectral acuity. the cochlear output response, a lowpass filter with a sharp108 neuromorphic systems engineering  audio  input  silicon cochlea  i i 1 i i l 1 1 1  sensory transduction  l i i i i i i l l  temporal autocorrelation  i i i i i i i i i  temporal adaptation  i i i i i i 1 i i  (l l ~ outpnts~  figure 4.2 analog signal path of the silicon auditory model.  cutoff and a slight resonant peak, derives its spectral selectivity from the col-  lective interaction of the slightly-resonant circuits in the series cascade, not  from parallel highly-resonant circuits as in a standard filterbank. by avoiding  highly-resonant filters, the cochlear processing preserves the temporal details  in each output channel.  this cochlear design is the first stage of processing in our chip. the cochlea  consists of 139 filter stages; we use the outputs of the last 119 stages. the first  20 outputs are discarded, because their early position in the cascade results in a  poor approximation to the desired differential equation solution. four param-  eters control the tuning of the silicon cochlea, supporting variable frequency  ranges and resonance behaviors.  next in the signal processing chain (figure 4.2) are circuits that model the  signal processing that occurs during the sensory transduction of mechanical  motion in the cochlea. these operations include time differentiation, half-wave  rectification, amplitude compression, and the conversion of the analog waveform  representation into probabilistic trains of fixed-width, fixed-height spikes [43].  each of the 119 cascade outputs is coded by 6 probabilistic spiking circuits.  note that no time averaging has been done in this signal processing chain; the  cycle-by-cycle waveform shape is fully coded in each set of 6 spiking outputs.  different secondary representations in the brain use the cochlear signal as in-  put, and produce outputs that represent more specialized aspects of the sound.  in our chip processing chain, two signal processing blocks follow the sensory  transduction block, that may be used to model a variety of known and proposed  secondary representations.  the first processing block (figure 4.2) implements temporal autocorrelation,  in a manner described in detail in [16]. the six spiking outputs associated with  each cochlear output are sent into a single temporal autocorrelator, which pro-silicon auditory models 109  duces a single output. six parameters fix the autocorrelation time constant and  autocorrelation window size at both ends of the representation; autocorrelation  parameters for intermediate taps are exponentially interpolated.  the temporal autocorrelation block can be configured to generate a repre-  sentation that codes the spectral shape of a signal. to generate this spectral  shape representation, the autocorrelator associated with each cochlear channel  is tuned so that the best frequency of the cochlear channel matches the first  peak of the autocorrelation function [30]. figure 4.3 illustrates the algorithm:  figure 4.3(a) shows the frequency response of a cochlear output, figure 4.3(b)  shows the output of a temporal autocorrelator tuned to the best frequency of  the cochlear output, and figure 4.3(c) shows the effect of combining the tem-  poral autocorrelator and cochlear filter. by cascading the cochlea and temporal  autocorrelator blocks, a narrow, symmetrical filter is created; this filter is non-  linear, and achieves a narrow bandwidth without using a highly resonant linear  filter.  7oo 5oo ~ ~  r.~ 3oo  ]00  0,01 0.1 i  khz  (~) ?oo  5o0  300  ioo  10 0,01 .  o.1 , ,, i ~  0,01 0.1  khz khz  (b) (c) i 10 250  ~ 15(  50  figure 4.3 periodicity-based spectral shape computation: (a) silicon cochlea tuning re-  sponse (b) temporal autocorrelator tuning response (c) the combination of cochlear and  autocorrelator processing.  the final processing block in the signal processing chain (figure 4.2) imple-  ments temporal adaptation, which acts to enhance the transient information  in the signal. figure 4.4 illustrates temporal adaption: in response to a tone  burst (top trace), the circuit produces a series of pulses (bottom trace). the  number of pulses per second is highest at the onset of the sound, and decays  to a lower rate during the unchanging portion of the tone. five parameters fix  the time constant and peak activity rate of temporal adaption at both ends  of the representation: parameters for intermediate taps are exponentially in-  terpolated from these fixed values. these parameters support a wide range of  adaptive responses, including temporal adaptation behaviors typical of audi-  tory nerve fibers, as well as behaviors typical of on-cell neurons in the cochlear  nucleus. the circuits used in the temporal adaptation block are described in  detail in [177].110 neuromorphic systems engineering  ~ ',,-;.=..:~.%,  , , :. ;~ ~-:~  figure 4.4 temporal adaptation: top trace is audio input (gated tone burst), bottom  trace shows adaptive response. bar length is 5ms.  as shown in figure 4.4, the final outputs of the auditory model take the  form of pulse trains. these pulses are fixed-width, fixed-height, and occur  asynchronously; they are not synchronized by a global clock. the information  sent by a spike is fully encoded by its moment of onset. in collaboration  with other researchers, we have developed efficient methods to transmit the  information from an array of asynchronous spiking circuits off chip [11], and to  combine the information from several chips to form a single data stream in an  efficient way [20]. we use these methods in our multi-representation system.  figure 4.5 shows the programmer's model of this data stream. data from the  system takes the form of a list of "events": each event corresponds to a single  spike of an output unit from a chip in the multi-representation system. each  event includes information specifying the chip sending the spike, the cochlear  channel associated with the spike, and the moment of onset of the spike. the  onset timestamp has a resolution of 20#s; event lists are strictly ordered with  respect to onset times.  data format for an event (32 bits)  xxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxx  16-bit timestamp 4 output unit  (lsb - 20gs) chip number |  figure 4.5 programmers interface for events.  we designed a software environment, aer, to support real-time, low-latency  visualization of data from the multi-converter system [15]. the environment  also supports a scripting language for the automatic collection of system re-  sponse to large sound databases.silicon auditory models 111  4.3 representations for speech recognition  we configured our multi-representation system to generate specialized represen-  tations for speech analysis: a spectral shape representation for voiced speech, a  periodicity representation for voice/unvoiced decisions, and an onset represen-  tation for coding transients. figure 4.6 shows a screen from aer, showing these  three representations as a function of time: the input sound for this screen is  a short 800 hz tone burst, followed by a sinusoid sweep from 300 hz to 3 khz.  for each representation, the output channel number is plotted vertically; each  dot represents a pulse.  the top representation codes for periodicity-based spectral shape. for  this representation, the temporal autocorrelation block generates responses as  shown in figure 4.3, and the temporal adaptation block is inactive. spectral  frequency is mapped logarithmically on the vertical dimension, from 300 hz to  4 khz;'the activity in each channel codes the presence of a periodic waveform  at that frequency. the difference between a periodicity-based spectral method  and a resonant spectral method can be seen in the response to the 800 hz sinu-  soid onset: the periodicity representation shows activity only in a narrow band  of channels, whereas a spectral representation would show broadband transient  activity at tone onset.  the middle representation is a summary autocorrelogram, useful for pitch  processing and voiced/unvoiced decisions in speech recognition. this represen-  tation is not raw data from a converter; software post-processing is performed  on a converter's output to produce the final result. the frequency response  of the converter is set as in the onset representation; the temporal adaptation  response, however, is set to a looms time constant. the converter output pulse  rates are set so that the cycle-by-cycle waveform information for each output  channel is preserved. to complete the representation, a set of running autocor-  relation functions x(t)x(t - ~-) is computed for t = kl05tts, k = 1... 120, for  each of the 119 output channels. these autocorrelation functions are summed  over all output channels to produce the final representation, a summary of  autocorrelation information across frequency bands. t is plotted as a linear  function of time on the vertical axis. the correlation multiplication can be  efficiently implemented by integer subtraction and comparison of event times-  tamps; the summation over channels is done by merging event lists. figure 4.6  shows the qualitative characteristics of the summary autocorrelogram: a repet-  itive band structure in response to periodic sounds. in contrast, the summary  autocorrelation function of a noise signal shows no long-term spatial structure.  the bottom representation codes for temporal onsets. for this representa-  tion, the temporal adaptation block is active, and the temporal autocorrelation  block is inactive. the spectral filtering of the representation reflects the silicon  cochlea tuning: a low-pass response with a sharp cutoff and a small resonant  peak at the best frequency of the filter. temporally, the representation pro-  duces a large number of pulses at the onset of a sound, decaying to a small  pulse rate with a 10ms time constant. the black, wideband lines at the start of112 neuromorphic systems engineering  4 khz  (log)  300 hz  0 ms  (linear)  12.5 ms  4 khz  (log)  300 hz ~= :-----'~w ~:~.~ -.::~,., ,~=:~. .... ~:, ,,~ .::~ ~;.~-  ~.~!.$  , .,~v~  _ .:i~:!~ '  .. ::~  -~ ~z. . .  ' ~ ~:.,. ~ i~;~:~22~5~,:~.~.:,i,,(:~,~.,,.:,:,~:~:.,~::.-~  .... ::~,.,,,: ~"-: ~: ~: ~" ~':~-~'~ "-~-~,~:~;~.;~:,,:.~a.~:~;'~.:~z2,~.~:::.':  ,~:~ .... :'" ' ~" a~'. ~ ~y~::~-t~'Â£-m~:~:5~:~"~'~:~  ~,~': ~" ~: ~ ~;~2~ ~ ~ ~= ~2:;::~,~.: ..~. ~: ,~ ~ ~&n ~::~-~::-~4~# :~x:~:::2;~:~:.~ ~ ~, ~ ~. ~ ~ .~# &~-~ ~: ~.:~ z~-~.,..~ ~.~ ..,~,~ .:,:~-.~+  )~.:~.:, ~: ~.~.:~ e<~':~j.~;s~:~#:~,~4~:~Â¢~:~:~,Â¢.(~,.:..aa.-::.,.:..:,.:~ ~ ~ ~: . ~'~'Â£~u~=~..:.c_~+~:~3.:~w~'~:,~'~,~:~? ~22.;:::.~,.{ t i~a~:,,;Â¢: ~:~~:~ â¢ .~,. ..... f~ -5;~:~:j~{~:if;~;:,~:~:  ~,.~:.::. ,~ ~ ~ ~::~':~ i'~%~,~.~:;Â£~,~:~:~:~5~;%-2~  :~$~i ~ ~o;<)~4~ ~ ~;+;~":~ ~;~:~::~::~a~: :~ :5'~: . ~. ~.~ ~ ~.~ ,, ~ ~ ~.- ~;~.~,~ ~..~..~ .~.~.-~.~ â¢ f~ ~,~ ~ ~:~x~ ~ ~ ~ ~ ~,~ ~- ,- ,,, ~., ............. ............. ":,  "~'":" ~ ~ ~ ~ !' ~ /$;~ ~ f/~ ~" ~%c~(,~:~;';~:&'~::'~?~2:~:~.::~:~  â¢ â¢ â¢ .~ ~g, ~ ~ ~ .,..,v~, ~ t~ ~v ,,~Â¢;,c.~- ~:~.- .,a~- ~ ~Â¢~,~.:~.~,~ .~,,,,~.:~,~.,~Â¢  - ~ :~)~; ~$~z:i~,~,]]~1/,~':~'~.'~:~;~:'~5":~:;~  ~;~::~.5 ~ ~,:/~::~ :~4~~ ~:.::~,-':~: .'; :' ,:~:,~::~,  ~ ~.:~.;~:~ '~:;~;;.;i~:~::;:~::~: ....  ......  ~::.:::;,'.: ~:~!: ~,;.';:~:,~f~':-~;:  ~.?.~:~..~: :: ..................... ~. :. i~:r:.::~::)-?~;~l::  ~: ~: .~~ ~.~ $ ~: ~)~ :::: :.~ â¢  " ~.?~:'~(~::l  $~ ~ â¢ ~  [~.  i.~  l__i  200 ms spectral  shape  summary  auto  corr.  onset  figure 4.6 data from the multi-converter system, in response to a 800-hz pure tone,  followed by a sinusoidal sweep from 300hz to 3khz.  the 800 hz tone and the sinusoid sweep illustrate the temporal adaptation; the  tuned response throughout the sinusoid sweep illustrates the low-pass spectral  tuning.  figure 4.7 shows the output response of the multi-converter system in re-  sponse to telephone-bandwidth-limited speech; the phonetic boundaries of thesilicon auditory models 113  two words, "five" and "nine", are marked by arrows. the vowel formant in-  formation is shown most clearly by the strong peaks in the spectral shape  representation; the wideband information in the "f" of five is easily seen in  the onset representation. the summary autocorrelation representation shows  a clear texture break between vowels and the voiced "n" and "v" sounds.  ,~.~  ~ ~ ii~t, ~im ~ t~ ~  f~.:::.... ..... i - ve .  .~ ~: ...~,, ~ ': :.-.  .;:!~ z<.~- ~  f '5~ ' ~'f ~%~: :.. r â¢ " - ~:'.~)~.'}  ; ":"...~ . '.,~i~i~l~.,.~g.>~,f~i' {  ..... 2~tzir?.~!z:~g=.~g:3~:iy,.r.,~ ..... ~,~.l..g.~a~z ~ ~:..:.x:~,,  ...... ~ ~i;;.~t'{~g~ ~ ~'~:" '," ............ :';~>-.?}'}~o ,~ :  .g~,~-.{., ......  {:':.: ; ~" : {  ........... ~..  f~ i , ve  ~ ~i'~,,~?:777;:,~,~:~;",:.~:,: ~: ; .,:, ..,,:77 ?, "  ~ g~:,.-a~ ": ::.~; ,~ ; ,. i ~ ". ~ ~ a.,:2,..~'~ ~a "~'~ ~ { ~ = ; ~ 4~ ~.;~, ,7 1~" '~.:~ ~.'.'Â¢~. :~'.* ", .~'. :j~,x' ,- , .,.,,, ~,~.~: ,,.g.,~,.<..::y: ~a~ ~ . ~: ;..~::.  ~ ~;:~'.y~: e3~.,~;..~r:9,: ,.~.,.~,~,tÂ¢~,~7~Â¢. r g~ ~,~ ~.,#.,;Â£.~.~{,: #/:~ ~.',.~ ;'-~{~,(.g~ ~ ,~ ', ,.  â¢ , -:'=. ,," ~ .~ ~"'"." : ' ".', "'.'~m.'w, :~-'.~. ~,~ , ,..~.~.,~,.,~. ~'~:~,,:~,':... ..7:cx~.~: "::Â¢~':-"Â¢~:~:',~,~,./~%, "'~=~'a'.'~+"v -  g~)a~.,,'~g,.g~9.v.:i4:t~.,~.d~.;::~{.Â¢~.a;~j~ , .~a~?::,z~n~f,#:~.;:~,:;~{4 2, :.  â¢ ,~.~*i;.;~.?~ ~5"~d~.,~2" ~. %~-iv ~a. ~:~  nn~:,~,~f~.~,'~i ~2..i.~:3~%.~?.Â¢. "~" '" ~3.~- r,-~: ~f,  ~' ~'" ::.~,~-~..r:.. .,.-.-'.-r,. . ~.a-.~.,. ~.... ~ ~a~*~, ~Â¢i.:~d:%~i.~ ,,,, .~, ~2~.i~#:t~'~.~ ,,-.~ ...... â¢ .~ ......... ~ ,.. ~.,q ..........  ' ~:~"55~~.~5~35',~'~:g.~>~& ;~'~d~'~;~:.'::'"  ! ~ ;~ 7~ >d g{4 g;:::~&.2k x:.,. ~;.. ~*e: ;'.'<::~gg...  { ~'.'~"2g-~v.~.-,.gk:Â¢~,.,~;,:.,.-: ;.~,:,,~';Â¢.'~qu~'.<,~  ~ ~f.~#~,~?y~,h~9>?<~eee-~a'~'Â¢#~'- ~ ~ ~.:,',~,'..Â¢" .,,,~ *,.,, :%,':,q~,,~:~:~',-,a~a = :~4a~'~, ~ ~ ?~i~.h,f~:~..~: e~."~ :  ~*~!/;iei{~'~*~.~ z~+~Â£~4'~ Â¢.2: &' gq~:;~.~z, le~" i-  ~,,~..~: ': ~'~,;~,~a,~,,e~Â¢~r ~..:~-~, ,.~.~v,::~. ,..  : ~a; ...~ ~?.~.,~ ,.. ~ ,.., .v ~,.~..jt"~:.~:z-~ .:.~ - {~# ,=~',~7~g~a: ,~,." --' ~""  f} ~ . ve  , ~::':~}';,~7,c :~:": b'~'.'. ~':<" '."'":' ..'~'.?; <':'.~''-:  ~...2 :.. ..',#: ?2.3..: ~:&:: 2....~ z ~.:; .~ ?;! .... ', .,~,-......~ ,..,..,:.::.., ,.~.:.....,.... ,.  ~ ?~;:...v~.,. ,,..: ~.7;~.:.:~:.:!.i::.:4.:5,;,;;. 7, ~" ,.; ='~'.,,,..: .','.;,','.~,.'... = ,.,...,,,,:: .,.  i~(:- :" ."." ;~.<':..:: ):..'.',:..:', =.. ," , ,,':~ '  i~?'-_':~?:i:<';? .'.".~'~., ..... ":"' " â¢ : ~ .: .. :,;::. ,,.:::,-. ?- ;~ ~.', ':~':: .~ :..., :, :::. ;-:~[ :( ~<. ",.,  , ~'::','.,'.'" "}1,;,q;'",": ,<c,g;;'~.~,~:  ~;j..-:.. ".:2 "q. :, . .  :~,,.:'.. ; :..?~. ,,., ;  ,,=~,..,.  ...... , k n ........ i . .. ne _  '-, -.,,"  .............. ~ .......... ~_;-'..,;Â¢Â¢.;~....~, ,.,.  ":i'i" ~ ~.i':;i:i;~ ~ f~;-~i - :~ ~:2"~ ~ ~" ":~.~, ~ .? ~"  ,l~fj~5~. . ~ .~" y~;~.~f t~] ~k~ , ~,u . t'f" ,"  :~,~ :;;~;:.-" ~ ~:  .-~ ~  --~  .......... .~,,.~  â¢ ..,,~:~d ..... ~. ~. .  , ."  - _.  2"'' .....  :  ~-  ~ â¢ . ne _  2  '. . ,~,:~.~e ~,4:~2z.~:~:, "" ~':~> "  ..~--~" " ~i~&~-?~::g~'.~2~i',~'.~,,:~-:',g:.: ',:" :.:.'.~v~>.~:,,:::,, :, ~'~.::",~,, ~'.,~ ~,~'~'.'~,~.~,,~:~ ,,  ~::~z~.~:-~~:~,'~'~ ~z~'f~e~,:~:x~,,~2z~,'5~z,",, + ~  :.#~hl~?~5~@~:;~4~'~,.'~::~t~'~'": :..,  , ~: ~.~:~,,~:~4~.%~.~:~&~,'~; :..r~2::' t.2  ;:~,'~',~;~ii~'~;~;~y~.~:i.~:z  ~.',~x~,:~,,~c~;~'~'~,:,~~Â£~ ,  r~ ~'~z~.. ;, ~-~ ~~ . .,.~.~ ,:  , .~. ~..~,~,Â¢~-..~...~.,f~.~-...#~*~v~%..: , ~ ~t' ~:' ~," ~, ,~',.',~,' ~z ~ v,. ,c..,~ ," ,. ,: :, : ,,~ ',, ,..~ â¢ â¢ "~.~,.~.,.,~:~~.~:z.~.,~.~y~.'~a,:~:~:.~ ~ : "  , ,~,~.~,:,...:..,.,~ ~.-~..,..,,,~,~.~,~ ~,,. .  ~,~74~-~:~: ~j~2~ ~:Â£~';~@~ :~7.~.:7~:~:,:..: ).:.2 ~-" , %e~_~=~:.r~g,-~:.{}~.~:~.~.~;~a:~aq2. â¢ â¢  " ,~2:~ >~?:.: :5~:e~>v:': ~ ~:~: " ~ " ~ .. ~a. e .~'.,v..'ewav,..a,,vay~ ~..> 1...:1.  , :,,~v~:~:~.v<...,.e~+:~,,~.e.-,r..:~.Â¢dg;a;~..~. ,,  _c~.,i~':~2;~.~z:~5~:&kl:~"e:~.<4~2:~:~7 e"?~fe:. ." ,~2:~ >~?:.: :5~:e~>v:': ~ ~:~: " ~ " ~ .. ~a. e .~'.,v..'ewav,..a,,vay~ ~..> 1...:1.  , :,,~v~:~:~.v<...,.e~+:~,,~.e.-,r..:~.Â¢dg;a;~..~. ,,  _c~.,i~':~2;~.~z:~5~:&kl:~"e:~.<4~2:~:~7 e"?~fe:. .  â¢ .~Â¢ :.,.:ma~., ,',~<~" .,r<,.:.*~,<,:~.:.,=..:,,  z~,..~i~.~7~.~.~,.~,,~..~.;.~...~ ~ : "  n . 1 ~ne ~,+, .,c,.,.,. ,,Â¢,,~.,,: ,.:,~,,., ,~,,.:,,,.~ .... ,,.. , ,, ,  "~"= "5- . " "'~,';" 'l'.. ~:.'~< "..= ....:, .: '~ : " ." ,=;..~ =,:::q..~.~...:..~<=>.....=,.:. :...~-,,,,: : â¢ .  :~-: : 4 khz  300 hz  0ms  r-  -Â£  .<  l"  g  12.5ms  4 khz  Â©  300 hz  ~  ~'~"~'~'i~'~'2 i ~  i~s ..............................  [__i  l o0 ms  figure 4.7 data from the multi-converter system, in response to the word "five" followed  by the word "nine".114 neuromorphic systems engineering  4.4 features for speech recognition  the data shown in figures 4.6 and 4.7 share many properties with neural re-  sponses. each output unit codes information asynchronously, and the effective  sampling period is adaptive and data dependent. each representation in the  system is specialized for a certain property of sound. these representations  are not uncorrelated: there is considerable redundancy between the represen-  rations, and among output units of a single representation. these properties  of neural auditory representations are summarized in figure 4.8.  auditory models speech recognition  adaptive sampling  specialized features  multiple representations  high-dimensional  correlated features uniform sampling  general-purpose features  single representation  low-dimensional  uncorrelated features  figure 4.8 comparison of auditory representations and current speech recognition tech-  nology.  also shown in figure 4.8 are the contrasting properties of conventional fea-  ture representations used in speech recognition systems. these representations  generate features at a single uniform frame rate, typically 10-20ms, unsynchro-  nized to acoustic features. a single, general-purpose spectral representation  is typically used: often, both signal energy information and pitch contour in-  formation is removed from this representation. finally, low-dimensional repre-  sentations are used (5 to 15 elements, typically), and the components of the  representation are often uncorrelated. these front-end properties reflect the  statistical and architectural properties of recognition systems.  figure 4.8 depicts a "representation-recognizer gap" that complicates the  use of auditory models for speech recognition. we address this issue in two  ways: by transforming the representations shown in figures 4.6 and 4.7 to have  properties closer to conventional front-end representations, and by choosing  speech recognition technology that is more compatible with auditory models.  the method we used to extract a feature vector from our multi-representation  system output is described below.  the first step in feature extraction is to convert the asynchronous, event-  list representation into a sequence of uniformly sampled frames. each frame  output consists of 3 vectors (one for each representation) with 119 floating  point elements (one for each output unit), and codes the spike activity that  occurs during a 25ms interval. subsequent frames overlap in time by 12.5ms.  to generate each frame element, the spiking pattern during the 25ms intervalsilicon auditory models 115  is considered as a train of delta functions with unit height: this function is  multiplied by a hamming window. after multiplication, the heights of the  delta functions are summed to yield the final floating-point feature element  value. these operations are graphically shown in figure 4.9.  [ (a)  i i  25ms  i  25ms  figure 4.9 graphic description of algorithm for converting the asynchronous event-list  representation into uniformly sampled frames. a 25ms series of unit-height events (a) is  scaled by a hamming window (b). the heights of the scaled events are added to form the  frame value.  to reduce the size of the spectral-shape and onset representations, we sub-  sample the original ll9-element vectors using symmetrical triangular filters  with a 50-percent filter response overlap. this subsampling produces a 5-  element vector coding onsets, and an 8-element vector coding spectral shape.  the subsampling procedure is graphically shown in figure 4.10.  [- (8 elements) -] reduced vector  reduction filters  [- (119 elements) "~ feature vector  figure 4.10 graphic description of algorithm for subsampling full 119-element represen-  tations into a reduced feature vector.  to reduce the size of the summary autocorrelogram, we compute the discrete  cosine transform of the ll9-element vector, and use the first two components  of the transform as the summary autocorrelogram feature vector. we choose116 neuromorphic systems engineering  this reduction technique to enhance the coding of voicing in the representation,  while de-emphasizing formant information.  these reduction techniques resolve the feature-size table entries in figure 4.8;  they do not, however, address the correlation between feature elements. as  detailed in the next section, we use recognition technologies that are relatively  insensitive to correlations between feature elements to address this issue.  to complete our feature vector computations, we compute temporal differ-  ence feature vectors ("delta" features) for each primary representations, using  a 5-frame window to compute differences. the resulting feature vector has 30  elements: 8 spectral-shape elements, 5 onset elements, 2 summary autocor-  relogram elements, for a total of 15 primary elements, together with 15 delta  features.  4.5 speech recognition architectures  in modern speech recognition architectures, word recognition from a sequence  of feature vectors is a two-step process. in the first step, a pattern classifier  maps the sequence of feature vectors into a sequence of predictions of the spo-  ken phoneme in progress. in the second step, a vocabulary of permitted words  is introduced (a lexicon), expressed as probabilistic state machines (hidden  markov models, abbreviated as hmms). the sequence of phoneme probabili-  ties is then mapped into a sequence of words from the lexicon, using dynamic  programming.  while state models for a lexicons are typically crafted by hand, the features-  to-phonemes pattern classifier is trained automatically, using a large database  of example words. one popular classifier for this task uses a linear mixture of  multivariate gaussian functions to map the feature vector into the probability a  particular phoneme is in progress. a complete mixture model has several types  of parameters: each multi-variate gaussian function has a mean vector and a  diagonal covariance matrix, and weighting parameters control the contribution  of each function in the mixture.  the choice of a diagonal covariance matrix reduces the number of covariance  parameters for an n-element feature vector from n 2 to n. this choice enables  the liberal application of multiple gaussians to model probability space, using  an acceptably small number of parameters. low-parameter models require less  training data for effective parameter estimation, and often improve generaliza-  tion properties.  choosing a diagonal covariance matrix is warranted if the off-diagonal matrix  elements are small; this matrix property will be true if the elements of the  feature vector are uncorrelated. however, as noted in the last section, the  elements of the feature vector we generate from our multi-representation system  are indeed correlated.  an alternative approach for mapping a feature vector into a phoneme proba-  bility vector is to use a multi-layer perceptron (mlp) architecture, trained with  the backpropagation algorithm. this approach, as described in [3], is more tol-silicon auditory models 117  erant of feature vectors whose elements are correlated. the speech recognition  results we present in this paper all use this mlp-based recognizer.  the input to the neural-network classifier is the feature vector of the current  frame, as well as feature vectors from the 4 previous frames and the 4 upcoming  frames for context. the net has a single hidden layer; unless otherwise indicated  we use 200 hidden units in the system. there are 56 network outputs, associated  with the 56 most-common phonemes; these outputs are the inputs to a dynamic  programming module that performs word recognition.  in our speech recognition experiments, we used a database of 200 adults  speaking 13 english words in isolation (the digits, including both "oh" and  "zero", plus "yes" and "no"), for a total of 2600 utterances. the database,  supplied by bellcore, was recorded over the u.s. public telephone network: the  recordings typically have good signal-to-noise ratios, but display the limited  bandwidth typical of the telephone network. most of our experiments used  this database directly; in some experiments, we added recorded noise from the  interior of a running automobile to the speech, at a level resulting in a 10db  signal-to-noise ratio.  the small size of the database results in a significant variance of recognition  performance, depending on the particular words chosen to be in the training  set and the test set. to counter this problem, we divide the database into 4  segments, each consisting of 650 utterances by 50 speakers. we then train up  4 different recognizers using a different selection of three segments for training  data, while testing on the fourth segment. note that a recognizer is never  tested using utterances used for its training. in this paper, we report the error  scores for each of these four recognizers, along with the averaged score of the  four recognizers.  in our recognition experiments, each word is modeled using a multi-state  hmm; each state has a self-loop branch and a branch to the next state, with  fixed transition probabilities of 0.5 for each branch. the model length varies  with the number of phonemes in the word: "eight" is the shortest model, with  13 states, while "seven" is the longest model, with 18 states.  this isolated word database has been used for several previous speech recog-  nition studies at icsi, using the mlp-based recognizer in conjunction with  two popular feature extraction systems, plp and j-rasta-plp. these recog-  nition studies, summarized in figure 4.11, serve as a benchmark for com-  parison with recognition experiments using feature vectors derived from our  multi-representation auditory model. perceptual linear prediction (plp) is a  popular feature extraction system based on human perceptual data from psy-  chophysics, that works well for speech recorded with high signal-to-noise ratios  through benign transmission channels [i0]. the j-rasta-plp system [ii, 28]  is an enhanced version of plp, designed to provide feature vectors relatively  independent of noise mixed with the speech signal, as well as providing feature  vectors independent of slowly-varying changes in the spectral properties of the  speech transmission channel.118 neuromorphic systems engineering  front end conditions  j-rasta-plp clean  j-rasta-plp noisy  plp noisy 1 2 3 4 average  2.3 1.5 1.4 2.0 1.9  11.4 10.2 10.3 11.5 10.9  42.8 37.1 40.8 49.1 42.4  figure 4.11 percent error for plp-based front-ends (four database partitions).  the first table entry in figure 4.11 shows the performance of j-rasta-  plp on the isolated word database, for the 4 data segmentations described  above: both training and test data are the original "clean" database, without  added car noise. the 1.8 percent average error score is comparable with current  commercial systems used in voice-mail applications, using real-world telephony  data: trade publications for interactive voice response telephony advise users  to expect 3-5% scores for isolated digit recognizers in the field. systems with  error rates under 5% can work well in an application, if good error recovery  strategies are available for the task.  the next two table entries in figure 4.11 describe performance for recogniz-  ers that were trained "clean" utterances, but whose test utterances were mixed  with automobile noise (10 db signal-to-noise ratio), as described earlier. note  that in addition to being corrupted with noise, the test utterances were also  novel: a recognizer is never tested using noisy utterances whose clean versions  were a part of the training set. these table entries show the benefits of en-  hancing a feature extraction system to be robust to additive noise: the average  error for plp is 4 times greater than for j-rasta-plp. however, the absolute  error of j-rasta-plp tested with noisy speech (10.9%) is marginal for use in  an application.  4.6 speech recognition experiments  we used the isolated-word database and mlp-based recognition system de-  scribed in the previous section to evaluate the performance of the 30-element  feature vector derived from our multi-representation system. figure 4.12 sum-  marizes the error performance of the system; all the scores in this table reflect  "clean" testing and training data. the final line of figure 4.12 shows recogni-  tion performance using the full 30-element feature vector. this 4.1% error rate  is sufficiently low for many applications, although it is significantly larger than  the j-rasta-plp's benchmark error rate (1.8%).  to illustrate the relative contributions of the three representations in our  system, we also trained recognizers using subsets of our 30-element feature  vector, that contained only the elements from one or two of the representations  in the system: the penultimate lines of figure 4.12 show these scores. the  number of hidden units for each recognizer was varied inversely with the numbersilicon auditory models 119  features parameters hidden unit,, 1 2 3 4 average  ss 65,586 326 6.6 6.9 5.4 8.0 6.7  ss+auto 65,468 276 5.7 5.8 4.5 5.5 5.4  ss+onset 65,531 225 4.9 5.1 4.3 4.9 4.8  is+auto+onset 65,456 200 4.9 4.2 3.2 4.0 4.1  figure 4.12 percent error for feature vectors derived from auditory representations (four  database partitions). other fields show number of hidden units and number of parameters  in the mlp classifier net. code: ss = spectral shape features, onset = onset features,  auto = autocorrelogram features.  of elements in the reduced feature vector, to yield an mlp with approximately  65,000 parameters for each experiment.  a comparison of the different recognizers in figure 4.12 shows the effective-  ness of combining multiple representations of speech. adding features from two  additional representations (the onset features and the autocorrelation features)  to the primary spectral-shape features decreases the average error by 61%.  figure 4.13 shows an error analysis of the recognizers of figure 4.12; for  each recognizer, the percentage error attributed to the two most likely word  confusions ("five" and "nine", and "no" and "oh") are shown, along with the  residual error contributed by all other confusions. the addition of onset features  and autocorrelogram features improves the recognition performance for all three  categories of confusions.  features total 9/5 oh/no others  ss  ss + auto  ss + onset  ss + auto + onset 6.7  5.4  4.8  4.1 1.4  1.1  1.0  0.7 1.0  0.8  0.7  0.6 4.4  3.5  3.1  2.8  figure 4.13 error analysis of the recognition experiments in figure 4.12 (averaged over  partitions). errors due to the two leading word confusions are listed (confusing "five" and  "nine", and confusing "oh" and "no"), as well as the residual error.  in figure 4.13, note that for five/nine confusions, the error improvements  for adding onset features (1.4% to 1.1%, a 0.3% improvement) and for adding  autocorrelation features (1.4% to 1.0%, a 0.4% improvement) add to equal the  error improvement for adding both onset and autocorrelation features to spec-  tral shape features (1.4% to 0.7%, a 0.7% improvement). this linear addition  suggests the statistical independence of the information added by the onset and  autocorrelation features for disambiguating "five" and "nine". conversely, the  table shows the statistical dependence of the information added by the onset120 neuromorphic systems engineering  and autocorrelation features for disambiguating words in the "others" category.  if these features were statistically independent, an error rate of 2.2% (not 2.80/0)  would be expected for the "others" category for the full feature vector.  figure 4.14 shows the error performance of the recognizers trained for fig-  ure 4.12, when tested on the "noisy" utterances described in the last section.  this table shows uniformly poor recognition results, comparable with the noisy  recognition performance of plp shown in figure 4.10, and 5.5 times worse than  the noisy recognition performance of j-rasta-plp. although early studies of  speech recognition in noisy conditions using auditory models reported encour-  aging results [9, 31], later studies found no significant noise robustness qualities  for auditory models [24], and the data in figure 4.14 confirms this finding.  features parameters hidden units  ss 65,586 326  ss + auto 65,468 276  ss + onset 65,531 225  ~s + auto + onset 65,456 200 1 2 3 4 average  50 54 50 55 52  53 55 51 55 54  55 57 61 62 59  57 57 59 62 59  figure 4.14 recognition results for noisy test data; automobile noise source, mixed with  speech at signal-to-noise ratio of lodb (nist measurement method).  figure 4.15 shows the effect of reducing the number of parameters in the  mlp pattern classifier on error rate, for the full 30-element feature vector. the  table compares recognizers with approximately 32,000 parameters (100 hidden  units) and 16,000 parameters (50 hidden units) with the full 65,000 parameter  recognition system. the effect of parameterization on error performance is  particularly important for low-cost, low-power recognizer implementations.  features parameters hidden units  ~s + auto + onset 65,456 200  gs + auto + onset 32,756 100  ;s + auto + onset 16,406 50 1 2 3 4 average  4.9 4.2 3.2 4.0 4.1  5.1 4.6 3.8 4.3 4.5  5.7 6.8 4.6 6.6 5.9  figure 4.15 recognition results showing the effect of the number of parameters in the  mlp-classifier on recognition results. results are for clean testing data, using the full feature  vector (ss+onset+auto).  the 200 speaker, 13-word isolated-word database consists of approximately  5 hours of speech. the analog processing circuits in the multi-representation  system are not compensated for temperature drift; we omitted temperature  compensation circuitry from our prototype system for simplicity. ambient  temperature variation in our laboratory over five hours results in a significant  drift in auditory model responses.silicon auditory models 121  to counter temperature drift problems during datataking for the experi-  ments reported above, several steps were taken. data was presented to the  chip ordered by word: 200 speakers saying "1," followed by 200 speakers saying  "2," ect. all chip parameters were recalibrated between each set of 200 ut-  terances; this recalibration resets parameters with 5% accuracy. the complete  dataset was taken several times, on different days of the week and different  times of the day, and pilot recognition experiments guided the choice of the  final dataset. within the limits of our present hardware prototype, these er-  ror scores approximate the performance of a temperature-compensated multi-  representation system.  for comparison purposes, figure 4.16 shows recognition performance for the  multi-representation system, if less care is taken to reduce temperature effects.  in these experiments, data was presented to the system ordered by speaker,  not by word: speaker 1 saying all 13 words, followed by speaker 2 saying all  13 words, ect. recalibration of parameters occured every 10 speakers. the  scores in this table reflect "clean" testing and training data; the final line of  figure 4.12 is reproduced in figure 4.16 to provide a direct comparison between  the two datasets.  order parameters hidden units  by word 65,456 200  by speaker 65,456 200 1 2 3 4 average  4.9 4.2 3.2 4.0 4.1  6.6 4.9 6.0 7.7 6.3  figure 4.16 recognition resuits showing the effect of data presentation on recognition  results. results are for clean testing data, using the full feature vector.  4.7 discussion  the speech recognition performance of our multi-representation system, as  shown in figure 4.12 and figure 4.14, is inferior to jorasta-plp, both for  clean and noisy test data. however, under high signal-to-noise conditions, the  system provides adequate performance (4.1% error) for many isolated-word ap-  plications. for specialized applications where a micropower speech feature ex-  tractor is required, the signal processing technology used in our special-purpose  analog-to-digital converter chip is a competitive option. for these applications,  the remaining challenges include the micropower implementation of the rest of  the recognition system [23], and the identification of end-user applications with  sufficient market size to support the development effort.  apart from the micropower niche, however, analog auditory models are cur-  rently uncompetitive with conventional front-end approaches for speech recog-  nition applications. the success of auditory processing in biological systems,  however, leaves us hopeful that a sustained research effort in using analog au-  ditory models for speech recognition could result in recognition systems that122 neuromorphic systems engineering  perform significantly better than conventional front-end approaches. we see  the following areas as important elements of such a research effort:  4.7.1 improved circuit techniques  the 4.1% error of the multi-representation system, for clean speech, is distinctly  inferior to the 1.8% error for 3-rasta-plp on the same task. in contrast,  studies of software implementations of similar auditory models [13] typically  show comparable performance in comparison with conventional front-ends. the  shortcomings of our analog circuit implementation, including limited signal-to-  noise ratio, limited dynamic range, and inaccuracy due to parameter variation  and temperature-related drift, may play a role in this difference.  the circuit technologies that implement the signal processing datapath  shown in figure 4.2 date from the first silicon audition designs [27]. several  generations of improved circuits and algorithms for silicon audition have been  published since these early designs, and research continues in several groups  worldwide. many of these improvements focus on signal-to-noise, dynamic  range, and improving uniformity across cochlea channels. these improvements  may directly translate to improvements in speech recognition scores, bringing  silicon auditory models to the performance of their software counterparts.  parameter drift due to inadequate temperature compensation is another  area for improvement, the temperature compensation approach we use in our  multi-representation is primitive [15], and parameter drift may be a significant  source of recognition error, as figure 4.16 suggests. improvements in this area  are straightforward, using techniques such as those described in [33].  4.7.2 enhanced auditory models  the cochlear model in our special-purpose analog-to-digital converter chip is  an extreme simplification of physiological cochlear processing; software-based  auditory models used in other speech recognition studies share most of these  simplifications. key physiological cochlear response characteristics, including  synchrony suppression, rate suppression, and temporal masking, are absent  from these models; many auditory theorists believe these characteristics under-  lie the robust coding of speech in the presence of noise in biological auditory  systems. physiological cochleas are deeply non-linear, and exhibit characteris-  tics consistent with extensive channel-specific automatic gain control: the au-  ditory models used in speech recognition experiments to date do not correctly  model these characteristics. we believe that more accurate cochlear models  are an important part of future research in using auditory representations for  speech recognition.  in addition to improving the cochlear models, cleaner implementations of  the computations underlying the secondary representations in our system (cor-  relation and temporal adaptation) would add considerable robustness to these  representations. also of interest is the addition of other secondary represen-  tations, in particular models of neural maps that code for temporal offsets,silicon auditory models 123  amplitude modulation, frequency modulation, and quick temporal sequences  typical of the voiced-onset transition in speech. if multi-microphone recordings  of speech databases are available, binaural representations are another possible  enhancement to the system.  4.7.3 adapting robust techniques to auditory models  a variety of techniques for robust feature extraction in noisy environments  have been developed for use with conventional front-ends for speech recogni-  tion. adapting these techniques to function with auditory representations is a  promising avenue of research.  one popular method of speech enhancement in noise is spectral subtrac-  tion [2]. in this approach, a spectral model of the background noise in the  recent past is generated, and subtracted from the current input. another  method of speech enhancement in noise, the 3-rasta-plp system [11, 28],  uses information about the temporal properties of speech to filter speech signal  from background noise. both approaches could be used in conjunction with  auditory representations.  4.7.4 closing the representation-recognizer cap  as figure 4.8 summarizes, auditory representations are a poor match to cur-  rent speech recognition systems. this paper makes no significant contribution  towards closing this "representation-recognizer gap". our straightforward ap-  proach of collapsing the spike-based, high-dimensionality auditory representa-  tions (hamming windows and gross sub-sampling) destroys most of the unique  coding aspects of the auditory representation. apart from choosing an mlp-  based pattern classifier, no advances in recognition algorithms were made to  help close the gap from the recognition side.  we believe that making significant contributions to closing this gap, both  by modifying core speech recognizer technology, and by developing enhanced  methods of distilling information from high-dimensional, adaptively-sampled  representations, is essential to significantly improve speech recognition perfor-  mance of auditory models.  several research groups have done initial work on changing core speech recog-  nition technology to be more amenable to auditory representations. these  methods take different approaches to the problem; one recent publication uses  the visual scene analysis concept of occlusion as a starting point [8], while  other recent work is motivated by the importance transient information in the  speech signal [29]. attacking the problem from the representation side, research  in mapping in high-dimensional spaces into low-dimensional features has been  recently applied to cochlear models [12].124 neuromorphic systems engineering  4.8 summary  in this paper, we have evaluated the suitability of analog implementations  of auditory models, using an empirical approach: we integrated a multi-  representation analog auditory model with a speech recognition system, and  measured the performance of the system on a speaker-independent, telephone-  quality 13-word recognition task. the performance of the system is adequate  for many applications, but inferior to conventional approaches for front-end  processing. in addition, the auditory models show no advantages for robust  speech recognition applications.  acknowledgments  we thank nelson morgan for providing the speech databases, the speech-recognition  software, and the computational resources for this study; we also acknowledge his  extensive advice on mlp design and training. we thank su-lin wu for her extensive  help in the recognition studies; we also thank the entire icsi speech recognition group  for suggestions and software maintenance. thanks to richard duda, steve greenberg,  hynek hermansky, richard lippmann, richard lyon, carver mead, malcolm slaney,  and the members of the physics of computation laboratory at the california institute  of technology for advice and suggestions on auditory modeling. we used the htk  toolkit version 1.4 for preliminary speech recognition studies preceding this work.  research funded by the office of naval research (uri-n00014-92-j-1672).  references  [1] n. a. bhadkamkar. binaural source localizer chip using subthreshold ana-  log cmos. in ieee international conference on neural networks, vol-  ume 3, pages 1866-1870, 1994.  [2] s. boll. suppression of acoustic noise in speech using spectral subtraction.  ieee trans. on assp, assp-27(2):113-120, 1979.  [3] h. bourlard and n. morgan. connectionist speech recognition: a hybrid  approach. kluwer academic publishers, boston, mass, 1994.  [4] g. j. brown and m. p. cooke. computational auditory scene analysis.  computer speech and language, 8(4):297-336, 1994.  [5] a. p. chandrakasan and r. w. brodersen. low power digital cmos de-  sign. kluwer academic publishers, boston, mass, 1995.  [6] r. coggins, m. jabri, b. flower, and s. pickard. a hybrid analog and dig-  ital vlsi neural network for intracardiac morphology classification. ieee  journal solid state circuits, 30(5):542-550, 1995.  [7] c. colomes, m. lever, j. b. rault, and y. f. dehery. a perceptual model  applied to audio bit-rate reduction. j. audio eng. soc., 43(4):233-239,  1995.silicon auditory models 125  [8] m. cooke, p. green, and m. crawford. handling missing data in speech  recognition. in 1994 international conference on spoken language pro-  cessing, volume 3, pages 1555-1558, 1994.  [9] o. ghitza. temporal non-place information in the auditory nerve firing  patterns as a front-end for speech recognition in a noisy environment.  journal of phonetics, 16(1):109-123, 1988.  [10] h. hermansky. perceptual linear predictive (plp) analysis of speech. jour-  nal acoustical society of america, 87(4):1738-1752, 1990.  [11] h. hermansky and n. morgan. rasta processing of speech. ieee trans-  actions of speech and audio processing, 2(4):578-589, 1994.  [12] n. intrator. combining exploratory projection pursuit and projection pur-  suit regression with application to neural networks. neural computation,  5:443-455, 1993.  [13] c.r. jackowoski, h. d. h. vo, and r. p. lippmann. a comparison of signal  processing front ends for automatic word recognition. ieee transactions  of speech and audio processing, 3(4):286-293, 1995.  [14] j. lazzaro, j. wawrzynek, , and a. kramer. systems technologies for  silicon auditory models. ieee micro, 14(3):7-15, june 1994.  [15] j. lazzaro, j. wawrzynek, m. mahowald, m. sivilotti, and d. gillespie.  silicon auditory processors as computer peripherals. ieee journal of  neural networks, 4(3):523-528, 1993.  [16] j. p. lazzaro. a silicon model of an auditory neural representation of  spectral shape. ieee journal solid state circuits, 26:772-777, 1991.  [17] j. p. lazzaro and c. mead. circuit models of sensory transduction in  the cochlea. in mead and ismail, editors, analog vlsi implementation  of neural systems, pages 85-101. kluwer academic publishers, norwell,  ma, 1989.  [18] j. p. lazzaro and c. mead. silicon models of auditory localization. neural  computation, 1:47-57, 1989.  [19] j. p. lazzaro and c. mead. silicon models of pitch perception. in proc.  natl. acad. sci. usa, volume 86, pages 9597-9601, 1989.  [20] j. p. lazzaro and j. wawrzynek. a multi-sender asynchronous extension  to the address-event protocol. in w. j. dally, j. w. poulton, and a. t.  ishii, editors, 16th conference on advanced research in vlsi, pages 158-  169, 1995.  [21] john lazzaro. temporal adaptation in a silicon auditory nerve. in john e.  moody, steve j. hanson, and richard p. lippmann, editors, advances in  neural information processing systems, volume 4, pages 813-820. morgan  kaufmann publishers, inc., 1992.  [22] john lazzaro and john wawrzynek. silicon models for auditory scene anal-  ysis. in david s. touretzky, michael c. mozer, and michael e. hasselmo,126 neuromorphic systems engineering  editors, advances in neural information processing systems, volume 8,  pages 699 705. the mit press, 1996.  [23] john lazzaro and john wawrzynek. silicon models for auditory scene  analysis. in m. jordan, m. mozer, and t. petsche, editors, advances in  neural information processing systems, volume 9. the mit press, 1997.  [24] john lazzaro, john wawrzynek, and r. lippmann. a micropower analog  vlsi hmm state decoder for wordspotting. in m. jordan, m. mozer, and  t. petsche, editors, advances in neural information processing systems,  volume 9. the mit press, 1997.  [25] w. liu, a. andreou, and m. goldstein. voiced-speech representation by  an analog silicon model of the auditory periphery. ieee transactions of  neural networks, 3(3):477-487, 1992.  [26] r. f. lyon. ccd correlators for auditory models. in ieee asilomar  conference on signals, systems, and computers, pages 785-789, 1991.  [27] r. f. lyon and c. mead. an analog electronic cochlea. ieee trans.  acoust., speech, signal processing, 36:1119-1134, july 1988.  [28] k. w. ma. applying large vocabulary hybrid hmm-mlp methods to  telephone recognition of digits and natural numbers. technical report  tr-95-024, international computer science institute, 1995.  [29] n. morgan, h. bourlard, s. greenberg, and h. hermansky. stochastic  perceptual auditory event-based models for speech recognition. in 1994  international conference on spoken language processing, volume 4, pages  1943-1946, yokohama, japan, 1994.  [30] m. b. sachs and e. d. young. effects of nonlinearities on speech encoding  in the auditory nerve. j. acoust. soc. am, 68(3):858-875, 1980.  [31] s. seneff. a joint synchrony/mean-rate model of auditory speech process-  ing. journal of phonetics, 16(1):55-76, 1988.  [32] a. van schaik, e. fragni~re, and e. a. vittoz. improved silicon cochlea  using compatible lateral bipolar transistors. in david s. touretzky,  michael c. mozer, and michael e. hasselmo, editors, advances in neu-  ral information processing systems, volume 8, pages 671 677. the mit  press, 1996.  [33] e. a. vittoz. the design of high-performance analog circuits on cmos  chips. eee journal solid state circuits, 20(3):657-665, 1985.  [34] l. watts, d. kerns, r. f. lyon, and c. mead. improved implementation  of the silicon cochlea. ieee journal solid-state circuits, 27(5):692-700,  may 1992.i i retinomorphic systemsthe retinomorphic approach"  pixel-parallel  adaptive amplification, filtering,  and quantization  kwabena a. boahen  physics of computation laboratory,  ms 136-93, california institute of technolos;y,  pasadena ca 91125  buster@pcm p.caltech.edu  5.1 smart-pixel arrays  the migration of sophisticated signal processing down to the pixel level is  driven by shrinking feature sizes in cmos technology, allowing higher levels of  integration to be achieved [24, 18]. new pixel-parallel architectures are required  to take advantage of the increasing numbers of transistors available [1]. inspired  by the pioneering work of mahowald and mead [6], i describe in this paper a  retinomorphic vision system that addresses this need by mimicking biological  sensory systems.  in particular, my approach uses the system architecture and neurocircuitry  of the nervous system as a blueprint for building integrated low-level vision  systems--systems that are retinomorphic in a literal sense [1, 9]. morphing of  biological wetware into silicon-based hardware results in sensory systems that  maximize information uptake from the environment, while minimizing redun-  dancy in their output; that achieve high levels of integration, by performing  several functions within the same structure; and that offer robust system-level  performance, by distributing computation across several pixels.  i describe a retinomorphic system that consists of two chips: a focal-plane  image processor that adaptively amplifies, filters, and quantizes the visual signal  at the pixel-level, and a postprocessor that has a two-dimensional array of130 neuromorphic systems engineering  integrators. the system concept is shown in figure 5.1. both chips are fully  functional; specifications and die micrographs are shown in table 5.1 and in  figure 5.2, respectively.  i outline the general design principles of the retina, and contrast them with  standard engineering practice in section 5.2. having defined the retinomorphic  approach to imager design, i describe a design for a retinomorphic pixel in  section 5.3, and present test results from the complete two-chip neuromorphic  system in section 5.4. the communication channel used to transmit the pulse  trains from chip to chip is described in detail a companion paper [4]; a brief  description is already available [186]. my concluding remarks are presented in  section 5.5.  i~  "3 .... ~.  i ~ ~ : i~ .~  :,~ m ii1 ~ :~,~  : ~ i~  <[  retinomorphic chip cortical  processing  interchip communication neuromorphic chip  figure 5.1 system concept. the retinomorphic chip acquires, amplifies, filters, and quan-  tizes the image. all these operations are performed at the pixel level. the interchip commu-  nication channel reads out asynchronous digital pulses from the pixels by transmitting the  location of pulses as they occur. a second neuromorphic chip decodes these address events,  and recreates the pulses.  table 5.1 specifications of the two-chip retinomorphic system. l is the minimum feature  size, which was 2#m for this process; s/s is samples per second.  specification imager postprocessor  technology  number of pixels  pixel size (l 2)  transistors/pixel  die size (mm 2)  supply  dissipation (0.2 ms/s)  throughput 2-#m 2-poly 2-metal p-well  64 x 64  53 x 49 31.5 x 23  32 8  8.1 x 7.4 5.1 x 4.0  5v  230 mw (total)  2 ms/sretinomorphic approach 131  table 5.2 retinal design principles versus electronic-imager design principles.  operation standard retinal  detection integrating continuous  gain control global local  filtering allpass bandpass  quantization fixed adaptive  architecture serial parallel  x arbffer & encoder  (a) (b)  figure 5.2 (a) retinomorphic focal-plane processor. the core of this chip is a 64 x 64  array of pixels arranged on a hexagonal grid. pixels generate pulses and communicate the  occurence of these pulses by signalling on the column and row request lines. the arbiters  ensure that pulses are read out of the array one by one, in an orderly manner, by selecting one  pixel at a time with the column and row select lines. the encoders generate the addresses  of the selected row and column; this pair of binary words uniquely identifies the location of  the pulse. (b) postprocessor. the core of this chip is a 64 Ã 64 array of diode-capacitor  integrators. we can feed short current pulses to any integrator in the array by supplying its  row and column addresses to the decoders. we use the scanners (shift registers) to read  out analog currents from the array for display on a video monitor.  5.2 pixel-parallel processing  the functional and structural organization of the retina is radically different  from that of standard human-engineered imagers. the principles of operation  of the retina are outlined in table 5.21 the principles of operation of standard  imager technology also are listed for comparison.132 neuromorphic systems engineering  5.2.1 sensing: continuous versus integrating  integrating detectors (e.g., charge-coupled devices (ccds) [23] and photo-  gates [16]) suffer from blooming at high intensity levels and require a destructive  readout (reset) operation. continuous-sensing detectors (e.g., photodiodes or  phototransistors) do not bloom, and can therefore operate over a much larger  dynamic range [30]. in addition, redundant readout or reset operations can be  eliminated, with considerable power savings, because charge does not accumu-  late.  continuous-sensing detectors have been shunned, however, because they suf-  fer from gain and offset mismatches that give rise to salt-and-pepper noise in  the image. however, buhman et.al, have shown that the powerful learning  capabilities of image-recognition systems can easily compensate for this fixed  pattern noise [ll].  the real benefit of using continuous sensors lies in their ability to perform  analog preprocessing before quantizing the signal. a signal that takes on a dis-  crete set of values at a discrete set of times (quantized in amplitude and time)  carries less information than does a signal that takes on the full continuous  spectrum of amplitudes and times. for instance, graded potentials in the ner-  vous system can transmit information at the rate of 1650 bits per second--over  four times the highest rate measured for spike trains [14].  the analog operations described in sections 5.2.2 and 5.2.3 reshape the  spectral distribution and the amplitude distribution of the analog signal, to  transmit information efficiently through this bottleneck.  5.2.2 amplification: local versus global control  imagers that use global automatic gain control (agc) can operate under only  uniform lighting, because the 1000-fold variation of intensity in a scene with  shadows exceeds their 8-bit dynamic range. 1 a charge-coupled device or pho-  togate can achieve 12 bits (almost four decades) [16], and a photodiode or  phototransistor can achieve 20 bits (six decades) [15, 30j--but the phototran-  sistor's performance in the lowest two decades is plagued by slow temporal  response. the dynamic range of the system's output, however, is limited by  the cost of precision analog read-out electronics and a/d converters, and by  video standards.  when agc acts globally, the input dynamic range matches the output dy-  namic range, and the only way to extend the input range is to extend the  output dynamic range. in practice, we must reduce the noise floor to improve  resolution.  as shown in figure 5.3, local agc decouples dynamic range and resolution,  extending the input dynamic range by mapping different parts of the input  range to the limited output range, depending on the local intensity level. this  solution is beneficial if the resolution required to discriminate various shades  of gray (1 in 100 for the human visual system) is poorer than the resolutionretinomorphic approach 133  = 3  m  .~  ~  ,,~  -5 -4 -3 -2 -)  intensity (log cd/m^2) Â°f 3= .c rou= .  log cd/m = -5 -4/ -3 / -2 ~ -1 0 =  1100  /  -5 -4 -3 -2 -1 0  test flash (log cd/m z)  (a) (b)  figure 5.3 input-output transfer curves for light sensors. (a). as larger and larger input  ranges are spanned, the slope decreases, and finer resolution is required to detect the same  percentage change in the input signal. (b). using transfer curves that can be centered at the  local intensity level decouples dynamic range and resolution. each curve spans only a 20-fold  input range, since local variations in intensity are due primarily to changes in reflectivity: a  black sheet has a reflectivity of 0.05, and a white sheet has a reflectivity of 0.95. these  transfer curves were measured for the cat retina, and were reproduced from [35].  required to span the range of all possible input levels (at least 1 in 100,000 for  the photopic range of human vision).  5.2.3 preprocessing: bandpass versus allpass  on average, natural images have a 1/f 2 power spectrum for both spatial and  temporal frequency [17, 21], whereas noise, due to quantum fluctuations, has a  flat spectrum. consequently, imagers that transmit the full range of frequencies  present pass on useless information at high frequencies, where the signal-to-  noise is poor, and pass on redundant information at low frequencies, where the  signm-to-noise is good. bandpass spatiotemporal filtering rejects the wideband  noise, and attenuates the redundant low-frequency signals; this strategy is the  optimal one for removing redundancy in the presence of white noise [2, 18, 39].  i illustrate in figure 5.4b,d the redundancy reduction achieved, for a typical  outdoor scene, by computing the correlation between pixel values. 2 the corre-  lation is over 40% for pixels 60 pixels apart in the raw image. in the filtered  image, pixels more than 10 pixels apart have less than 5% correlation. com-  parison of the amplitude histograms before and after filtering (figure 5.4c,f)  demonstrates that bandpass filtering has two additional benefits.  first, bandpass filtering results in a sparse output representation. for our  sample image, 24.4% of the pixels fall within Â±0.39% of the full-scale range  (i.e., Â±ilsb at 8-bit resolution); 77.5% of them fall within Â±5% (i.e., Â±13  at -127 to +127 amplitude range). hence, if we choose to ignore amplitudes  smaller than 5%, we need to transmit only 22.5% of the pixels. in practice,  the degree of sparseness will depend on the cut-off frequency of the bandpass134 neuromorphic systems engineering  (a) (b) (c)  (d) (e) i  (f)  figure 5.4 bandpass filtering. the top row shows the original 512 Ã 479 Ã 8-bit image  (a), its autocorrelation (b), and its amplitude histogram (c). the bottom row shows the  bandpass-filtered image (d), its autocorrelation (e), and its amplitude histogram (f). in  the original image, pixels are highly correlated, and the correlation falls off slowly with  distance. the distribution of amplitudes in the original image is broad and bimodal, due to  the relatively bright overcast sky and the dark foreground objects. in contrast, the amplitude  distribution for the filtered image is clustered around zero (119), and decays rapidly.  filter. although rejection of high frequencies introduces some redundancy, this  rejection is necessary to protect the signal from noise that is introduced by the  signal source or by the circuit elements.  second, bandpass filtering results in a unimodal amplitude distribution that  falls off exponentially. for our sample image, the distribution is fit by a sum of  two exponentials that change by a factor of e -- 2.72 whenever the amplitude  changes by 2.5 and 14.0, on a =e128 scale; the rapidly decaying exponential  starts out 4.5 times larger. empirical observations confirm that this simple  model holds for a wide range of images.  in contrast, the distribution of raw intensity values is difficult to predict,  because gross variations occur from scene to scene, due to variations in illumi-  nation, image-formation geometry (surface and light-source orientation), and  shadows [34]. these slowly changing components of the image are removed by  local agc and bandpass filtering. when the bandpass characteristics are fixed  and the intensity is normalized, the parameters of the amplitude distributionretinomorphic approach 135  vary much less, and the quantizer can exploit this invariance to distribute its  codes more effectively.  5.2.4 quantization: adaptive versus fixed  the quantization intervals of traditional a/d converters are set to match the  maximum rate of change and the smallest amplitude, as shown in figure 5.5.  this uniform quantization is optimum only when high frequencies dominate  and all amplitudes are equally likely. as we have seen, neither case applies  to natural scenes: the power spectrum decays with frequency, as in 1/f 2, and  the amplitude probability density decays exponentially. therefore, uniform  quantizers produce numerous redundant samples, because changes in the signal  are relatively rare [17], and underutilize their large amplitude codes, because  these signal amplitudes occur rarely in natural scenes [34].  assuming that temporal changes are due primarily to motion, we can es-  timate the amount of redundancy from the spatial-frequency power spectrum  and from the velocity distribution. the velocity distribution, measured for  / "  >: at >  ~ ~  t ~ ~ ~ ~ ~ 1 t t ~  av at = ~ rnax(dv/dt) av at = ~ dv/dt  , /  ~- ~v  ~--~  v ---~  ~ ~ "i ............................... i"~ ~ -~-~-.  ~,v = range(v)  2 b ../  v  i ................................... !l~ii~ ............................................ i  ap 1  av = d pld~ a p = -~ 2 ~  figure 5.5 quantization in time and amplitude. top row: time intervals (a~p) are set  to match the maximum rate of change (left column). the signal is sampled repeatedly,  even when dv/dt ~ 0--that is, when the chanl~e is insignificant (oversampling). instead  of fixing the time step, it is more efficient to fix the voltage step (l~vi, and to adapt the  time intervals dynamically to achieve this change in voltage, as shown on the right. bottom  row: amplitude intervals (av) are uniformly distributed. the signal is sampled repeatedly,  even though dp/dv ~ 0--that is, although the probability that the input amplitude falls  in this interval is negligible. instead of fixing the voltage step, it is more efficient to target a  certain change in the cumulative probability (ap __-- 2 -b, where b is the number of bits per  sample), and to choose voltage intervals statistically to achieve this change in probability,  as shown on the right.136 neuromorphic systems engineering  movies and amateur videos, is dominated by low velocities and falls off with  a power law of 3.7 [17]. high velocities will be even more drastically attenu-  ated in an active vision system that compensates for global motion, and that  tracks objects [19]. after bandpass filtering, signals that change gradually over  space are eliminated and rapid changes occur only rarely and over much more  restricted areas.  due to the absence of high speeds and of nonlocal intensity varaitions, the  imager's output signals rarely change rapidly. consequently, adapting the sam-  pling rate to the rate of change of the signal greatly reduces the number of  samples produced. alternatively, this adaptation allows higher temporal band-  widths to be achieved for a given mean sampling rate.  using the amplitude distribution of our bandpass-filter sample image, we can  calculate the probability of failing to discriminate between a pair of samples  drawn from that distribution: it is 0.0384 when the 2 s quantization levels are  uniformly distributed an order of magnitude bigger than the the minimum  confusion rate of 1/256 = 0.0039, which occurs when the quantization levels are  chosen to make it equally likely that we will draw a sample from each interval.  in fact, the confusion rate of 0.0384 can be achieved with just log2(1/0.0384 ) =  4.7bits per sample if the quantization levels are optimally distributed.  a quantizer that assigns its codes to probable amplitudes, rather than to  improbable ones, maximizes the probability of discriminating between any two  amplitude levels drawn from the input distribution; thus, information is maxi-  mized when all codes are equiprobable [36].  5.2.5 architecture: parallel versus serial  in addition to differing in the aforementioned design principles, biological and  human-made vision systems use radically different architectures. the retina  performs the four operations listed in table 5.2 in a pixel-parallel fashion,  whereas most synthetic imagers perform only detection in the pixel; the few that  also amplify and quantize the signal perform these operations pixel-serially, and  set the gain, sampling rate, and step size to be the same for all pixels [16, 22, 25].  in sharp contrast to human-engineered imagers, the retina adapts its gain,  sampling rate, and step size locally, to maximize information uptake; it also  whitens the signal in space and time, to minimize the redundancy in its output  samples. i present a pixel design in section 5.3 that performs continuous sens-  ing, bandpass spatiotemporal filtering, local agc, and adaptive quantization.  5.3 a retinomorphic pixel  i designed the pixel circuit shown in figure 5.6 using the retinomorphic ap-  proach; it senses, amplifies, filters, and quantizes the visual signal. in general  terms, this retinomorphic pixel operates as follows.  the transducer is a vertical bipolar transistor; its emitter current is pro-  portional to the incident light intensity [30]. two current-spreading net-  works [1, 9, 12, 14] diffuse the photocurrent signals over time and space; theretinomorphic approach 137  ww::   opl irese4  ~'pu  ~i~~ logic  pulse generator )vad pt r-~~ iktÂ¢  (ay vw>~l c vressectanou t  ) ry %  ;~x x a )r"~- j~,. ~,,i-< sea n i n  diode-capacitor  integrator  figure 5.6 retinomorphic pixel. the outer-plexiform-layer (opl) circuit performs spa-  tiotemporal bandpass filtering and local automatic gain control (agc) using two current-  spreading networks. nodes v0 and w0 are connected to their six nearest neighbors on a  hexagonal grid by the delta-connected transistors. the opus output current is converted  to pulse frequency by the pulse generator. the logic circuit communicates the occurrence  of a pulse (vspk) to the chip periphery using the row and column request and select lines  (ry/ay and rx/ax), turns on ireset to terminate the pulse, and takes vadapt low, to  feed a current pulse to the integrator; the logic circuit is described elsewhere [4, 186]. the  integrator's output current (ik) is subtracted from the input to the pulse generator; the  device in series with the integrator's output, whose gate is tied to a fixed bias vreset, is  used to isolate the integrator from the rapid volatges swings that occur at vmem when  spikes occur. the two series-connected transistors on the right are used to scan out the  integrator's output for display on a video monitor.  first layer (node v0) excites the second layer (node w0), which reciprocates  by inhibiting the first layer. the result is a spatiotemporally bandpass-filtered  image [6, 13, 33]. the second layer computes a measure of the local light in-  tensity, and feeds back this information to the input layer, where the intensity  information is used to control light sensitivity. the result is local agc [9].  a pulse generator converts current from the excitatory layer into pulse fre-  quency. the diode-capacitor integrator computes a current that is proportional  to the short-term average of the pulse frequency; this current is subtracted from  the pulse generator's input. the difference becomes larger as the input changes  more rapidly, so pulses are fired more frequently. hence, the more rapidly the  input changes, the more rapidly the pulse generator fires.  adding a fixed charge quantum to the integrating capacitor produces a mul-  tiplicative change in current--due to the exponential current-voltage relation  in subthreshold. hence, the larger the current level, the larger the step size.  the result is adaptive quantization. i also use the diode-capacitor integrator in  the postprocessor to integrate the pulses, and to reconstruct the current level  that was encoded into pulse frequency.138 neuromorphic systems engineering  i discuss the behavior of these circuits in detail in sections 5.3.1  through 5.3.3. the performance of the solutions adopted are analyzed, with  emphasis on the tradeoffs inherent in the circuit topologies chosen.  5.3.1 preprocessing: spatiotemporal bandpass  using the small-signal model of the coupled current-spreading networks shown  in figure 5.7, i found that  io + v~v~/r~c = g~ov~ + c~o~/~ + g~v~,  g~vc + v~v~/r~ = g~ov~ + c~o~, (5.1)  in the continuum limit. here, v~ is the voltage in the excitatory network,  which models retinal cones; vh is the voltage in the inhibitory network, which  models retinal horizontal cells (hcs); and io is the photocurrent [6]. these  functions are now continuous functions of space, (x, y), and of time, t; ~72f is  the laplacian of f (i.e., o~f/cgx ~ + c92f/cgy~), and ] is the temporal derivative  io(.r,,_l) .//~o(.~',,} ~ io(.~',~+1) ,  ( / ia a  gco< ~ ~  ......  ~ ~ ~  g~,c l "~( :c~,-1  ~ ~ ~  figure 5.7 linear circuit model of the outer plexiform layer (opl) circuit. two resistive  networks model the intercone and the inter-horizontal-cell electrical synapses (gap junc-  tions), and transconductances model the reciprocal chemical synapses between cones and  horizontal cells. ~ is the pixel size; it relates the modeled quantities, which are in current  per unit area, sheet resitance, conductance per unit area, and capacitance per unit area, to  quantities in the real circuit. the model is analyzed in the continuum limit, where ~ --~ 0..ioi.a'el.[oq oiiins oq& "[9] so!auanboaj lni,:~nds ti$i, q an ssndmo i somoaoq stli,lln a inaod  -mo~ oq:) pun 'sot.auonboaj inaodulo:) q~i,q :~n ssndato i souloaoq ~ui,un ~ in!p, nds or&  "aaaig faodmaa ssndptlnq n pun aoaitj fi,:)nds ssndpunq n jo o:)l.sodmoa oq:) .zld  -rots aou s~. :)i, 'si. :)nqa !oiqnandos xi:tnoui,i aou si, osuodsoa iin.ioao oq:) '.ioaoato h  â¢ (xai,ai,ai,suas  ~ui,~n~ ) s~!puonbo n i~odm~ mo i ]~ ss~dpu~q st osuodso~ 2~uonb~j-i~ds  ~q] ~q~ pu~ '(x~dd~suos ~g) soduonbog f~ds tool ~ ss~dpu~q s~ osuods  -o~ x~u~nbog-f~odmo~ oq] ~q~ ~h~osqo 'am/vduo = ~ 'i'0 = ~ '~'0  = ~ 'sm00g = ~z 'sm0~ = ~z 'ou0 = ~3 'oÂ£0'0 = ~ :s~oiioj s~ ~ posn  san[~a saaaam~a~d jo aas aq~ -Â£~ o~n~ u~ poalo[d s~ sfÂ£l~uv s~qa moaj potq~a  -qo ~aomaau auoa xaoamdxa aqa jo asuodsoa Â£auonbaaj lvaodmaao~a~ds aq&  '(~) ,{3uanbajj  lejodujal pue (d) ,~3uanbajj lep, eds snsjaa aseqd (q) pue apnl!u:~eua (e) ~oqs s~,old a3ej  -ms leuo!suatu!p-aa~q~, asaq_l 'lapoua -1do ~eau!l jo ~i!a!i!suas lemdma~,op, edcj 8"Â£ am~!_-i  (q) (n)  (#apie~eqe [(~d/a~)aal#o~  "'esaoa oai.a pu~ 'auoa oq:) o:) dh Â°td moaj sut~$  o~][oa dooi-uado oqa aa~ ~ pue ~ jo si~aoadpaa aq& "oaumanpuoasu~aa aq~ oa  oaumanpuoa a~a i 1o so~a aqa aa~ ~6/o~6 = ~ pu~ ~6/o~6 = ~ pu~ ~punoa~  oa saau~aanpuoa 2q paa~idaa saau~aanpuoasu~aa qa~ 's~ao~aou p~idnoaop aqa jo  saumsuoa oa~ds aql oar a/t_(~6~a) = ~ pu~ a/~_(~6~) = ~ ~xiaa~aaodsoa  '$u~idnoa ohma-auoa aqa qa~m pu~ su~idnoa auoa-m-oh aqa qa~m poa~pos~  sau~asuoa om~a oqa aa~ "~6/o~a = ~ pu~ ~'6/o.a = .~ 'oao h â¢[9] (suv:pva  u~ qaoq) xauanboaj faod=aa s~ ~ puv 'xauanboaj f~a~ds s~ (~d + {d)~ = d  '(~ '~ 'z), jo ~aojsu~aa aa~ano d aqa soaouop (m' ~d'~o)[ .~i/~d ~ (~'d)~g aaaq~  . d~fap , i + (~ + m~ + z ae~ + m~ + ad~) ~6  . d ~ ~ + m~, + a ~ i  u~aqo oa suo~avnbo or10aios pu~ 'om~a pu~ aa~ds u~ smaojsuvaa ao[ano d  a~m i 'suo~l[puoa i~l~u~ snoauo$omoq pu~ ~uolxa f!a~ds oa~ugu~ su~mnss v  '[z~ 'e~ 'g*] ~  pazxlvn~ pu~ pasodoad aaa~ auo sna oa an~mf siapo ~ '(~0/]0 "a'~) f jo  6~i hdvohddv dihdhoinoni&~h140 neuromorphic systems engineering  is observed in physiological data measured from cats [20] and psychophysical  data measured from humans [26].  there are tradeoffs among small low-frequency response, large dynamic  range, and high sensitivity. the circuit requires a high-gain cone-to-hc  synapse (i.e., small eh) to attenuate the cone's response to low spatial and  temporal frequencies, since /t/c(0,0) = eh/gÂ¢h. however, increasing the gain  of the cone-to-hc synapse decreases the dynamic range of the cone, (i.e.,  vc < hvlin, where vlin is the linear range.) it also makes the circuit ring  since, q = (e~x/~ + ehv/~) -1.  smith and sterling realized this constraint on the loop gain, and proposed  using feedforward inhibition to second-order cells (bipolar cells) to attenuate  the low frequency response [37]. alternatively, to maintain temporal stability,  we can decrease the gain of the hc-to-cone feedback synapse (1/e~), or reduce  the hc's time constant (~-h). unfortunately, both changes reduce the peak  sensitivity of the cone 1)(0, &) = qv/(~-h/~-~). the circuit implementation shown  in figure 5.6 has high gain from the excitatory cone node (v0) to the inhibitory  hc node (w0), giving it small dc response and high sensitivity, but poor  temporal stability.  5.3.2 amplification: local automatic cain control  i achieve local agc by making the intercone conductance (l/roe) proportional  to the local average of the photocurrent. this adaptation is realized in the  circuit simply by the fact that (vdd-v0) is equal to the sum of the gate-source  voltages of two devices. the currents passed by these devices represent the  activity in the inhibitory network, ih, which is equal to the local average of  the intensity, and represent the activity of the excitatory network, ic, which  is equal to the laplacian of the smoothed intensity profile (see equation 5.1).  hence, by the extended transfinear principle [1], the current that spreads in  the excitatory network is proportional to the product, icih, of these currents.  since ih scales with the intensity, the internode coupling in the excitatory cone  network will scale accordingly [9].  it remains to show that the response of the excitatory cone network is pro-  portional to the intercone resistance [5]. closed-form solutions for the impulse  response may be obtained in the one-dimensional space:  vc(x) = rccio2~e-lx[/l sin(ixl/f - ~r/4),  where l = ~ = (rccgchrhhghc) -1/4 is the effective space constant of the  dual-layer network. these solutions are valid for the case go0 = gho : 0, which  is a fairly good approximation of the actual circuit. linear system theory thus  predicts that the gain of the cone is equal to the product of the space constant  and the intercone coupling resistance.  this analysis also reveals that we compromise receptive-field size constancy  by using the intercone coupling to implement local agc, because the spaceretinomorphic approach 141  constant depends on the intercone resistance: l = (rccgchrhhghc) -1/4. thus,  as we increase rcc to increase the gain, the receptive field contracts.  the images shown in figure 5.9 demonstrate the effects of bandpass filter-  ing and local agc. these data are from the retinormorphic chip described  in [9]; images of the same scenes acquired with a ccd camera are included  for comparison [11]. 3 bandpass filtering removes gradual changes in intensity  and enhances edges and curved surfaces. it also reduces the variance of the  amplitude distribution by setting uniform areas to the mean level (gray). lo-  cal agc extends the dynamic range by boosting the gain in the dark parts of  the scene. thus, the retinomorphic chip picks up information in the shadows,  whereas the output of the ccd camera is zero throughout that region.  unfortunately, the retinomorphic chip's output is noisier in the darker parts  of the image, due to the space constant decreasing with increasing gain. when  the space constant decreases, wideband salt-and-pepper noise is no longer atten-  uated, because the cutoff frequency shifts upward. the dominant noise source  is the poor matching among the small (4l Ã 3.5l; where l, the minimum  feature size, is 2#m) transistors used--it is not shot noise in the photon flux.  nevertheless, when it replaced the ccd as the front-end of a face-recognition  system, the 90 Ã 90-pixel opl chip improved the recognition rate from 72.5%  to 96.3%, with 5% false positives, under variable illumination [11].  figure 5.9 ccd camera (top row) versus retinomorphic imager (bottom row) under  variable illumination. the ccd camera has global automatic gain control, whereas the  retinomorphic imager has local automatic gain control and performs bandpass filtering.142 neuromorphic systems engineering  5.3.3 quantization: adaptive neuron circuit  i built an adaptive neuron circuit by taking a pulse generator and placing a  diode-capacitor integrator around it in a negative-feedback configuration (see  figure 5.6). the pulse-generation circuit has a high-gain amplifier (two digital  inverters) with positive feedback (capacitive divider) [8]. the high-gain ampli-  fier serves as a thresholding device, and the positive-feedback network provides  hysteresis. in addition, there is a reset current ([reset) produced by the logic  circuit that terminates the spike.  the response of the adaptive neuron circuit to a 14% change in its input  current is shown in figure 5.10; these data demonstrate the adaptive sampling  rate, the adaptive step size, and the integration of pulse trains by the diode-  k(c|) channel current for step input  .................. ,~,., .,..~..~ ,.~,~ .......... ?. ..... ,.",.-~ .....  ~. i -~ ~ ~-  ~, ~-.._~j ~ ~ -~ - (*~ ! "-  ~'\ ~i ~\'d  ..... ,~',~r"~ '~'/' "~  â¢ ~<,[,.~:~,,,~!4,,~  oo~ ~ ~ o.~ ~ ~ ~ o1~ o1~ ~ ~a o1~ ~  t~m~ ~e~  m b a voitag f st p input em r ne e or e  â¢ ! ~ : ,~,~ ,~  /, ill i !!il ~  ~: ~:.:r: i il i  / i"!!~!!!i ( ; " /  ~1~ c~  ooe o.oa o.1 o12 o14 o16 ola 02  t~me cseos)  spike train for step input  ~ ~  , o  time (s)  figure 5.10 adaptive neuron's step response. top: the neuron's input current and the  integrator's output current. middle: input voltage ramping up between the reset (1.5v) and  threshold levels (2.2v). bottom: the spike train. the difference between the input current  and the integrator's output current ramps up the input voltage as the surplus current charges  the input capacitance.retinomorphic approach 143  capacitor integrator. other designs for adaptive neurons are described in [177,  52].  the diode-capacitor integrator is based on the well-known current-mirror  circuit. a large capacitor at the input of the mirror integrates charge, and the  diode-connected transistor leaks charge away. this circuit's temporal behav-  ior is described by the following nonlinear differential equation in the current  domain:  qt d/out  /out(t) dt - iin(t) - jflout(t),  where ut = kt/q is the thermal voltage, a = exp(va/ut ) is the current gain  of the mirror, and qt = cut/n is the charge required to change the current  by a factor of e = 2.72 [3, 30]. this circuit has a time-constant "r = qt/iout(t),  that is proportional to the current level due to exponential current-voltage  relation in the subthreshold region.  the output produced by a periodic sequence of current pulses is  1  /out(t0 + nt) = 1~it + (1~lout(to) - l/it)(1 + a) -n, (5.2)  immediately after the (n + 1)th pulse; it decays as  /out(t) = iout(t0 + nt)  iout(t0+.t) (t - (to + nt)) + 1' aqt  during the interspike interval, to + nt < t < to + (n + 1)t, where a =  (exp(qa/qt) - 1) is the percentage by which the output current is incremented  by each spike, and it = aaqt/t is the steady state [3].  we can use the current-mirror gain a to control the decay rate, because the  time scale is set by ~- = aqt/iout. the fixed quantity of charge qa supplied  by each current pulse multiplies the current by exp(qa/qt), since it takes qt  to change the current by a factor of e = 2.72. hence, the incremental change  in the output current caused by a spike is not fixed: it is proportional to the  output current level at the time that the spike occurs. the peak output current  levels attained immediately after each spike converge to it =-- aaqt/t when  (1 + a) -n << 1. therefore, the equilibrium output current level is proportional  to the pulse frequency.  the complete adaptive neuron circuit is described by two coupled differential  equations:  dvmem cmem dt -/in - ik -- qth~(vmem -- vth),  1 qt die _ q~5(vmem - vth) - xik,  ik dt  where/in is the current supplied to node vmem by the opl circuit, and cmem  is the total capacitance connected to that node; ik is the current subtracted144 neuromorphic systems engineering  from node vmem by the integrator; cca is the integrator's capacitance; qt =  ccauw/~ is the charge required to change the integrator's output current by a  factor of e = 2.72; and qth is the repolarization charge (i.e., the charge that we  must supply to vmem to bring the latter from the reset level to the threshold  level (vth)).  the parasitic coupling capacitance between vmem and the integrator's input  node is not included in these equations. this capacitance can have a large  influence on the circuit's behavior [3]. in this particular design, however, the  cascode device between the integrator's output and the pulse generator's input  (tied to vreset) eliminates virtually all coupling.  for a constant input current, these equations may be integrated to obtain  ) qth = iin/kn -- aqt in \k--~t + 1 ,  where an -= tn+l - tn is the interspike interval.  when adaptation is complete, the interspike intervals become equal, and we  have ikn = aaqt/an (from equation 5.2). hence,  an = (qth + =  qth/iin  (remember that q~ -- qt ln(1 + ~)). this result is understood as follows.  during the interspike interval, an, the input current must supply the charge  qth to the capacitors tied to vmem, and must supply the charge aq~ removed  by the integrator, where q~ is the quantity of charge added to the integration  capacitor by each spike. notice that firing-rate adaptation reduces the firing  rate by a factor of y ~ 1 + aq~/qth.  it is preferable to have i~(t) < /in(t) for all t, because v~m stays close to  the threshold, making the latency shorter, and less variable, and keeping the  integrator's output device in saturation. the circuit operates in this regime if  ~/ < 2/a [3]. a tradeoff is imposed by my desire to operate in this regime: if  we want a large adaptation-attenuation factor ~, we must use a small charge  quantum q~and must turn up a to compensate~making the number of spikes  required to adapt large.  5.4 overall system performance  the output of the postprocessor--after image acquisition, analog preprocessing,  quantization, interchip communication, and integration of charge packets in the  receiver's diode-capacitor integrators--is shown in figure 5.11. the sparseness  of the output representation is evident.  when the windmill moves, neurons at locations where the intensity is in-  creasing (white region invading black) become active; hence, the leading edges  of the white vanes are more prominent. these neurons fire more rapidly as the  speed increases because they are driven by the temporal derivative. the time  constant of the receiver's diode-capacitor integrator is intentionally set shorter  than that of the sender, so temporal integration occurs at only high spike rates.retinomorphic approach 145  figure 5.11 video frames from postprocessor chip showing real-time temporal integration  of pulses. the stimulus is a windmill pattern (left) that rotates counterclockwise slowly  (middle) and quickly (right).  this mismatch attenuates low frequency information, and results in an overall  highpass frequency response that eliminates the fixed-pattern noise and en-  hances the imager's response to motion. the mean spike rate was 30hz per  pixel, and the two-chip system dissipated 190 mw at this spike rate.  5.5 discussion  i have described the design and performance tradeoffs of a retinormorphic im-  ager. this vlsi chip embodies four principles of retinal operation.  first, the imager adapts its gain locally to extend its input dynamic range  without decreasing its sensitivity. the gain is set to be inversely proportional to  the local intensity, discounting gradual changes in intensity and producing an  output that is proportional to contrast [9]. this adaptation is effective because  lighting intensity varies by six decades from high noon to twilight, whereas  contrast varies by at most a factor of 20 [34].  second, the imager bandpass filters the spatiotemporal signal to attenuate  low-frequency spatial and temporal signals, and to reject wideband noise. the  increase in gain with frequency, for frequencies below the peak, matches the  1if 2 decrease in power with frequency for natural image spectra, resulting  in a flat output power spectrum. this filtering improves information coding  efficiency by reducing correlations between neighboring samples in space and  time. it also reduces the variance of the output, and makes the distribution of  activity sparse.  third, the imager adapts its sampling rate locally to minimize redundant  sampling of low-frequency temporal signals. in the face of limited communi-  cation resources and energy, this sampling-rate adaptation has the additional  benefit of freeing up capacity, which is dynamically reallocated to active pixels,  allowing higher peak sampling rates and shorter latencies to be achieved [4].  and fourth, the imager adapts its step size locally to trade resolution at  high contrast levels, which rarely occur, for resolution at low contrast levels,  which are much more common. the proportional step size in the adaptive  neuron, which results in a logarithmic transfer function, matches an exponen-  tially decaying amplitude probability density, making all quantization intervals146 neuromorphic systems engineering  equiprobable. hence, it maximizes the expected number of signals that can be  discriminated, given their probability of occurrence.  for independent samples, information is linearly proportional to bandwidth,  and is logarithmically proportional to the signal-to-noise ratio (snr) [36]. we  increase bandwidth by making the receptors smaller and faster, so that they  can sample more frequently in space and time. as an unavoidable consequence,  they integrate over a smaller volume of space time and therefore snr degrades.  there is therefore a reciprocal relationship between bandwidth and noise power  (variance) [38]. since their goal is to maximize information, biological sensory  systems aggressively tradeoff snr for bandwidth, operating at snrs close to  unity [14, 38].  with this optimization principle in mind, i have proposed compact circuit  designs that realize local agc, bandpass filtering, and adaptive quantization  at the pixel level. the overriding design constraints are to whiten the signal,  which makes samples independent; to minimize the pixel size and capacitance,  which makes sampling more dense and more rapid; and to minimize power  consumption, which makes it possible to acheive very-large scale integration.  hence, all circuits use minimal-area devices and operate in subthreshold, where  the transconductance per unit current is maximum. i realized extremely com-  pact implementations by modeling these circuits closely after their biological  counterparts [3, 9].  i analyzed three limitations in these simple circuit designs.  first, attenuating low frequencies by using a high-gain receptor-to-hc  synapse (ratio of 9hc/gho = 1/eh) results in temporal instability. to break  this tradeoff, we must regulate the gain dynamically.  second, controlling the gain by changing the receptor-to-receptor coupling  strength compromises the receptive field size. to decouple these parameters,  we must change one of the synaptic strengths (transconductances, gch or ghc)  proportionally.  and third, attenuating the firing rate by using an integrator with a long  time constant results in extremely slow adaptation, because we must use a  small charge quantum to avoid sending the integrator's output above the input  level. the circuit would adapt more rapidly, and fire fewer spikes in the process,  if it maintains a uniformly high firing rate until the integrator catches up with  the input, and then switches abruptly to a low firing rate.  5.6 conclusions  taking inspiration from biology, i have described an approach to building ma-  chine vision systems that perform sophisticated signal processing at the pixel  level. these retinomorphic systems are adaptive to their inputs, and thereby  maximize their information-gathering capacity and minimize redundant infor-  mation in their output data stream.  these optimization principles are radically different from those that drive the  design of conventional video cameras. video cameras are designed to reproduceretinomorphic approach 147  any arbitrary image to within a certain worst-case error tolerance, whereas  biologics exploit the statistical properties of natural spatiotemporal signals,  giving up worst-case performance to get better average-case performance.  optimizing average-case performance maximizes the discrimination ability  of biologics. consequently, biomorphic systems promise superior solutions for  human-made systems that perform perceptive tasks, such as face recognition  and object tracking, energy efficiently.  acknowledgments  this work was partially supported by the office of naval research; darpa; the  beckman foundation; the center for neuromorphic systems engineering, as part  of the national sceince foundation engineering research center program; and the  california trade and commerce agency, office of strategic technology.  i thank my thesis advisor, carver mead, for sharing his insights into the operation  of the nervous system. and i also thank misha mahowald, tobi delbriick, john  lazzaro, and peter sterling for numerous helpful discussions.  notes  1. i am assuming a linear encoding--a practice that is the standard. this assumption  limits the dynamic range to 2 b for a b-bit encoding.  2. i performed bandpass filtering by convolving the input image with the laplacian of a  gaussian with a = 2.5 pixels. i calculated the autocorrelation of the images by subtract-  ing out the mean, shifting a copy of the image up or right by 1 to 75 pixels, multiplying  corresponding pixels, and summing; i normalized the results to yield a maximum of unity.  rightward shifts are plotted on the positive axis (0 to 75), and upward shifts are plotted on  the negative axis (0 to -75).  3. ccd camera specifications: cohu solid state rs170 camera (142320), auto iris,  gamma factor enabled, 512 x 480 pixels, 8-bit gray-level outputs. lens specifications: cos-  micar tv lens, es 50inm, 1:1.8. this comparison, and the face recognition studies, were  done in collaboration with frank eeckman, joachim buhman, and martin lades of the  lawrence livermore national labs, livermore ca.  references  [1] a. andreou and k. boahen. translinear circuits in subthreshold mos. j.  analog integrated circ. sig. proc., 9:141-166, 1996.  [2] j. atick and n. redlich. what does the retina know about natural scenes.  neural computation, 4(2):196-210, 1992.  [3] k. boahen. the adaptive neuron and the diode-capacitor integrator. in  preparation.  [4] k. boahen. communication neuronal ensembles between neuromorphic  chips. in preparation.  [5] k. boahen. toward a second generation silicon retina. technical report  cns-tr-90-06, california institute of technology, pasadena ca, 1990.148 neuromorphic systems engineering  [6] k. boahen. spatiotemporal sensitivity of the retina: a physical model.  technical report cns-tr-91-06, california institute of technology,  pasadena ca, 1991.  [7] k. boahen. retinomorphic vision systems. in int. conf. on microelec-  tronics for neural networks, volume 16-5, pages 30-39, los alamitos, ca,  1996. epfl/csem/ieee.  [8] k. boahen. retinomorphic vision systems ii: communication channel  design. in proceedings of the ieee international symposium on circuits  and systems, volume supplement, pages 9-14, atlanta, ga, may 1996.  [9] k. boahen and a. andreou. a contrast-sensitive retina with reciprocal  synapses. in j e moody, editor, advances in neural information process-  ing, volume 4, san mateo ca, 1991. morgan kaufman.  [10] k. a. boahen. a retinomorphic vision system. ieee micro, 16(5):30-39,  october 1996.  [11] j. buhman, m. lades, and eeckman f. illumination-invariant face recog-  nition with a contrast sensitive silicon retina. in j d cowan, g tesauro,  and j alspector, editors, advances in neural information processing, vol-  ume 6, san mateo ca, 1994. morgan kaufman.  [12] k. bult and g. j. geelen. an inherently linear and compact most-only  current division technique. ieee j. solid-state circ., 27(12):1730-1735,  1992.  [13] p. c. chen and a. w. freeman. a model for spatiotemporal frequency  responses in the x cell pathway of cat's retina. vision res., 29:271-291,  1989.  [14] r. r. de ruyter van steveninck and s. b. laughlin. the rate of information  transfer at graded-potential synapses. nature, 379:642-645, february 1996.  [15] t. delbriick and c. mead. photoreceptor circuit with wide dynamic range.  in proceedings of the international circuits and systems meeting, ieee  circuits and systems society, london, england, 1994.  [16] a. dickinson, b. ackland, e. el-sayed, d. inglis, and e. r. fossum. stan-  dard cmos active pixel image sensors for multimedia applications. in  william dally, editor, proceedings of the 16th conference on advanced re-  search in vlsi, pages 214-224, chapel hill, north carolina, 1995. ieee  press, los alamitos ca.  [17] d. dong and j. atick. statistics of natural time-varying scenes. network:  computation in neural systems, 6(3):345-358, 1995.  [18] d. dong and j. atick. temporal decorrelation: a theory of lagged and non-  lagged responses in the lateral geniculate nucleus. network: computation  in neural systems, 6(2):159-178, 1995.  [19] m. eckert and g. buchsbaum. efficient coding of natural time-varying  images in the early visual system. phil. trans. royal soc. loud. biol,  339(1290):385-395, 1993.retinomorphic approach 149  [20] c. enroth-cugell, j. g. robson, d. e. schweitzer-tong, and a b. watson.  spatiotemporal interactions in cat retinal ganglion cells showing linear  spatial summation. j. physiol., 341:279-307, 1983.  [21] d. j. field. relations between statistics of natural images and the response  properties of cortical cells. j. opt. soc. am., 4:2379-2394, 1987.  [22] b. fowler, a. e. gamal, and d. yang. a cmos area image sensor with  pixel-level a/d conversion. in john h. wuorinen, editor, digest of tech-  nical papers, volume 37 of ieee international solid-state circuits con-  ference, pages 226-227, san francisco, california, 1994.  [23] k. fujikawa, i. hirota, h. mori, t. matsuda, m. sato, y. takamura, s. ki-  tayama, and j. suzuki. a 1/3 inch 630k-pixel it-ccd image sensor with  multi-function capability. in john h. wuorinen, editor, digest of technical  papers, volume 38 of ieee international solid-state circuits conference,  pages 218-219, san francisco, ca, 1995.  [24] b. hoeneisen and c. mead. l~ndamental limitations in microelectronics-i:  mos technology. ieee j. solid-state circ., 15:819-829, 1972.  [25] c. jansson, i. per, c. svensson, and r. forchheimer. an addressable 256  Ã 256 photodiode image sensor array with 8-bit digital output. analog  integr. circ. ~ sig. proc., 4:37-49, 1993.  [26] d. h. kelly. motion and vision ii: stabilized spatiotemporal threshold  surface. j. opt. soc. am., 69(10):1340-1349, 1979.  [27] john lazzaro. temporal adaptation in a silicon auditory nerve. in john e.  moody, steve j. hanson, and richard p. lippmann, editors, advances in  neural information processing systems, volume 4, pages 813-820. morgan  kaufmann publishers, inc., 1992.  [28] m. mahowald and r. douglas. a silicon neuron. nature, 354:515 518,  1991.  [29] m. mahowald and c. mead. the silicon retina. scientific american,  264(5):76-82, 1991.  [30] c. mead. a sensitive electronic photoreceptor. in h. f~chs, editor, 1985  chapel hill conference on vlsi, pages 463-471, rockville md, 1985.  computer science press, inc.  [31] c. mead. scaling of mos technology to submicrometer feature sizes. j.  of vlsi signal processing, 8:9-25, 1994.  [32] c. a. mead and m. ismail, editors. analog vlsi implementation of neural  systems. kluwer, norwell, ma, 1989.  [33] s. ohshima, t. yagi, and y. funashi. computational studies on the inter-  action between red cone and h1 horizontal cell. vision res., 35(1):149-160,  1994.  [34] w. a. richards. a lightness scale for image intensity. appl. opt., 21:2569-  2582, 1982.150 neuromorphic systems engineering  [35] b. sakman and 0. d. creutzfeldt. scotopic and mesopic light adaptation in  the cat's retina. pfldgers archiv f(tr die gesamte physiologie, 313:168-185,  1969.  [36] c. e. shannon and w. weaver. the mathematical theory of communi-  cation. univ. illinois press, urbana il, 1949.  [37] r. g. smith. simulation of an anatomically defined local circuit: the  cone-horizontal cell network in cat retina. visual neurosci., 12(3):545-  561, may-jun 1995.  [38] w. r. softky. fine analog coding minimizes information transmission.  neural networks, 9(1):15-24, 1996.  [39] j. h. van hateren. a theory of maximizing sensory information. biol.  cybern., 68:23-29, 1992.  [40] e. vittoz and x. arreguit. linear networks based on transistors. elec-  tronics letters, 29(3):297 299, february 1993.analog vlsi excitatory  feedback circuits for  attentional shifts and tracking  t.g. morris and s.p. deweerth  school of electrical and computer engineering,  georgia institute of technology atlanta, ga 30332-0250 u.s.a.  tgrnorris@sedona.intel.com  6.1 introduction  in biological vision systems, the term attention describes the way that infor-  mation is prioritized and selected [23]. selective attention is necessary in visual  processing in order to handle the overwhelming amount of sensory information  that is available. visual systems, such as those found in primates, have a hy-  brid architecture in which low-level processing is performed in parallel across  the entire visual field, and high-level processing is only performed on a selected  subregion of the visual field [2]. low-level tasks that are computed entirely in  parallel are described as preattentive. attentive processing uses this preatten-  tive information to select a smaller region of interest for subsequent high-level  processing. the duality of parallel computation and serial selections of regions  of interest exemplifies the trade-off between speed and processing sophistication  that results from the utilization of a limited amount of processing circuitry. if  the attentional selection were not performed, an overwhelming amount of neu-  ral circuitry would be required in order to perform the high-level processing in  parallel over the entire visual field [1].  the transition from preattentive to attentive processing requires the selec-  tion of a region of interest. in 1985, koch and ullman [13] introduced a theory  of selective shifts of attention based upon a saliency map. the saliency map  is a measure of levels of interest across the visual field, created through the  integration of features that are detected preattentively, such as color, orienta-  tion, spatial derivatives, temporal derivatives, and motion. specific features152 neuromorphic systems engineering  may be attenuated or amplified, depending upon the visual processing appli-  cation [4, 23]. the saliency map encodes where interesting features are within  the visual field, but not what those features are [21]. measurements of neurm  activity in the posterior parietal cortex, the pulvinar, and the superior collicu-  lus demonstrate characteristics that are consistent with the idea that a saliency  map exists [4]. koch and ullman proposed that shifts in selective attention are  determined by a winner-take-all computation that is performed on the saliency  map information. furthermore, the transition of attention to each new location  exhibits a preference to salient features within close spatial proximity to the  most recently attended location [13].  the attentive search task is defined as a sequential search from one location  of the visual field to the next. this sequential search method has been ex-  plained as a "spotlight" of attention that moves across the visual field [2, 27].  the selection process, which drives the movement of the spotlight, must deter-  mine whether to fixate on a presently selected region of interest or move to a  new region of interest. two main theories have emerged to describe how the  spotlight moves from one point to the next [25, 27]. one theory, the analog  description, states that the spotlight moves in a continuous path, which would  imply that locations between the two points of interest are not neglected in the  transition. another theory, the quantum description, states that there exist  several spotlights that turn on and off, depending on where the attention is  directed. the latter theory has been supported by experiments showing that  the time required to make an attentional shift is independent of the distance  traversed [24]. the winner-take-all selection mechanism that was described by  koch and ullman [13] is consistent with the quantum theory of attentional  shifting.  attentional processing is important for the localization and discrimination of  objects within the visual field. the primate visual system is able to accomplish  these tasks in spite of extremely cluttered environments containing stimuli that  are constantly moving with respect to the sensing array. the process by which  regions of interest are selected plays a key role in the success of these visual  systems. in an attempt to understand how such high performance is attained,  we have implemented a model of selective attention within the neuromorphic,  analog very large-scale integrated (vlsi) [18] paradigm. the development of  these neuromorphic circuits may help us model similar cortical processing ar-  chitectures found in the visual system of primates. hardware implementations  also provide an obvious contribution to machine vision systems by using the  massively parallel architectures to achieve real-time performance.  previous work in the analog vlsi community has focused on visual pro-  cessing tasks that model the preattentive processing within biological sys-  tems [1, 182, 10, 51, 18]. these preattentive features, such as spatial deriva-  tives, temporal derivatives, orientation, color, and motion can be used to create  a saliency map for the inputs to the selective attention circuitry. a number  of researchers within the neuroscience community are also working toward an  understanding of how these features might be combined to create a saliencyexcitatory feedback circuits 153  map [20, 21, 22]. our efforts lie in the processing that occurs after the feature  detection stage, and thus the specific choice of a saliency map is not critical to  the evaluation of the circuits we present in this paper.  there are two aspects of selective attention that we have chosen to address  with our analog vlsi implementation of attentional processing. these aspects  include the modeling of shifts of selective attention from one object of interest  to the next and the ability to attend to a single object as it moves across the  visual field. we use a winner-take-all computation with excitatory feedback to  model attentive selection [7, 15]. the excitatory feedback enhances a neighbor-  hood around the location that is presently attended, thus facilitating shifts of  attention that demonstrate a preference to stimuli that are within close spa-  tial proximity [8, 19]. the localized excitation also enables tracking of a single  stimulus within a noisy environment. the shape of the excitatory feedback  directly influences the system performance, both as a model for shifts in atten-  tion and as a tracking mechanism. the use of excitation produces a hysteresis  in the selection process. when the presently winning stimulus is excited, the  winner-take-all computation resists the selection of a new winner unless that  stimulus has a value much greater (as set by the excitation level) than the  present winner [7].  in this paper we present hardware implementations of the winner-take-all  computation with excitatory feedback to perform attentive selection. specifi-  cally, we present four different forms of excitation: local excitation, resistive-  spreading excitation, discrete (nearest-neighbor) excitation, and a combination  of the resistive-spreading and discrete forms that creates a convolved excita-  tion. the local excitation only provides excitation to the single pixel that is  presently winning in the selection process. the resistive-spreading excitation  uses a current-mode resistive network to create an exponential decay of ex-  citation with a variable space constant that is controlled by a single voltage  input. the discrete excitation provides excitation to the winning pixel as well  as to the nearest neighbors on each side, using a voltage input to determine  the gain of the communication to the nearest neighbors. the combination of  resistive-spreading excitation and discrete excitation produces an effective ex-  citation that is determined by the convolution of the exponentially decaying  signal of the resistive network and the three-pixel-wide pulse created by the  discrete excitation. the convolved excitation provides a shape that has similar  characteristics to that of a gaussian distribution. in section 6.2 we describe  the system that was used to test each of these forms of excitation in a selective  attention computation. we describe each of the circuits and their performance  in sections 6.3, 6.4, 6.5 and 6.6. section 6.7 demonstrates the use of these  circuits in a tracking application.  6.2 system architecture  we have fabricated a one-dimensional, 20-element array of selective atten-  tion circuits that include (i) phototransistors, (ii) normalization circuits, (iii)154 neuromorphic systems engineering  winner-take-all selection circuits with all aforementioned types of excitatory  feedback, and (iv) position-encoding circuits that indicate the present location  of attention. the chip was fabricated in a 2.0 #m cmos process through  the mosis fabrication service. figure 6.1 illustrates the organization of the  processing layers.  photodetectors  normalization circuit  l selection circuit with excitatory feedback  ]  position-encoding circuit  figure 6.1 system architecture illustrating the arrangement of processing layers in the  visual attention system.  we have included the capability of providing explicit input currents at each  position within the array in order to analyze the performance of the circuits un-  der well-controlled input conditions. the chip was designed such that we could  switch between using currents that come from the phototransistors or currents  that come directly from the input pads. thus, we are able to achieve two modes  of testing on a single chip. when evaluating the spreading characteristics of the  excitation, we use controllable input currents, and when evaluating the tracking  performance we use intensity levels as the representation of saliency. vertical  bipolar phototransistors are used to convert optical inputs to currents that are  passed into a linear normalization circuit [3]. normalization is necessary to  control the range of the inputs to the selective attention processing layers. we  have access to measurements of the phototransistor currents, the normalization  currents, the winner-take-all output currents, and the voltage output for the  position-encoding circuit.  the selection circuit includes all forms of excitation mentioned previously.  each form of excitation can be tested separately by changing the parameters  associated with the remaining circuitry, such that these influences are turnedexcitatory feedback circuits 155  "off." the separate types of excitation will be further described in subse-  quent sections. the final stage of processing is a position-encoding circuit that  receives an array of inputs and computes the centroid to produce a single ana-  log output [6]. the output of the selection layer serves as the input to the  position-encoding processing layer. this computation has also been used in  previous sensorimotor systems, where it is important to convert a large multi-  dimensional array of inputs into a small number of outputs to control a motor  response [6, 9].  6.3 local excitation  the winner-take-all is a nonlinear computation that receives an array of in-  puts and produces an equally dimensioned array of outputs such that there is  a high output in the location of the largest input and low outputs in all other  locations. when two inputs are very similar in value, the winner-take-all com-  putation can oscillate from one location to the next due to noise in the system.  such oscillations would be counterproductive in an attention system, because  there would not be enough time to process the information at either location.  the local excitation circuit provides a hysteretic feedback that eliminates the  possibility of oscillations in the selection process.  // //  ib  //  //  ih j  'n~ m! ~o~  vn  i ulll vc ~m4  m3  im6  jn  j~  figure 6.2 selection circuit with local excitation.  one element of the winner-take-all circuit with local excitation is shown in  figure 6.2. this circuit is based on a current-mode winner-take-all circuit [15]  that receives an array of input currents in and produces a single high out-  put current or voltage that corresponds positionally to the largest input. the156 neuromorphic systems engineering  winner-take-all selection is facilitated by a pair of transistors, m1 and m2, at  each element. the input transistor m1 in the location with the largest input  current is saturated producing a high output voltage vn at the input node  while all other output voltages are near ground, and the corresponding input  transistors are out of saturation. thus, the winning output current in the m2  transistor is equal to the global bias current ib and all other output currents  are 0. the bias current is set by the gate voltage vb on a single transistor  that sinks the sum of the currents in the m2 transistors via the common node  vc. local hysteresis can be added to the selection computation by feedback  of the winner-take-all output current to the input node within each element of  the array [7]. the winner-take-all circuit requires the bias current to be larger  than the nominal input current levels due to possible ringing behavior other-  wise [15]; therefore, it is necessary to attenuate the current that is fed back to  the input, providing the excitation. previous implementations have mirrored  the bias current with an adjustable gain that is controlled by changing the  source voltage on the current mirror [8]. the circuit shown in figure 6.2 uses  a different approach, whereby the excitatory (hysteretic) current ih is gated  by the output voltage of the winner-take-all circuit. the current is set by the  gate voltage vh. this excitatory current is also mirrored as an output current  jn, which serves as the input to the position-encoding circuit. for the local  excitation circuit the output current jn is equal to ih at the winning location,  and is 0 at all other locations.  the performance of this form of excitation was tested by using current in-  puts. the chip was optically shielded within a metal test fixture. the input  current at position 10 within the 20-element array was set to 100 na. for each  iteration of testing, we swept a competing input current at a specific location  within the array. we used a linear current sweep with 1 na increments. the  competing current was swept past the value of the "winning" current at po-  sition 10 until the competing position became the winner. the current level  at the newly winning position was then lowered until position 10 was once  again selected. the difference between the two switching currents determines  the window of hysteresis. the level of excitation that would produce such a  hysteretic window is approximately one half of the difference between the two  switching current levels. the measurement of the hysteretic window allows  us to separate the excitation characteristics from some of the offsets of the  winner-take-all circuit. the data in figure 6.3 demonstrates the performance  of the local excitation circuit when vh is set to 0.71 v and vb is set to 0.8 v.  the hysteretic current, ih, was measured to be 18 na for this voltage setting.  the hysteretic window is large (approximately 30 na) for every location that  is compared to the originally winning location at position 10.  6.4 resistive-spreading excitation  providing excitation at a single winning location is not sufficient for attention if  the stimulus moves with respect to the imaging array. once the stimulus movesexcitatory feedback circuits 157  40 local excitation  ~" 30 Â¢-  -~  t-  .~  n 20  .~  m ~,, -r ~0  o0 .... ~ .... 1'0 .... 1'5 .... 20  position  figure 6.3 experimental data demonstrating local excitation results.  to the next pixel, it must compete with all other inputs, without the advantage  of the excitation. this problem has motivated the use of a distribution of the  excitation to neighboring pixels. the distribution will allow a stimulus to move  with respect to the imaging array by maintaining some level of enhancement in  the winner-take-all computation. the theories on shifts of selective attention  are consistent with this idea of distributed excitation to a surrounding region  of the presently attended location. with a locally distributed excitation, two  problems are solved. a single attended stimulus can maintain attention as it  moves across the visual field, and shifts of attention demonstrate a preference  to salient stimuli that are in close proximity to presently attended locations.  one way of distributing a signal to a surrounding group of pixels is through  the use of a resistive network. a current-mode resistive network has been  developed that uses a small number of transistors to approximate the linear re-  lationship of a resistive network [1, 14]. the circuit consists of diode-connected  p-channel mosfets that serve as the vertical resistance elements, and lat-  eral n-channel mosfets that serve as the horizontal resistance elements in  the network. the gates of the n-channel transistors are connected to a global  voltage, so that the effective resistance of these elements can be changed. the158 neuromorphic systems engineering  spreading characteristic of the resistive network follows the shape of a decaying  exponential, dependent on the distance from the current being sourced. the  space constant is determined by the relationship between the vertical resistances  and the horizontal resistances. the use of a resistive network is advantageous  due to its small size for implementation, and its adaptability to different space  constants with the use of a single voltage.  figure 6.4 ~/  j  // mi'  ii  ii m~ j~  }"  11  j~  selection circuit with resistive-spreading excitation.  the resistive-spreading excitation circuit is shown in figure 6.4. lateral  transistors, mt, connect adjacent elements, creating a current-mode resistive  network [1, 14] that distributes the excitatory current to the elements in the  neighborhood of the winner. the gate voltage, vr, of these transistors sets the  space constant of the network. as vr is increased, more current flows from  the winner into the neighboring elements. the effect of this feedback in the  winner-take-all competition is that elements near the winner receive preference  over distant elements, as determined by the decaying exponential. by using  the exponential relationship between voltage and current for the mosfet in  its subthreshold regime, we have derived an equation that relates the current  for each position of the array to the peak current at the winning location. weexcitatory feedback circuits 159  used the following equation to describe the current-voltage relationship for the  n-channel mosfet in subthreshold operation [18].  ~-~b  ~ = ~ ~  the drain current is denoted by id. the gate and source voltages are denoted  by vgb and vsb, measured with respect to the bulk potential. the thermal  voltage is denoted by vt, the leakage current is denoted by i0 , and the gate  efficiency is denoted by ~, which models the backgate effect. the p-channel  mosfet is similarly modeled by changing the signs of the voltages.  the fact that the input current to the resistive network is at a single location  (determined by the winner-take-all computation) allows us to derive a relation-  ship that is fairly concise. by approximating ~ for the p-channel transistors as  1, we were able to derive a closed-form solution that describes the qualitative  behavior of this circuit. the equation that relates the current output of the  resistive network to the peak current at the winning position is as follows:  in = ino e-~b-nÂ°l (6.1)  where in is the current at location n, and no is the location where the input  current is being sunk (the winning location). the space constant, a, is defined  by the following equation which shows the dependence on the voltage v~.  ( v~-~v~ )  c~ = nln e vr +1  as the value of v~ gets very small, the value of a tends toward a large  number, thus providing virtually no spreading to neighboring elements in the  array. as the value of v~ increases, the value of c~ decreases and the amount of  spreading increases. the value of the peak current ino that occurs at position  no is also dependent on v~.  ioe v~ . ih (6.2) ino : ~ ~ v~  ioe vt + 2ioe-wi-r  as the spreading increases, with the increase of v~, the peak value decreases,  as indicated by equation 6.2. when v~ is very small, the ioe ydd/yt term in the  denominator dominates and the value of ino is very close to ih.  figure 6.5 shows experimental results of the resistive-spreading excitation  circuit. the same test that was described for the local excitation circuit was  performed for varying values of the voltage vr. as vr was increased, the value  of the voltage vh was also increased to achieve a comparable peak current value  at the winning location. the three curves of figure 6.5 demonstrate how the  hysteretic window follows an approximately exponential decay as the competing  position increases in distance from the originally winning location. the value160 neuromorphic systems engineering  40 resistive-spreading excitation  ~" 30 t-  -~ ~-  .~ n 20  .~  ffl  ~" ~0 --r v r = 6.1~  .... ] .... 'f'  o0 5 10  position 15 20  figure 6.5 experimental data demonstrating resistive-spreading results for varying values  of v~.  of ydd that was used for all the experimental data was 5.00 v. the first curve,  which demonstrates the least amount of spreading, was generated by setting  vr to 6.15 v and vh to 0.74 v. the peak hysteretic current, ino, was measured  to be 18.04 na. the second curve shows a moderate amount of spreading, and  was generated by setting vr to 6.21 v and vh to 0.77 v. for this parameter  setting, i~ o was 17.83 na. the spreading was further increased by setting v~  to 6.28 v and vh to 0.80 v in the third trial, and for this parameter setting,  ino was 18.27 na.  the current-mode resistive network provides a solution for the distribution  of excitation that is a compact implementation in an analog vlsi system. the  transistor count for each pixel only increases by one from the local excitation  implementation. the use of the voltage v~ provides flexibility in setting the  spatial extent of the excitation. this flexibility is helpful when testing the  effects of shifts of attention due to the influence of proximal excitation.excitatory feedback circuits 161  6.5 discrete (nearest-neighbor) excitation  a problem with the resistive network for the tracking task is that the distri-  bution follows an exponentially decaying curve, which means that the amount  of excitation that is fed back to the nearest neighbors is drastically reduced  from the amount of excitation that the winning element receives. this smaller  amount of excitation may not be enough to maintain attention on a stimulus  in a noisy environment when there are other stimuli within the visual field  that have similar saliency values. when addressing the problem of maintain-  ing attention on a single stimulus as it moves across the visual field, it makes  sense to consider distributing the excitation only to the nearest neighbors of  the winning element. such an approach would assume continuous movement  of the attended stimulus from one pixel to the next. because this approach  would only require local communication within the processing array, it is well-  suited for implementation using analog vlsi circuits. we have implemented  this form of current distribution by mirroring the excitation current from the  winning element to the nearest neighbors. we refer to this method of distri-  bution as discrete excitation, because the communication to surrounding areas  implements spreading to a discrete number of pixels. the communication is  determined at design time, according to the number of wires that are used to  connect each pixel to its neighbors.  invn l u,~ ~ m3~44~ ai~j n u~ ~-  //  // [ m2  figure 6.6 selection circuit with discrete (nearest-neighbor) excitation.162 neuromorphic systems engineering  the schematic for the discrete excitation circuit is shown in figure 6.6. the  only change from the local excitation circuit is the addition of two transistors,  ms and mg, that are used to mirror the excitation current from each neighbor-  ing pixel within the array. (a two-dimensional implementation would require  four or six additional transistors, depending on the sampling grid.) the source  voltage, vdisc, for these mirror transistors is used to control the amount of gain  in the communication. by adjusting this voltage, the distribution can take on  the shape of a pulse or a windowed triangle function. the relationship between  the excitatory current at the winning location and the excitatory currents at  the neighboring locations is described as:  ydisa-- ydd  ino+l = ino-1 = ino e vt  40 discrete (nearest-neighbor) excitation  < 30 ~-  -~ ~-  o~  n 20  .~  i~  ffl ~,, -r ~0 vdisc = 5.01 v  0 0 .... ~ ' ' ~ f'~-; ....... 1 ~5 20 vdisc = 5.02 v ~./t/ ii  i~' vdisc = 5.03 v ~  5 10  position  figure 6.7 experimental data demonstrating discrete excitation ressults for varying values  of vdisc.  the experimental results shown in figure 6.7 demonstrate the use of this  form of excitatory distribution for three values of vdisc. as in previous experi-  ments, the current at position 10 was set to 100 na while a competing current  was swept at each position to determine the hysteretic window. the curve that  demonstrates a flat signal across positions 9, 10, and 11 was achieved by settingexcitatory feedback circuits 163  vdisc to 5.03 v. the other two curves demonstrate the circuit's operation when  vdi~c is set to 5.02 v and 5.01 v. the gain of the mirroring to the neighboring  pixels increases as vdi~ increases.  the discrete excitation circuit is a good solution to the problem of attending  a single stimulus that moves across the visual field in a noisy environment.  further evidence to support this idea is presented in the tracking experiments  shown in section 6.7 of this paper. there are disadvantages to using only this  approach in a selective attention system. the proximal enhancement for shifts  to new objects of interest is not possible due to the limited communication that  is feasible with direct wiring. by the same token, the extent of the excitation  cannot be altered once the hardware has been fabricated.  6.6 convolved excitation  the primary strength of the discrete excitation circuit is its ability to excite  neighboring pixels with very little attenuation. the primary strength of the  resistive-spreading excitation circuit is its ability to provide a distribution of  excitation that can be changed. the goal in our selective attention system is  to include the strengths of both forms of excitation. the desired distribution  would have little attenuation to the nearest neighbors and then decay with a  space constant that can be controlled during operation. we have implemented  an excitation circuit that addresses both goals. by combining the resistive-  spreading circuit with the discrete excitation circuit, we have developed a cir-  cuit that computes the convolution of the exponentially decaying signal with  the three-pixel-wide pulse. the result is a distribution that has some similar  characteristics to that of a gaussian distribution. there are many parameters  that can be set to adjust the shape of the distribution, such as the shape of the  pulse (from a square to a triangle), the space constant of the resistive network,  and the peak current value for the resistive network.  a schematic for the convolved excitation circuit is shown in figure 6.8. the  resistive network spreads the hysteretic current, ih, in the same way as the  resistive-spreading excitation circuit. the distributed current is then mirrored  to the nearest neighbors, where the gain of the mirror can be adjusted by setting  the source voltage valise. setting the gain of these mirrors is the same as setting  the coefficients for an fir (finite impulse response) filter. the total current  that excites each location can be defined as follows:  i[n] = i~[n] + bl . l.[n + 1] + bl . l.[n - 1]  where the coefficient bl is the gain of the mirror, e (vd~sc-v~)/vr, and the func-  tion,  zr[n] = 1no e-~ln-nÂ°l  is the output of the resistive network that was described in (6.1). the result of  these relationships is the sum of three shifted exponentials, two of which have  a variable gain in the summation.164 neuromorphic systems engineering  /~  ~l /~_  "  vn m3 im  ~ 2  /~- ~/ i ~  ii i~ ii  figure 6.8 selection circuit with convolved excitation (combination of resistive and dis-  crete excitation).  experimental results demonstrating the operation of the convolved excita-  tion selection circuit are shown in figures 6.9 and 6.10. the value of vr was  set to 6.21 v and the remaining parameters were changed to produce different  shapes of the hysteretic window. the first experiment, shown in figure 6.9,  demonstrates how the curve changes as the voltage vdisc is set to 5.03 v, 5.02 v,  and 5.01 v. this voltage sets the gain of the filter coefficients. as these co-  efficients increase, the difference between the peak excitation at the winning  location and the excitation at distant locations becomes much larger. this  trend is evident in the data for each value of vdisc. the data in figure 6.10  demonstrates the trend as the voltage vh is set to 0.75 v, 0.73 v, and 0.71 v.  as the input current to the resistive network, ih, decreases, the excitation curve  takes on a smaller range of values.  the convolved excitation addresses the two issues of attentional processing  that we have stressed in this paper. the high level of excitation that is mir-  rored to the nearest neighbors of the selected location enables the tracking of aexcitatory feedback circuits 165  60  < ~-  v  ) 40  0 "i~  c"-  ,~  .~  ~ 20 m  --r convolved excitation  0 5 10 15 20  position  figure 6.9 experimental data demonstrating convolved excitation results for varying val-  ues of vaisc, vr = 6.21 v and vh = 0.75 v.  single stimulus within a noisy environment. the ability to change the charac-  teristics of the spreading (the space constant) during operation is important for  the modeling of shifts of attention from one stimulus to the next by enhancing  proximal locations. the convolved excitation circuit does require a few more  transistors in each pixel, but the approach achieves the excitation shape that  is necessary for selective attention in regard to the two goals associated with  shifts of attention and tracking. a previous attempt to model a gaussian shape  in focal-plane analog circuits used many more transistors [12]. while our im-  plementation does not precisely model a gaussian function, it is an acceptable  compromise for the application that we are addressing.  6.7 tracking performance  as stated in section 6.5, the discrete excitation is advantageous for the ap-  plication of tracking a single stimulus within a noisy environment. we tested  this hypothesis with our selective attention circuits by using optical inputs in  a tracking task. the inputs were two leds that were attached to a rotat-  ing disk. the leds were imaged such that they moved across the array of166 neuromorphic systems engineering  convolved excitation  v ~o 40  -0 .=_  â¢ 20  ~.~  -r ~ .  o0 .... ~ .... 10 .... 1 '5 .... 201  position  figure 6.10 experimental data demonstrating convolved excitation results for varying  values of vh, vr = 6.21 v and vdisc = 5.01 v.  phototransistors as the disk rotated. attempts were made to set the leds to  intensity levels that were very close in value, according to measurements from  the phototransistors. the leds were spaced such that their images were lo-  cated approximately 12 pixels apart. the goal of the experiment was to track  the first stimulus once it entered the field of view of the processing array until  the first stimulus exited the array. then the processing array could track the  second stimulus for the remainder of its movement across the array. in order to  demonstrate the need for an attentional mechanism, it was necessary to have  some noise in the system that would cause the selection circuit to jump to the  second stimulus before the first stimulus had left the array.  the illumination from the leds was not entirely consistent as their angle  changed with respect to the imaging array. the mismatch of the phototransis-  tots also contributed to inconsistent measurements for a single stimulus as it  moved across the array. so, when the second led entered the field of view, its  measured intensity level was higher than the measured intensity level of the first  led, and the use of local excitation did not enable the continuous tracking of  the first stimulus. this result is shown in figure 6.11. the voltage levels shownexcitatory feedback circuits 167  3.5 local excitation  o >  v  o  o 3.0  2.5  2.0  1.5  time 10 ms  figure 6.11 experimental data demonstrating results of tracking with local excitation,  vh = 0.77 v.  on the vertical axis of the tracking plot represent positions that are attended  at each moment in time. these values are the output of the position-encoding  circuit. the range of voltage values are between 1.85 v and 3.25 v. each step  is approximately 0.07 v and there are 20 steps, denoting each location in the  array. the plot shown in figure 6.11 was generated by using only local exci-  tation. the value of vh was 0.77 v, thus setting the excitation current, ih, to  51 na. we also measured the output of the normalization circuits (the input  to the selection circuits) when the leds were stationary at the position where  the second led just entered the field of view. the peak current value for the  first led was 83 na and the peak value for the second led was 102 na. as  is evident from the plot, the first led is successfully tracked until the second  led enters the array at the first position. the output of the selection circuit  shifts briefly to the second led before returning to the first led, which has  a measured intensity level that is higher than the second led with the excep-  tion of the distracting locations at the edge of the array. the local excitation,  especially the relatively large value of 51 na, does cause a few of the positions  to be skipped as the hysteresis holds the winning position even as the stimulus  moves to the next position.168 neuromorphic systems engineering  3.5 discrete (nearest-neighb0r) excitation  o >  v  o  o 3.0  2.5  2.0  1.5  time 10 ms  figure 6.12 experimental data demonstrating results of tracking with discrete excitation,  vdisc = 5.00 v and v h :- 0.77 v.  in figure 6.12 a plot demonstrating the output of the attentive processing  array is shown, where the only difference from the previous plot is that the  discrete excitation is enabled with vd~sc set to a value of 5.00 v. the output  demonstrates successful tracking of the first stimulus even when there is a  distracting input within the field of view. also evident in the data shown, is  the ability of the system to continuously follow the stimulus; the winner-take-  all output steps sequentially from one position to the next, without skipping  any of the positions. thus, all twenty array positions are represented in the  data. the desired tracking behavior could also be achieved with a smaller level  of excitation, so long as the distribution of excitation is able to overcome the  difference between the presently attended stimulus and the distracting stimulus.  this system has the flexibility of setting numerous parameters to achieve the  desired relationships between excitation and input values.  in comparison, we ran the same test using the resistive-spreading excitation  while the discrete excitation was turned off. the value of vr was set to 6.21 v.  the inputs were exactly the same as in the previous experiment. the plot  shown in figure 6.13 demonstrates the output of the system when vh is set to  0.81 v. the system is not able to overcome the brief high-intensity level of theexcitatory feedback circuits 169  3.5 resistive-spreading excitation  o >  v  c'-  o  .b  o 3.0  2.5  2.0 ~  1.5  time 10 ms  figure 6.13 experimental data demonstrating results of tracking with resistive-spreading  excitation, vr = 0.21 v and tl/'h = 0.81 v.  second led as it enters the array. so, it is necessary to adjust the parameters  such that the excitation can overcome the deficit at the neighboring locations  of the winner as the tracked stimulus moves across the array. the adjustment  of vr would not enable continuous tracking of the first stimulus, because, as  the spreading increases, the excitation at the neighboring locations would not  increase with respect to more distal locations. the distribution would become  much flatter. the difference between the excitation at the neighboring loca-  tions and the excitation at all other locations is the key to successful tracking.  therefore, we adjusted the value of vh to increase the overall excitation levels,  without changing the shape of the distribution. we were able to overcome the  effect of the distracting input for successful tracking of the first stimulus, as  shown in figure 6.14. the lowest value of vh that produced this result was  0.92 v, and for this combination of parameter settings, ino was measured to  be 114 na. the problem with this approach is evident in the graph. the in-  creased overall level of excitation created similar behavior as that of the local  excitation, where the movement of the selected position did not progress con-  tinuously across the array. also, because the excitation currents were so high,  the second stimulus was not able to overcome the base hysteretic current level.170 neuromorphic systems engineering  3.5 resistive-spreading excitation  o >  v  o  .m  o 3.0  2.5  2.0  1.5  time 10 ms  figure 6.14 experimental data demonstrating results of tracking with resistive-spreading  excitation, vr = 6.21 v and vh = 0.92 v.  when the first led exited the field of view, the output of the winner-take-all  remained at the edge of the array instead of tracking the single stimulus that  was still within the field of view. while there exist input scenarios for which  the resistive-spreading excitation would be able to track a moving stimulus  successfully, we feel that the discrete excitation circuit provides a more robust  approach to continuous tracking. the data presented here serves as an example  of the performance of these systems.  we also took measurements in an attempt to determine the timing limita-  tions of these circuits for tracking a fast stimulus. it is possible that, if the  selection circuits could not switch quickly enough to match the stimulus move-  ment, the discrete excitation would not necessarily perform better than the  resistive-spreading excitation in the tracking task. the discrete excitation re-  lies on continuous movement from one pixel to the next. if the input currents,  or the selection circuit output currents, were not able to keep up with the  movement of the stimulus, the stimulus might be forced to compete with other  stimuli without the advantage provided by the excitation. we used similar  inputs as in the previous tracking tests. only the discrete excitation was en-  abled. the highest measured speed of tracking was beyond 1500 pixels/second.excitatory feedback circuits 171  limitations in the testing setup prohibited further increases in speed for the  stimulus movement. nevertheless, these measurements indicate that the circuit  time constants will not be a concern for real-world tracking problems.  6.8 conclusion  we have presented several circuits that constitute essential building blocks for  an analog vlsi selective attention system. the selection of a region of interest  must allow for stimulus movement with respect to the imaging array, and the  shifts of attention from one object to the next should demonstrate a prefer-  ence to stimuli that are within a proximal location of the presently attended  stimulus. the distributed excitation, centered at the selected location in a  winner-take-all computation, achieves the desired performance. the shape of  the excitation determines the efficacy of the attentional processing, for model-  ing shifts of attention and for tracking a stimulus as it moves across the visual  field. four different excitation circuits were presented. the local excitation  circuit is a building block upon which the remaining three circuits were built.  the resistive-spreading circuit is advantageous in its compact implementation  and the adaptability of the distribution of excitation to different spatial ex-  tents. the discrete excitation circuit demonstrates better performance for the  tracking task, because of the low-attenuation communication to the nearest  neighbors. the discrete excitation does not have the capability of adapting  the spatial extent of the distribution of excitation, however, which nmkes this  circuit a poor model for shifts of attention to new objects.  the combination of the resistive-spreading excitation and the discrete excita-  tion provides a solution that addresses the problems associated with modeling  shifts of attention and attentive tracking. the convolution of the exponen-  tially decaying signal from the resistive network and the three-pixel-wide pulse  from the discrete mirroring provides a shape that mimics the characteristics of a  gaussian distribution. the convolved excitation provides a feasible analog vlsi  implementation that performs favorably as a model for selective attention. the  ability to change several parameters also offers flexibility in defining the shape  of the excitation. all of the circuits that we have presented can be extended to  a two-dimensional architecture. we are currently designing a two-dimensional  selective attention system that utilizes these excitation circuits.  the larger scope of our work includes the modeling of other aspects of selec-  tive attention. inhibition-of-return is a characteristic of visual attention that  causes a location to be inhibited once it is attended, until a brief amount of time  has elapsed. we have already built systems that combine excitation circuits  with inhibition circuits to create complex behavior in analog vlsi visual at-  tention systems [18]. the use of excitation and inhibition together in a visual  attention system creates an automatic scanning mechanism as the attention  moves from one location of the visual field to the next. a variety of behaviors  can be achieved by adjusting the extent of the excitation and inhibition, or  by adjusting the timing of the inhibition. the excitation circuits presented in1"/2 neuromorphic systems engineering  this paper will enhance the performance of such systems, and the use of dif-  ferent distributions will offer interesting alternatives in the implementation of  inhibition. we are also beginning to address the issues surrounding compu-  tation based on objects within the saliency map, as opposed to a pixel-based  computation. the goal is to develop a hardware model, operating in real time,  that we can compare to the behavior of the human visual attention system, as  observed in psychophysical experiments.  acknowledgments  the authors would like to acknowledge david klein for his work with the analysis and  simulation of current-mode resistive networks in support of this project. we would  also like to thank tim horiuchi for countless technical discussions and for his careful  editing of this document. this research is supported in part by the georgia tech  research institute.  references  [1] y. aloimonos. active perception. lawrence erlbaum associates, hillsdale,  new jersey, 1993.  [2] j. r. bergen and b. julesz. parallel versus serial processing in rapid  pattern discrimination. nature, 303, june 1983.  [3] k. a. boahen and a. g. andreou. a contrast sensitive silicon retina with  reciprocal synapses. advances in neural information processing systems,  4:764-772, 1992.  [4] c. l. colby. the neuroanatomy and neurophysiology of attention. journal  of child neurology, 6:90-118, 1991.  [5] t. delbriick. silicon retina with correlation-based velocity-tuned pixels.  ieee transactions on neural networks, 4(3):529-541, may 1993.  [6] s. p. deweerth. converting spatially encoded sensory information to mo-  tor signals using analog vlsi circuits. autonomous robots, 2:93-104, 1995.  [7] s. p. deweerth and t. g. morris. analog vlsi circuits for primitive  sensory attention. in proceedings of the ieee international symposium  on circuits and systems, volume 6, pages 507-510, 1994.  [8] s. p. deweerth and t. g. morris. cmos current mode winner-take-all  circuit with distributed hysteresis. electronics letters, 31(13):1051-1053,  1995.  [9] b. gilbert. a monolithic 16-channel analog array normalizer. ieee jour-  nal of solid state circuits, sc-19(6):956-963, december 1984.  [10] j. g. harris. analog models for early vision. phd thesis, california  institute of technology, 1991.  [11] t. k. horiuchi, b. bishofberger, and c. koch. an analog vlsi saccadic eye  movement system. in cowan, tesauro, and alspector, editors, advances inexcitatory feedback circuits 173  neural information processing systems 6, pages 582-589, san francisco,  1994. morgan kaufman.  [12] h. kobayashi, j. l. white, and a. a. abidi. an active resistor network  for gaussian filtering of images. [eee journal of solid-state circuits,  26(5):738-748, may 1991.  [13] c. koch and s. ullman. shifts in selective visual attention: towards the  underlying neural circuitry. human neurobiology, 4:219-227, 1985.  [141 n. kumar, p. o. pouliquen, and a. g. andreou. device mismatch limita-  tions on the performance of an associative memory system. in proceedings  of the 36 th midwest symposium on circuits and systems, volume 1, pages  570-573, 1993.  [15] j. lazzaro, s. ryckebusch, m. a. mahowald, and c. a. mead. winner-  take-all networks of o(n) complexity. technical report cs-tr-88-21, com-  puter science department, california institute of technology, pasadena,  ca, 1989.  [16] m. mahowald. vlsi analogs of neuronal visual processing: a synthesis of  form and function. computation and neural systems, california institute  of technology, 1992.  [17] c. a. mead. analog vlsi and neural systems. addison-wesley, reading,  ma, 1989.  [18] t. g. morris and s. p. deweerth. analog vlsi circuits for covert attem  tional shifts. in proceedings of the fifth international conference on mi-  croelectronics for neural networks and fuzzy systems, pages 30-37, lau-  sanne, switzerland, february 1996. ieee computer society press.  [19] t. g. morris, d. m. wilson, and s. p. deweerth. analog vlsi circuits  for manufacturing inspection. in proceedings of the 16th conference on  advanced research in vlsi, pages 241-255, los alamitos, ca, 1995. ieee  computer society press.  [20] e. niebur and c. koch. a model for the neuronal implementation of  selectiave visual attention based on temporal correlation among neurons.  journal of computational neuroscience, 1:141-158, 1994.  [21] e. niebur and c. koch. modeling the 'where' visual pathway. in proceed-  ings of 2nd joint symposium on neural computation, volume 5. caltech-  ucsd institute for neural computation, 1995.  [22] b. olshausen, c. anderson, and d. van essen. a neural model of visual  attention and invariant pattern recognition. cns memo 18, august 6  1992.  [23] m. i. posner and s. e. petersen. the attention system of the human brain.  annual review of neuroscience, 13:25-42, 1990.  [24] r. w. remington and l. pierce. moving attention: evidence for time-  invariant shifts of visual selection attention. perception ~ psychophysics,  35:393-399, 1984.174 neuromorphic systems engineering  [25] g. sperling and e. weichselgartner. episodic theory of the dynamics of  spatial attention. psychological review, 3:503 532, 1995.  [26] e. vittoz and x. arreguit. linear networks based on transistors. elec-  tronics letters, 29(3):297-299, february 1993.  [27] s. yantis. control of visual attention. in h. pashler, editor, control of  visual attention. university college press, london, in press.7 floating-gate circuits for  adaptation of saccadic eye  movement accuracy  timothy k. horiuchi and christof koch  computation and neural systems program,  california institute of technology,  pasadena, ca 91125  timmer@klab.caltech.edu  7.1 introduction  the most common eye movements in primates are the quick reorienting move-  ments known as saccades. our eyes often reach speeds up to 750 degs/s during  a saccade which severely impairs our visual acuity. it is therefore important  to minimize the time during which the eyes are moving. while typical human  saccades have a duration of 40ms to 150 ms, changes in the optics, the oculo-  motor plant, or the underlying neural circuitry can cause deficits which delay  optimal viewing conditions.  there have been many types of adaptation behavior identified in the primate  oculomotor system in response to different induced deficits. for example, op-  tican and robinson [11] showed that weakening of the horizontal recti muscles  in the rhesus monkey initially caused saccades which fell short and exhibited  post-saccadic drift of the eyeball. recovery from this type of damage, which  affects all saccades in a given direction, requires about 3-5 days. in contrast to  this long adaptation period, which involves hundreds of thousands of saccades,  experiments where saccadic targets are moved a short distance during the sac-  cade, require only hundreds of trials for the adaptation to reach steady-state.  this type of visually-induced modification of saccade amplitude is known as  short-term adaptation. experiments by frens and van opstal [6] show this  adaptation to be confined to a limited range of saccade vectors around the  adaptation target.176 neuromorphic systems engineering  our laboratory is involved in building a hardware model of the primate  oculomotor system [9] using analog vlsi circuitry. the model oculomotor  plant simulates linear dynamics and provides a good foundation upon which  to build biologically-realistic eye movement systems. the current system can  be triggered using both visual and auditory stimuli [8] and begins to model  the convergence of multi-modal spatial information at the level of the superior  colliculus. three aspects of this modelling system that have become important  are the compensation for non-linearities, the need for self-calibration, and on-  chip storage of these parameters.  the intermingling of memory and computation is an important and pow-  erful aspect of neural architectures which has not yet been well exploited in  neuromorphic vlsi designs. smaller designs have been manageable by the use  of external sources of parameters or by array structures which share global  parameters. with the advent of large, multi-chip, neural systems, however,  the automatic selection, storage, and maintenance of these parameters will  become an unavoidable issue as it is in biological systems. the majority of  circuit designs which have attempted to use on-chip storage of parameters have  used digital ram or externally-refreshed, capacitive storage, both of which are  generally bulky and low-precision. until recently, the use of floating-gates (a  mos transistor gate completely isolated from the circuit by silicon dioxide)  required the use of ultra-violet radiation or bidirectional tunneling processes  which have also been fraught with difficulties, impeding their widespread use.  the development of a complementary strategy of tunneling and hot-electron  injection [28] in a commercially-available bicmos process has alleviated some  of these difficulties.  in previous work, we demonstrated the use of floating-gate devices in a model  of the saccadic burst generator to reduce post-saccadic drift using visual motion  as an error signal [10]. in this paper we present a chip which uses floating-gate  structures to store a mapping of retinal position to motor command voltage.  in the beginning of the next section we will discuss the architecture of the chip,  in section 7.3 we present the circuits and their behavior, and in section 7.4 we  discuss the chip's performance within the training system.  7.2 vector-specific adaptation  in an experiment where human subjects are performing saccades from a fix-  ation point to a visible target, if the target of a specific retinotopic position  consistently moves to a new location during the saccade, l~rens and van opstal  (1994) have shown results indicating that the adaptation timecourse to learn  the offset is short (requiring only a few hundred presentations) and that the  adaptation is confined to a limited range of saccade vectors around the tar-  get [6]. this type of learning can be explained by a mapping similar to that of  a look-up table.  in the previous implementation of our analog vlsi-based saccadic sys-  tem [9], visual stimuli were mapped linearly from pixel position to motor com-saccadic eye movement 177  a centroid  output  voltage  external  training  input  figure "/.1 system block diagram: this chip consists of an array of 32 pixels which consist  of an adaptive photoreceptor (p), a temporal derivative circuit (td), a centroid circuit (c),  a floating-gate circuit (fg) which provides reference voltages to the centroid circuit, and a  control circuit (u/d = "up/down") for training the floating-gate.  mand in a functional model of the deep layers of superior colliculus. any  non-linearities in the optics, photoreceptor triggering circuit, burst generator,  or motor plant would create errors in proper programming of the saccade. we  have modified the visual-triggering circuit [9] (also in figure 7.2) to use the  output of a floating-gate circuit to determine the proper motor command for  each pixel.  as shown in figure 7.1, an array of adaptive photoreceptor circuits (p) are  used to drive a temporal-derivative circuit (td), activating regions in an image  where the intensity is changing. these temporal derivative signals trigger three  circuits: one which activates a slowly decaying memory of which units have been  active (u/d), another which drives a centroid circuit (c) to map the pixel's  position to a motor command voltage, and finally a triggering circuit which  compares the total activity on the chip to a threshold (not shown). the trigger  circuit provides an output signal from the chip, indicating that something has  occurred in the image and that the centroid output information is "valid". the  centroid circuits [4] require reference voltages (motor command voltages) at178 neuromorphic systems engineering  photo  bias  ~-- 1:6 ~ ~,  ~ i bump i rip  _ low,ass -  )hoto to up/down control circuit  & triggering circuit  motor command  voltage (from floating-gate  circuit)  centroid output voltage  (common to all pixels)  figure 7.2 temporal triggering circuit (p+td+c): on the far left, the adaptive pho-  toreceptor circuit amplifies temporal change in the light intensity while slowly adapting to  the mean light level. the temporal-derivative (td) circuit acts as a high-pass filter by mea-  suring the difference in voltage between the original photoreceptor value and a low-passed  version of it. the signal is then full-wave rectified and mirrored to the u/d, centroid, and  thresholding circuits. the centroid circuit (on the right), operates as a follower powered by  the current from the temporal derivative circuit. the motor command reference voltage is  received from the floating-gate amplifier circuit (fg).  each pixel which represent the saccade vector required to center the stimulus  on the center of the array.  in previous versions of this visually-based, triggering circuit [9], the motor  command voltages were provided by a resistive line running across the array.  each end of the resistive line was held at a different voltage, providing each pixel  in the array with a unique voltage reference, which changed linearly across the  array. in contrast, the pixels in this new system are provided with the output  voltage of a floating-gate circuit, each of which can, in principle, be set to  arbitrary values, making it similar to a programmable, look-up table.  the training input to the system is a global signal indicating whether the  system's output was too high or too low. pixel locations which contributed  to the output remain active for a short amount of time (about 3 sec) via the  u/d circuit. when the training signal becomes active, after evaluating the  centroid output voltage, only those units which contributed to the output are  trained in the appropriate direction. since the triggering stimuli may activate a  neighborhood of pixels, the learning is similar to kohonen's stochastic learning  algorithm where the topology of the network is preserved by training a node  and its neighborhood at the same time. this technique has been explored in  software in the context of saccadic learning by both ritter et al. [13] and by  rao and ballard [12].  the training system consists of a workstation which flashes visual stimuli  (bars) at different locations on its monitor. the chip, with a lens, is positioned  to image the stimuli on its photoreceptor array. the centroid output voltage is  measured after each flash using a gpib-equipped (general purpose interfacesaccadic eye movement 179  bus) oscilloscope. each stimulus position on the monitor is assigned a target  centroid output value. if the measured value is lower than the target value, the  training input voltage, (driven by a gpib-equipped voltage source) is lowered  to a pre-determined training voltage for a fixed amount of time to increase that  stored value by increasing the tunneling rate. similarly, if the measured value  is higher than the target value, the training input is raised. after repeated  trials, a target function can be learned to a level of accuracy limited primarily  by the system noise.  7.3 circuits  the implementation of the architecture described above was fabricated on a  tinychip (2.25mm x 2.22mm) using a 2.0 #m, n-well, double-poly, bicmos  process. the chip we discuss in this paper is a one-dimensional array of 32  pixel elements.  figure 7.2 shows the combined circuit schematics for the adaptive photore-  ceptor (p) (left), the temporal-derivative (td) (middle), and the centroid cir-  cuit (c) (right). the adaptive photoreceptor [2] is a high-gain photoreceptor  circuit which slowly adapts to the average light level to prevent saturation. the  temporal derivative circuit combines a lowpass filter with a "bump" circuit [1]  to signal the absolute-value of the temporal-derivative. the centroid circuit [4]  computes the weighted-average, motor command voltage. since every cell in  the array would connect to an n-type mirror, the gray box in the figure denotes  the use of a single, common mirror on the edge of the array to reduce capaci-  tance on the output node. an amplifying ratio of 6 to 1 was used on the mirror  for inverting the bump current to cancel the tail currents of the differential  pair. overall, these circuits map the retinotopic location of temporal change  to a motor command voltage.  the floating-gate circuit (figure 7.3), is a modification of the circuit used  by hasler et al. [28] to train a 2x2 array of floating-gate synaptic elements.  a tunneling process is used to remove electrons from the floating node and a  hot-electron injection process is used to put electrons onto the floating node.  the tunneling current is controlled by manipulating the difference in voltage  between the floating-node and the high-voltage tunneling line. larger voltage  differences produce larger tunneling rates. injection of electrons is performed in  an n-type transistor fabricated in the pbase layer provided for the construction  of bipolar transistors. the threshold voltage for this type of transistor is near  6 volts, which allows the gate to capture high-energy electrons flowing through  the drain while the transistor is still operating in the subthreshold. since the  injection current is the product of the injection efficiency (controlled by the  drain voltage) and the source current, injection current can be adjusted by  manipulating the source current in the pbase transistor.  the floating-gate circuit (figure 7.3) uses two pbase transistors, one used  as an electron injector (pb1) and the other used as the current source for  the amplifier (pb2). since pb2 is only setting the amplifier current (and not180 neuromorphic systems engineering  up/down  control input  vdl __  pbase ~  transistor  vsl  pb1 cascode ~.~  reference  n1  vd2  tunneling  voltage (~26 v) pb2  pbase  __ transistor amplifier  upper limit  voutput  >  amplifier  bias  amplifier  lower limit  figure 7.3 floating-gate amplifier circuit (fg): the floating node defines a subthreshold  current in transistor pb2 which is mirrored and used in a high-gain amplifier stage which  has variable output limits. cascode transistor n1 defines pb2's drain voltage to prevent  hot-electron injection. nodes vdl, vsl, and the high-voltage tunneling node are fixed  global values which define an equilibrium floating-gate value, and a decay rate towards this  value. modification of the floating-gate voltage is performed by capacitively moving the  floating-gate up or down transiently to either increase injection or increase tunneling.  injecting), its drain voltage vd2 can be set to a low voltage allowing the upper  limit of the amplifier's output range to be fairly large. modification of the  floating-gate charge is performed by transiently increasing the rate of either the  tunneling or injection. this is performed by capacitively raising or lowering the  floating-gate using the up/down control input. raising the floating-node both  increases the source current in pb1 and reduces the floating-gate to tunneling  voltage. likewise, lowering the floating-node both increases the floating-node  to tunneling voltage and decreases the source current in pb1.  as in the system described by hasler et al. [28], the tunneling and hot-  electron injection currents are both active, but extremely low and in opposite  directions. since both processes operate in a negative-feedback fashion (e.g.  the tunneling process raises the floating-gate which tends to reduce the rate of  tunneling), the system reaches an equilibrium value when the tunneling current  equals the injection current. when the floating-gate voltage is larger than the  equilibrium voltage, the hot-electron injection current dominates the tunneling  current and the floating-gate voltage drops. conversely, when the floating-gate  voltage is lower than the equilibrium voltage, tunneling dominates and the  voltage rises.  while this technique avoids high-voltage switching circuits, it suffers (or  possibly benefits) from the eventual loss of stored information as the floating-saccadic eye movement 181  floating-gate  center  reference  weakbias _  external ----_  training  input voltage updown  control  "~  strong  bias p11  from  temporal  derivative  mirror  ~ ~_1-~ leak  bias  __ __  figure 7.4 up/down learning control circuit (u/d): this circuit consists of two competing  followers, a weak follower carrying the center reference voltage and a stronger follower which  receives the training voltage from off-chip. when a given pixel in the array generates a pulse  of current in the td circuit, this current is mirrored onto transistor pi, charging the capacitor  node up towards vdd. a small leak current discharges the capacitor slowly. this node acts  as a switch to turn on the strong ampifier to drive the floating-gate control node towards  the globally-received, training voltage. in this fashion, only those circuits which participated  in generating the output centroid voltage receive the training signal.  gate decays back to its equilibrium voltage. this decay rate, however, can be set  to be extremely slow by using a low vdl (transistor pb1) and a low tunneling  voltage. since the tunneling and injection parameters are kept constant, the  equilibrium voltage should not depend on the stored value and the memory  should decay towards an equilibrium determined solely by these parameters.  memory decay tests of our floating-gates exhibited extremely low, tunneling-  dominant rates (less than 0.07 mv/hour), while the injection-dominant rates  showed a decay of about 1.0 mv/hour. for more details of the physics of these  floating-gate devices, see hasler et al. [28] and diorio et al. [5]  the "learning" can also be turned off by bringing vdl, vsl, and the tun-  neling voltage down to zero. unfortunately, the absolute voltage level of all  the floating-gates will be dc-shifted downwards as the tunneling voltage drops  due to capacitive coupling. this shift can easily be countered by increasing the  u/d circuit's center reference voltage until the values have returned to their  trained state. this step, however, may introduce a dc shift error since it is  done manually.  to train the chip for a certain mapping, pixels are stimulated and the resul-  tant centroid output voltage is determined to be either too high, too low, or  inside a window of tolerance around the target value. since the pixels which  contributed to the output value are the ones that need to be modified, some182 neuromorphic systems engineering  photoreceptor ~  trigger output voltage  (digital)  -0.01 i i i i i i i i i i i  0.00 0.01 0.02 0.03 0.04 0.05 0.06 0,07 0.08 0.09 0.10  time (sec)  figure 7.5 top trace: photoreceptor voltage, middle trace: centroid output voltage  (analog), bottom trace: trigger signal (digital). the photoreceptor output voltage jumps  from 0.96 volts to 1.30 volts during the flash of the stimulus. the oscillation riding on the  step response of the photoreceptor is due to the flicker induced by the monitor. the centroid  circuit also shows some o0 hz noise, resulting from feed-through of noise from the high-gain  floating-gate circuits.  mechanism is required to remember those pixels. the up/down circuit shown  in figure 7.4 performs this function by storing charge at each pixel location that  contributed to the centroid output. if the pixel has not been active, the circuit  holds the output to a global reference voltage. if the pixel was just used to  drive the centroid output, the u/d circuit drives the output to an externally-  provided voltage level for approximately five seconds (with our current leak  settings). this external signal is the training voltage which is used to increase  or decrease the floating-gate voltages at those locations which contributed to  the previous output.  figure 7.5 shows some of the relevant signals during a pulse of light on  the array. although not visible, the centroid output rises to a stable value  approximately 2ms after the beginning of the temporal change.  the data presented in this paper was taken using a tunneling voltage of  about 26 volts, vdl = 3.1 volts~ vsl = 0.2 volts with the floating-gate values  centered around 5.5 volts. the up/down control line was moved from 4.0  to 7.0 volts for increased hot-electron injection and from 4.0 to 0.0 volts for  increased tunneling. the coupling coefficient between the u/d control linesaccadic eye movement 183  4  ~" 2  z 0 ~ 0 ~1  m 4  ~ 2  o  <  .~ 4  ~ ~-  > 0- <  -2  3 0 0 cbar width  1 line-w~dth  bar width  = 3 line-widths  o o  o ~ = 5 line-widths --e  4 5 6 7 8 9 l0  pixel location on imager  figure 7.6 when a bar of one line-width (defined by the graphics board) is flashed at  the chip, it stimulates a single photoreceptor as shown in the top plot and the one pixel is  trained for a mean duration of 2.75 seconds. this timing is primarily determined by the leak  bias (see figure 7.4). when the bar is widened to 3 line-widths (middle plot), 2 adjacent  pixels are stimulated and they are trained together in the same direction. a bar width of  5 line-widths stimlates 3 pixels as shown in the bottom plot. in the multi-scale training  regime, all three types of bars were used randomly interleaved in the training set. the bar  of 5 line-widths was also used to generate figure 7.10. these plots show the results of  measuring the mean time each pixel spent training for bars of different widths flashed at a  position on the monitor near pixel ~7. the mean was computed over 7 trials.  and the floating-gate was measured to be about 0.6. in order to scan off the  floating-gate values, we operated the chip using a vdd of 8 volts.  7.4 system performance  in training, the chip is aimed at a computer monitor which flashes vertical bars  at different positions in the field of view. while the current chip has only 32  pixels, the training system flashes stimuli at the maximum line resolution of the  screen. our current optics configuration allows for approximately 75 different  locations at which we can stimulate the array of 32 pixels. this is done both  to map the subpixel behavior as the stimulus moves from one pixel location to  the next and to train the pixels individually rather than as groups of pixels.  in real-world situations, however, the pixels will be activated in groups and  the subsequent output will be an appropriate average of the individual pixel  values. although training the system with large stimuli does work, the training184 neuromorphic systems engineering  0 > 2.505  2.504  2.503  2.502 "flat15f.trg" --  "flat 15f.dat" ~  2.501 -- -  2.499 -- -  2.501  2.5  2.499  2.498  2.497  2.496  2.495  10 20 30 49 50 60 70 80  stimulus position  figure 7.7 flat target function - in this case, all stimulus positions were trained to lie  at 2.500 volts. this plot shows the performance of the chip after approximately 20,000 pre-  sentations spread over 75 positions. the floating-gate outputs were initially spread between  2.4 and 2.6 volts. after training, the centroid array was "queried" sequentially from left  to right five times without training. the error bars represent one standard deviation. the  training procedure continued to modify the floating-gate until the voltage was within i mv  of the target voltage.  time dramatically increases since the training must rely on the uniform statistics  of the training set to sort the proper values out. the training stimulus size also  sets the minimum size for which the array will report the proper value. for  this reason it is important to also train at the appropriate resolution. a multi-  resolution training schedule may be the best strategy since training can occur  in parallel, yet the smaller stimuli can fill in the details at each position. the  training positions are chosen by shuffling a list of positions and selecting them  from the list without replacement. once the list is exhausted, the whole list is  reshuffled. this sets an upper bound on the inter-example training time and  guarantees a uniform distribution.  after training, the array can be "probed" with either a bar of one line-width  or a bar of 5 line-widths to stimulate output values. the one line-width bar will  stimulate individual pixels and the 5 line-width bar will stimulate the average  of a group of 3 pixels. (see figure 7.6) the effects of averaging can be seen  in figure 7.10 for the case of the sinewave mapping, which is a particularly  difficult case to learn, since individual pixels cannot satisfy the wide range of  values occuring on a steep part of the function.e 2.6  2,55  2.5  2.45  2.4 Â¸ saccadic eye movement  ~ ~ ~ ~ ~ ~ ~  "slope1 e.trg" --  "slopele.dat" ~  ~ ~*  ~.~  i i ,'o 20 ; 4o 20 ;  stimulus position 185  80  figure 7.8 linear target function - this function most closely represents a realistic sen-  sorimotor mapping function for triggering saccades to a visual target. the training and  testing procedure is the same as in the previous graph. the error bars represent one stan-  dard deviation.  the first test of system level operation we discuss is an experiment in which  we attempt to load a fiat target function. with this function it is easiest to see  the accuracy with which the system can learn a specific value. figure 7.7 shows  the results after extended training. from initial conditions where the floating:  gate amplifier outputs were sitting at fairly random voltages, the system was  presented approximately 20,000 examples at 75 different stimulus locations (ap-  proximately 625 examples per pixel) and then the system was probed at the  75 stimulus locations to evaluate the mapping. noise in the chip and in the  testing system contribute to the variations seen in repeated trials. it should  be noted that the floating-gate amplifiers are non-linear and the highest gain  occurs in the center of the range. since the target value for the flat function in  figure 7.7 is in the center of the range, we expect the largest reporting variance  here due to noise. the error tolerance of the training system for this mapping  was 1 inv.  the linear target function (figure 7.8) is the mapping which was previously  used to map retinal position to motor command, where 2.60 volts represented  a full-scale saccade to the right and 2.40 volts represented a full-scale saccade  to the left. in this case and in the following mappings, the error tolerance for  learning was 2.5 mv.186 neuromorphic systems engineering  >  0 2.6  2.55  2.5  2.45 "sine5d.trg" --  "sine5d.dat" ~  ~ ~  ~ {  ~ Â¢,Â¢~  ~ ,~  2.4  0 10 20 30 40 70 80  stimulus position  figure 7.9 sinewave target function - in this case, the target values followed a sinewave.  photoreceptor granularity is evident by the "staircasing" seen in the plot. stimulus locations  where the flashed bar occurs on the boundary of two pixels exhibit large variations in output  voltage due to the narrow (one line-width) stimuli being used. figure 7.10 shows the same  pattern being probed with a much wider stimulus (three line-widths). the training and  testing procedure is the same as in the previous graphs. the error bars represent one  standard deviation.  in order to challenge the system we also tested a sinewave target function  (figures 7.9 and 7.10) whose spatial derivative was difficult to match with the  resolution of the current system. the expected final value in this situation  when training with a uniform distribution of examples and balanced step sizes  is the average of the different target values associated with the same pixel.  this behavior is seen most clearly in figure 7.9. convergence of this mapping  function takes much longer due to the statistical nature of the equilibrium and  the final value is not very stable since nearly all the training examples drive  the pixel away from its current value.  during the testing process, we determined that modifications should be made  to reduce the gain of the floating-gate output amplifier. the measured dc gain  from the floating-gate to the output of the amplifier was found to be approxi-  mately 60. this created many problems with noise, particularly at 60 hz due  to electrical noise in the laboratory and the 60 hz light flicker coming from  the monitor. we partially solved this problem by using a considerably smaller  output voltage range (2.4 volts to 2.6 volts) to push the amplifier's output tran-  sistors partially out of saturation for the subthreshold current regime. this hadsaccadic eye movement 187  0 2.6  2.55  2.5  2.45  2.4 i i  "sine5d-b.trg" --  "sine5d-b.dat" ~ _  ~ 2~0 3~0 ~ ~ ~ ~ 10 40 50 60 70 80  stimulus position  figure 7.10 sinewave target function - in this case, the evaluation of the pattern in  figure 7.9 was performed using a bar which spanned 3 pixels. the training and testing  procedure is the same as in the previous graphs. the error bars represent one standard  deviation.  the effect of reducing the gain down to about 2.0, but left a very small signal  range with which to work.  7.5 discussion  we have successfully fabricated and tested a trainable array of floating-gate  memories whose operation and modification is integrally related to a specific  visual task. by storing information locally about which units contributed to a  computation, the distribution of the training signal back through the system  has been made simpler. the hardware approach to this problem of delayed  assignment-of-error may provide a valuable testbed in which to consider how  this problem is solved in biological systems.  the neurobiological substrate for this adaptation is still unknown. both the  superior colliculus and the frontal eye fields are attractive areas for investigation  of this adaptation due to their vector-specific organization for driving saccadic  eye movements. while both areas are capable of driving of saccadic eye move-  ments, the frontal eye fields are implicated in the generation of "volitional"  saccades and the superior colliculus has been implicated in the generation of  reflexive, visually-guided saccades. experiments by deubel [3] indicate that  there are context-dependent differences in vector-specific, short-term adapta-  tion. adaptation performed during reflexive, visually-guided saccades was not188 neuromorphic systems engineering  expressed during volitionally-driven saccades. the converse has also been found  to be true. prens and van opstal [6] also demonstrated the transfer of vector-  specific adaptation to saccades triggered by auditory cues. these experiments  together point to the interpretation that the adaptation is occurring at a stage  after integration of these different sensory modalities, but before the parallel  streams of information from the superior colliculus and frontal eye fields have  converged. following these constraints, it is our hope to also demonstrate this  transfer of adaptation with our vlsi-based auditory localization system.  the investigation of neural information processing architectures in analog  vlsi can provide insight into the issues that biological nervous systems face.  analog vlsi architectures share many of the advantageous properties with  neural systems such as speed, space-efficiency, and lower power consumption.  in addition, analog vlsi must face similar constraints such as real-world noise,  component variability or failure, and interconnection limitations. with the  development of reliable floating-gate circuits, the powerful ability of neural  systems to modify and store their parameters locally can finally be realized in  analog vlsi.  beyond our effort to understand neural systems by building large-scale,  physically-embodied biological models, adaptive analog vlsi sensorimotor sys-  tems can be applied to many commercial and industrial applications involving  self-calibrating actuation systems. in particular, we believe that for real-world  tasks such as mobile robotics or remote sensing, these circuits will be invaluable  for systems trying to keep up with the ever-changing world.  acknowledgments  the authors would like to thank reid harrison for constructing one of the test boards  and generally motivating the completion of this project, paul hasler for valuable  technical advice on the design and analysis of the floating-gate cell, tonia morris  and giacomo indiveri for assistance in the careful editing of this document. t.h. is  supported by an office of naval research aasert grant and by the nsf center for  neuromorphic systems engineering at caltech.  references  [1] t. delbrfick. bump circuits for computing similarity and dissimilarity  of analog voltages. in proc. of intl. joint conf. on neural networks,  volume 1, pages 475-479, 1991.  [2] t. delbrfick. investigations of analog vlsi visual transduction and mo-  tion processing. phd thesis, california institute of technology, 1993.  [3] h. deubel. separate adaptive mechanisms for the control of reactive  and volitional saccadic eye movements. vision res., 35(23/24):3529-3540,  1995.  [4] s. p. deweerth. analog vlsi circuits for stimulus localization and centroid  computation. intl. j. comp. vis., 8(22):191-202, 1992.saccadic eye movement 189  [5] c. diorio, p. hasler, b. a. minch, and c. mead. a high-resolution non-  volatile analog memory cell. in proc. ieee intl. symp. on circuits and  systems, volume 3, pages 2233-2236, 1995.  [6] m. a. frens and a. j. van opstal. transfer of short-term adaptation in  human saccadic eye movements. exp. brain res., 100:293-306, 1994.  [7] p. hasler, c. diorio, b. a. minch, and c. mead. single transistor learning  synapses with long term storage. in ieee intl. syrup. on circuits and  systems, volume 3, pages 1660-1663, 1995.  [8] t. k. horiuchi. an auditory localization and coordinate transform chip.  in advances in neural information processing systems 7, pages 787-794.  mit press, 1995.  [9] t. k. horiuchi, b. bishofberger, and c. koch. an analog vlsi saccadic eye  movement system. in cowan, tesauro, and alspector, editors, advances in  neural information processing systems 6, pages 582-589, san francisco,  1994. morgan kaufman.  [10] t. k. horiuchi and c. koch. analog vlsi circuits for visual motion-based  adaptation of post-saccadic drift. in 5th intl. conf. on microelectronics  for neural networks and fuzzy systems, pages 60-66, los alamitos, ca,  1996. ieee computer society press.  [11] l. m. optican and d. a. robinson. cerebellar-dependent adaptive control  of the primate saccadic system. j. neurophysiol., 44:1058-1076, 1980.  [12] r. rao and d. ballard. learning saccadic eye movements using multiscale  spatial filters. in advances in neural information processing systems 7,  pages 893-900. mit press, 1995.  [13] h. ritter, t. martinetz, and k. schulten. neural computation and self  organizing maps: an introduction. addison-wesley, reading, ma, 1992.iii neuromorphic  communicationintroduction to  neuromorphic communication  tot sverre lande  department of informatics,  university of oslo,  n-0316 oslo, norway  bassenoifi.uio.no  8.1 why neuromorphic communication  the somewhat artificial term "neuromorphic communication" indicates the  aim of transmitting information similar to our neural system. the "spiky"  information coding found in our nerve-fibers seems to be quite inadequate for  microelectronics. with a limited dynamic range of two to three orders of mag-  nitude and poor noise margins, this kind of information coding may look like  a bad choice from an engineering perspective.  there are, however, several interesting properties of the rich neural spiking  representation.  â¢ one of the essential properties of a sequence of neural spikes is the statis-  tical information coding. the average number of spikes carries the value  to be transmitted. the term "mean-rate coding" arises from this inter-  pretation. of course, the mean-rate depends heavily on the time-scale or  integration time, and different time-constants are actively used in neural  computation. a coding frequently found in biology is the conveyance of  the rate-of-change or transmitting the derivative of the signal. this kind  of coding is often called adaptive coding, keeping the circuits within the  usable range of operation. for mean-rate coded variables some operations  like multiplication come easy. both autocorrelation and crosscorrelation  have been reported using simple and-gates as multipliers [4, 5].  â¢ the large number of interconnections in the neural system is overwhelm-  ing and gives rise to a significant amount of parallelism or redundancy.194 neuromorphic systems engineering  nature seems to remedy the limited dynamic range of a single axon  with redundant coding of information in several parallel nerves. in [12]  it is shown that the dynamics of mean-rate coding may be improved  with redundancy. on the other hand redundant coding introduces fault-  tolerance, handling defects in a graceful way. in a technological perspec-  tive the ability to cope with defects may extend the size of implementable  systems in silicon.  the coding of states as spikes is still an analog representation. although  each spike is "digital", the inter-spike interval is still analog. the ampli-  tude of an analog state is mapped to the time-domain and encoded with  the high resolution inherent in an analog state. this analog property is  most likely the fundamental reason for improvements by redundancy and  graceful handling of defects.  this list of interesting properties of neural spike-coding is far from com-  plete, but exposes some of the more essential properties from an engineering  perspective.  the same way as neuromorphic engineering is exploring neural paradigms  for computational systems, neuromorphic communication is exploring the prop-  e~'ties of neural information coding to build up modular systems. the digital  nature of a single spike is inviting to use standard digital communication sys-  tems like data-buses or techniques known from digital communication such as  different pulse modulation techniques. the spiking frequency of a neuron is  less than 1000 spikes/s. with a bus transfer rate of hundreds of mbyte per sec-  ond, a digital bus should be able to transfer the spiking activity of a significant  number of spiking neurons. each neuron is given a unique number, and this  number is transmitted on the bus whenever the neuron is spiking.  8.2 earlier work  one of the early real system implementations using neuromorphic communi-  cation was done by the late misha mahowald [51, 181], implementing a silicon  model of stereoscopic vision. three analog chips are interconnected with asyn-  chronous digital buses and are able to extract depth information in real time  based on visual stimuli from two silicon retinas. this is truly a remarkable  achievement and is the first known analog multichip system using neuromor-  phic communication.  the term address-event-representation, or aer for short, was proposed  by mahowald. the spiking event of a neuron is simply coded as a unique  number on a binary encoded digital bus. this early version was a one-to-one  communication with one sender and one receiver. the system is working in  real time and is fully asynchronous.  another similar effort was done by mortara et. al. [63, 14] based on similar  ideas. in order to understand the different flavors of these approaches, some  more details have to be explained.neuromorphic communication 195  the time-multiplexing of a significant number of unclocked spiking neurons  or free running oscillators will lead to collisions. regardless of our bus-speed,  the possibility of collisions will always be present and concurrent events must be  accounted for. the strategy for collision handling in the work by mahowald is a  full arbitration scheme where spikes are delayed until the bus is free to transmit  the event. in this way, no collisions occur on the bus, but the events may be  delayed (or even lost), depending on the bus-load. the approach adopted by  mortara is the opposite, where events are transmitted immediately, ensuring  virtually no delay but increased collision-rate with increased bus-load.  both strategies have been demonstrated to work, however the full arbitration  scheme by mahowald may tolerate high bus-loads with large volumes of data  typically generated from vision systems, whereas the simple non-arbitrating  solution suggested by mortara is suited for low-volume data typically generated  by cochlea models.  8.3 weak arbitration  one of the essential features of neuromorphic communication is to stay analog  even on a digital bus. in order to understand how the analog state is maintained  the following classification may be useful:  quantized value, quantized time is the characteristics of a full blown  digital system as found in our computers.  continuous value, continuous time is the characteristics of the real world  and a pure analog system.  continuous value, quantized time is usually called a sampled data sys-  tem. switched-capacitor (sc) or switched-current (si) circuits are typical  engineering examples of these kind of systems.  quantized value, continuous time is the method of neuromorphic com-  munication and also found extensively in biology.  from the classification above we see that our analog state is maintained by  the continuous time-scale or asynchronous bus strategy. but time-multiplexing  of events over a digital bus will sometimes delay events or loose events due to  collisions. even with a fast bus some noise is introduced to our analog state.  the question is now what is the best approach: 1) to make sure all events make  it through the bus using full arbitration (with the extra delay introduced), or  2) to minimize timing errors by sending everything to the bus immediately?  the answer to this question is strongly dependent on the expected bus-load  as indicated above. for low bus-loads, the simplicity and quality of mortara's  approach is favorable, but as the accumulated spiking-rate is increasing, errors  due to collisions will degrade performance. at higher data rates, the full ar-  bitration scheme will give better performance, but the penalty is a significant196 neuromorphic systems engineering  timing-error at low data-rates. in order to reduce the arbitration-time, arbitra-  tion circuits are usually designed to do local arbitration implying that neigh-  boring neurons will be preferred in spite of neurons connected further away.  although the arbitration-time is reduced, this scheme introduces unfairness.  the transmission-error of the analog state-variable will depend on where the  neuron is connected to the arbitration-tree. the errors introduced by collisions  are, on the other hand, randomly distributed among all the connected neurons  (provided the spiking outputs from the neurons are stochastically independent).  there is no doubt that whatever strategy is used, time-multiplexing will  introduce errors. the question is what is the best way to trade the errors  due to crowding with the errors due to aging? in recent work abusland [1]  has suggested to utilize a protocol known from data communication. csma or  carrier sense multiple access used for the ethernet available on most computers  is a weak arbitration scheme where collisions are avoided by listening on the  bus and delaying the sending of data until the bus is free. but since computers  have their own independent clock, they may occasionally start transmitting  exactly at the same time, resulting in collisions. in such a case the data-packet  is retransmitted after a random delay. the strategy is said to be persistent,  ensuring that all packets eventually get transmitted. for acceptable loads this  strategy is doing fine as we all know from our ethernet-connected computers.  an early version of ethernet used the aloha protocol where no sensing is  done and the packets are transmitted immediately. in our context the mortara  approach is a variant of the aloha protocol.  8.3.1 o-persistent and 1-persisten~ csma  abusland has modified the csma strategy by simply discarding events when  the bus is busy. this modified strategy is called &persistent csma and is a  significant improvement compared to the aloha approach at higher bus-loads.  another approach known as 1-persistent csma is to latch the event until the  bus is free and transmit once. if two or more events are latched, a collision will  occur and the events will be discarded. the way the carrier-sense function is  implemented is by a shared bus-line, which when asserted indicates a busy bus.  the common busy-line ensures a fair competition for the bus, distributing the  introduced noise randomly between all transmitted analog states.  in figure 8.11 the throughput of different arbitration strategies are plotted  as a function of bus-load. the full arbitration scheme is able to utilize the total  bus capacity, whereas the aloha protocol degrades at low bus-loads. even  at 20% bus load the collision-rate is significant and the loss-rate unacceptable.  both the 0-persistent and 1-persistent protocols are doing pretty well up to  50% bus-load from where the collision-rate is degrading the throughput. the  results presented are consistent with the expected performance of a csma  protocol. the improved performance of the 1-persistent protocol is significant  for lower bus-loads. the solid lines are estimated performance based on models  while the circles are measured results from chips.neuromorphic communication 197  0.8  ~0.6  .- "~  ~ o.4  0.2  0 ~ 0 ! !  Ã aging chip  0 1-persistent ch~p  o o-persistent chip  0.2 Ã Ã:  x theoritical maximum  // i  x x x '! x < : Ã  Ã' ~ 9i3-queue Ã Ã Ã Ã:  Ã ......... ~, .................... ~ :x .....  x ~ Ã  Ã 1.5:aueue_ Ã Ã  Ã â¢ 1-queue  Ã :x x  ...  r ~ ~ .  0.4 0.6 0.8 1,2  normalizedofferedloadg i  aloha  1.4 1.6 1.8 2  figure 8.1 bus throughput for different protocols  spike ramp arbitration  figure 8.2 analog arnbitration with aging  8.3.2 analog arbitration and aging  another strategy proposed by marienborg [13, 10] is to do an analog arbitration  between competing events using the well know winner-take-all circuit or wta  for short [16]. the wta is a fair arbitration scheme based on a shared wire.  the original wta has moderate discrimination properties with the possibility  of allowing events simultaneous bus access. an improved wta with feed-back  [10] improve on the discrimination properties.  the next improvement is to convert each spike to a ramp were the voltage-  level of the ramp encodes the time of arrival (figure 8.2). the analog properties  of the wta-circuit will grant the oldest event access to the bus. recognizing  that older events will contribute with more noise, it may be more efficient to  discard old events than to steal bus bandwidth with outdated events. this  may be done by resetting the ramp when a certain voltage-level (indicating  the age) is reached. with the ramp we have achieved both a fifo (first-198 neuromorphic systems engineering  in-first-out) ordering, but also implemented aging by discarding old events.  with these added features, improved throughput is obtained with acceptable  noise-margins on the transmitted analog states.  in figure 8.1 the throughput of this analog arbitration scheme is plotted  against the other strategies with measurements indicated with x in the figure.  by changing the duration of the ramp different properties appear.  setting the ramp very short means discarding events when the bus is busy  and is identical to the 0-persistent strategy. the Ã-es in figure 8.2 follow  the 0-persistent model as expected.  extending the ramp to latch exactly one event is similar to the 1-  persistent strategy for low bus-loads. for higher bus-loads, the through-  put will not degrade due to the discrimination admitting one of the com-  peting events instead of discarding them both. again, measurements  confirm this behavior where the Ã-es follow the the 1-persitent line at  low bus-loads, but keep increasing instead of decreasing for higher loads.  by increasing the length of the queue, improved throughput is possible.  with an estimated queue of 3.3, the collision-rate is acceptable even at  80% load. again, these measurements confirm the good discrimination  properties of the improved wta-circuit.  these measurements only indicate the utilization of the bus with no indi-  cations of the quality of the transmitted analog state variables. a study of  introduced noise is underway looking for the optimal balance between aging  and crowding. unfortunately, these results are not available at the moment of  writing, but the flexibility of the "analog fifo" makes these studies feasible.  8.4 conclusion  weak arbitration combined with aging is a good trade-off between bus utiliza-  tion and acceptable noise-margins. both theoretical analysis and early mea-  surements indicate the feasibility of weak arbitration, which distributes the  bus-introduced noise randomly between all the transmitted analog states.  in the following three chapters different aspects of neuromorphic communi-  cations will be presented. in the first chapter by mortara, the aloha protocol  is used to convey analog states between a retina-chip and an orientation en-  hancing circuit. in the next chapter, kalayjian explores the analog properties of  the wta-circuit to implement a communication system from a retina chip. fi-  nally, boahen is using the full arbitration scheme for inter-chip communication  of 4096 analog states.  in spite of different solutions, the main objective of transmission of ana-  log states using asynchronous communication is the same in all the chapters.  hopefully the reader will find sufficient material to select a suitable solution.neuromorphic communication 199  notes  1. this figure was kindly provided by jan tore marienborg based on his own measuements  and measurements provided by/~nen abusland  references  [1] /~. abusland, t. lande, and m. h0vin. a vlsi communication architec-  ture for stochastically pulse-encoded analog signals. in proceedings of the  ieee international symposium on circuits and systems, volume 3, pages  401-404, atlanta, ga, 1996.  [2] z. kalayjian, j. waskiewicz, d. yochelson, and a. andreou. asynchronous  sampling of 2d arrays using winner takes all circuits. in proceedings of the  ieee international symposium on circuits and systems, volume 3, pages  393-396, atlanta, ga, 1996.  [3] j. lazzaro, s. ryckebusch, m. a. mahowald, and c. a. mead. winner-  take-all networks of o(n) complexity. in d.s. touretzky, editor, advances  in neural information processing systems, volume 2, pages 703-711, san  mateo- ca, 1989. morgan kaufmann.  [4] j. p. lazzaro and c. mead. silicon models of auditory localization. neural  computation, 1:47-57, 1989.  [5] j. p. lazzaro and c. mead. silicon models of pitch perception. in proc.  natl. acad. sci. usa, volume 86, pages 9597-9601, 1989.  [6] m. mahowald. vlsi analogs of neuronal visual processing: a synthesis of  form and function. computation and neural systems, california institute  of technology, 1992.  [7] m. mahowald. an analog vlsi stereoscopic vision system. kluwer aca-  demic, boston, ma, 1994.  [8] j. marienborg, t. s. lande,/~. absuland, and m. h0vin. an analog ap-  proach to 'neuromorphic' communication. in proceedings of the ieee in-  ternational symposium on circuits and systems, volume 3, pages 397-400,  atlanta, ga, 1996.  [9] j. marienborg, t. s. lande, m. hovin, and/~. absuland. neuromorphic  analog communication. in proceedings of the ieee international con-  ference on neural networks, volume 2, pages 920-925, washington dc,  1996.  [10] a. mortara and e. a. vittoz. a communication architecture tailored for  analog vlsi neural networks: intrinsic performance and limitations. ieee  transactions on neural networks, tnn-5(3):459-466, may 1994.  [11] a. mortara, e. a. vittoz, and p. venier. a communication scheme for  analog vlsi perceptive systems. ieee journal of solid state circuits,  sc-30(6):660-669, june 1995.200 neuromorphic systems engineering  [12] e. j. nayly. spectral analysis of pulse frequency modulation in the nervous  systems. ieee transactions on bio-medical engineering, 15(4):257-265,  october 1968.a pulsed  communication/computation  framework for analog vlsi  perceptive systems  alessandro mortara  swiss center for electronics and microtechnology,  rue jaquet-droz 1,ch-2007 neuch~tel,  switzerland  mortarac)csemne.ch  9.1 introduction  this paper reports on the main properties and some applications of a pulsed  communication system specifically developed for the service of multichip per-  ception schemes realized in analog vlsi. the project started with the goal to  obtain biological-like connectivity among functional subsystems capable of pro-  cessing sensory data in a collective fashion through several hierarchical layers  and through convergence, divergence and fusion of data from different origins.  as a consequence of the thin sheet organization of their biological counter-  parts, the realized subsystems consist almost invariably of several one- or two-  dimensional arrays of cells. the output of every cell, its activity, is relevant to  further processing and should be available for communication to the next layer.  to relax this requirement, the possibility exists of taking at least some ad-  vantage of the particular way data are processed by a neural system. it is  known, for example, that the retina responds faster to the variation of an in-  tense excitation than to the change of a dark area into an even darker one [2]  so that an adapted communication system will tolerate a worse performance  in the communication of weak activities. along the same line, another inter-  esting and ubiquitous property of biological perceptive systems is the tendency  to accentuate and encode variation in the sensorial landscape. the encoding202 neuromorphic systems engineering  of variation is only partially related to the usual communication engineering  data compression methods. the problem in this case is to optimize the bit  rate (in the sense of information theory) on a bandlimited channel by reducing  redundancy in the message. this should be achieved while using the whole  available bandwidth, thereby maximizing energy expense. conversely, as it has  been pointed out [11], the long distance biological communication style reduces  power consumption because no energy is allocated to transmit "useless" infor-  mation: biological systems do not fully use the available bandwidth to save  energy. however, if on the one hand a purely sequential conditioning (such as  a/d conversion or buffering in the case of analog transmission) and scanning  of the analog activities in a large neural network would mean energy waste, the  fully parallel communication architecture of some parts of the nervous system  is not realizable in the vlsi context because of on- and off-chip wiring and  pin limitations. probably, the best way out of this apparent dead end is to  trade the speed of silicon for the connectivity intrinsic to the nervous system.  this is the way we decided to follow from the outset. it has led at first to a  communication architecture, described and theoretically evaluated in [63], then  to a small-scale realization [14] containing all the essential hardware building  blocks. the scheme is now evolving from a means to set up point-to-point  connections between single elements located on different chips to a more gem  eral system where other types of connections can be envisaged (such as, for  instance diverging connections). this paper focuses on recent developments,  possible applications and on the experimental verification of the theoretical  basis of the method.  9.2 principle and theoretical performance of the  communication system  in the proposed system, all cells have access to a common parallel bus. the  cell's activity is proportional to the frequency of the train of short pulses it  emits without any deterministic timing relationship to the spikes generated by  other cells. when a cell emits a pulse, its code appears on the bus for the  duration of the pulse. pulses are decoded by the receiver and directed towards  the proper target cell, which can either simply accumulate the incoming pulses  or also perform other kinds of operations, such as broadcasting them in a region  with programmable boundaries as schematically represented by the "diffusion  network" box in figure 9.1 [4]. this amounts to a spatial convolution with  the kernel defined by the diffusion region. it is the designer's decision to keep  processing at the pulsed representation or to revert to analog voltages and  currents by demodulating the appropriate spike trains.  the proposed scheme is similar to the address-event representation of ma-  howald [51], but does not include arbitration of bus access when two cells  simultaneously emit a pulse (this event is called a collision): in this case the  resulting compound address, obtained by the bitwise or of the colliding codes,  is simply ignored by the receiver circuit. a formal discussion of the respec-excitatory feedback circuits 203  activity-frequency converter  ii ~ ~~-~  , , .:~:]--['-~  q ~ ii i~:~'-~  ~ q ,~:~}---~  i  i vÂ°lts ~ .~' i13 ~ i  code for address 10010 pulse accumulation  o  -  / ~  i~ i l ~  i i~.  i j~  ~ l ~ ~) ~  b ~ ~  ~ision be~een i and ~/ --  figure 9.1 communication system block diagram. pulse transmission is represented on a  5~wires bus,  tive merits and disadvantages of the two approaches appears in [14], but the  napfm scheme (non arbitrated pulse frequency modulation) we propose is  simpler. address-event uses an arbiter to decide which one of a number of  colliding pulses has the right to access the transmission channel and, in some  implementations, to allow a second chance to the losers of this competition.  this very desirable property is offset by increased circuit and signalling com-  plexity in the form of arbitration circuits and request and acknowledge signals  propagating through the body of the circuit. in the alternative we propose,  collision events are detected through coding and ignored (pulse loss). the  encoding hardware consists of just connected wires so that speed and design  simplicity are favored.  hardware simplicity is essential if arbitrary connections between cells of  the the same chip must be set up. this necessity has recently appeared in the  design of a perception-action loop system [6] which uses napfm to connect in a  nontopology-preserving fashion a sensory and a motor map located on the same  chip. in this case since communication stays internal to the chip, bandwidth is  maximum and collision probability is minimal thus napfm communication is  in its best performance range for a given technology. in such a configuration,  every cell must be in a position to decode by itself the bus contents because  connections are arbitrary and no row-column arrangement holds. to carry out  decoding, one gate is sufficient (as shown in section 9.3), and to further reduce  transistor count, one p-type transistor can be used in the pull-up branch of the  gate.  the consequence of collisions is uncertainty over the pulse count coming from  one particular cell. although in biological perceptive systems like cochleas or204 neuromorphic systems engineering  retinas, which respond vigorously and immediately to changes, many events  (start of an action potential) can be considered simultaneous if they occur in  a time window of the order of a millisecond; in a vlsi system such as the one  considered here, simultaneity (in the sense of pulse loss) means events separated  by a time of the order of ten to a hundred nanoseconds. the latter is a rather  conservative figure for the minimum event duration needed to elicit a valid  logic pulse in the receiver. thus, events that are simultaneous for biological  structures are sufficiently separated in time in the collision-prone vlsi context  to be able to notice them and take appropriate action. this is the main reason  for the viability of the non-arbitrated communication scheme we are presenting.  to gain a better understanding of system performance, the results of an  analysis developed in [63] are summarized. the point process "start of a pulse  emission anywhere in the network" is modeled as a poisson process whose rate  is determined by the average activity in the network. this seems to be a  reasonable model for a large population of independently firing cells and, as it  will be shown in section 9.5, it can be experimentally verified.  . ,,.  .  granularity ~~ ~2= 10-1 ~~ collisio~  10 ~ ~  1 10 0.001 0.01 0.1 2 ftot6  figure 9.2 dynamic range-maximum activity frequency relationship. each curve shows  how to choose the best total pulse rate to attain the largest dynamic range (i.e. the smallest  minimum activity detectable with a given tolerable error) the optimum (minimum of the  curves) occurs at slightly different values of normalized frequency but satisfactory behavior,  although suboptimal, is obtained in all cases for 2n~fo6 ~ 0.1  consider a network containing n cells. let 5 be the minimum pulse duration  necessary to generate a suitable pulse at the receiver and oifo the cell's pulse  rate, where 0i is the activity of cell i (0 _< 0i < 1) and f0 is the frequency  corresponding to maximum activity. because of the coding, simultaneous access  of two or more cells to the bus, in the sense specified above, results in the loss  of all colliding pulses. the probability of emission with no collision can be  expressed as [63]:  p = e -2nafÂ°~ (9.1)=  :sp~o~ ~po~ jo ~qmn~ oq~, o:~  :~inmaoj s,~u~iad~ ~u~Â£idd~ pau~mao~op s~ spaomopoa  jo aaqmnu pu~ saa~ snq jo aoqmnu oqa uoa~aoq d~qsuo~a~ioa a~aoadml~ aqÂ£  â¢ opoa oq~ jo ll~i~q~i~as smoil~ qa~qm p~oqaaao olqvadoaa~ u~ 'og jom jo soni~a  oa dn saai~ g u~ql ssa[ o~ poa~m~i s~ Â£au~punp~a a~qa ~no suana ai 'uo ~ou moaj  ,,i~mddo. paiiva oq ium apoa s[qÂ£ "g/m jo aaed aa$oau[ or1 ~ aoj su~a xq  pou~mqo s~ spao~ apoa jo d aaqmnu lsasa~ i aqa 'snq aqa u~ pasn oa~ soa~ m  ji "aapoaap aqa xq poaou$~ Â£[i~ad~moln~ s~ qa~qm ,,sauo,, i + ~ as~ai av qa~  opoa ~ u~ salnsaa sliaa su~p~iioa oaa jo sapoa oqa jo ~0 as~a~q aqa ~,,~souo,,  jo ~ aaqmnu am~s aqa qa~ papoa aa~ sassoapp~ [i v :apoa pa~aojaq$~aas ~  saso$$ns xlaadoad s~qÂ£ "sapoa su~p~iioa oql uo uod~aodo ho poa~ os~ma~q oqa  smaojaod snq oqa 'suodmuomaldm ~ ano u i 'ssaapp~ as[~j ~ ol puodsoaaoa x~m  snq or1 uo ~uasoad apoa oql 'uo~s~iioa ~ su~anp anq ~saslnd ~u~p~iioo w jo ssoi  oqa s~ suo~s~iloa jo aao~o *iuo aqa j~ pioq uodaas ~uspaaaad aqa jo sainsoa oqÂ£  dniciod ss~i~io0~ Â£'6  "snq oq~. uo alq~it.~a~ q~,p!~pu~q oq:~ jo %0i :lnoqv aq  plnoqs oa~a aslnd imo~ ~q~ :suo~s~iloa pu~ ~a~inu~s~ jo sms~u~qaam ~ud~m~i  -oau~mao$aod oma oql uoomaaq oauvl~q [~mddo oql sutssoadxo alnsoa oa~a~a~a  -u~nb aqa s~ (u6) uo~a~iah 'aa~mpa~q oldm~s Â£q 1no pa~aa~a aq u~a a~ osn~aoq  o[q~adaaa~ s~ sad~a~la~ jo uo~a~z~i~maou j~ ioaauoa oa Â£1du~nb i~qoi~ auo~uoa  -uoo Â£aoa ~ g~ ~ '.~aom~ou oq~ xq poaa~ma saslnd jo aaua i~qoi ~ aqa s! ,o,f oaoqm  v0 ~ 9~o~fg = pofvng  :dt.qsuot.:~eloa oq~ ~ut.oaojuo Â£q pout.mqo oq uuo  o~uu~ 3!tu~uÂ£p mnmtx'~m ~q:~ osi~ s~oqs u6 oan~h "Â°fg~n~ o~a oslnd i~o~  pvziium~ou oq~ pu~ ~ uoai~ ~ q~im poaaosqo (9n~/Â£)(~/~0) = ~ ~[ad3v  (poz!iumaou) mnmiuim ~q~ uoo~oq d!qsuo!~ulaa oq~ smoqs ~q~ u6 oan~h ut  oiqisia aau suod~na!s oma aq~ '[oao[ aoaaa ~oai~ ~ ~ olq~aaosqo ~0 xa!ado~  mnm!u!m aqa Â£q asu~a a!meukp pu~ aunoa aslnd oqa u~ (o~aua u~om-oa-uodu!aa p  paepu~as oq~ jo aaunbs aq~) :s aoaaa oa[a~[oa oqa xq poq!aaso p aq uva ~s!o n  â¢ ostou os~oaaii! suo!s!iio~ oaoqm uod~na!s oawa q$!q ~ oa (xa!a~inu~a~) u!uaa asind  pa~ima oq~ jo po:aad ~q~ o~ poa~dmoa ~aoqs s~ am d uo[~aa~sqo oq~ ~a~qm  uo~n~s o]~a ~o[ ~ mo~j oaom om '0~ Â£~a~o~ mnm~iu oq~ '0f ~uis~oaau: x~  â¢ sms!uvqaom auoao~p oma Â£q posn~a ~ lunoa oslnd oqa aoao (os!ou) Â£au!~aoaun  u~ aoaomoq sainsoa om d uo~l~a~osqo poa~m!i oqÂ£ "auva~!u~s oa~ son[~a oad~ioa  Â£[uo o.taqm suod~a!ldd~ 1sore u~ oauo~uoauoau! ou s~ qa~qm 'd aola~j ~ Â£q poanpoa  avodd~ oa Â£1~ada~ Â£aoao osn~a suo!s~iio d .(ofÂ£)/,~ ql~m Â£a!ada~ paaaosqo oqa  soa~m~aso pu~ Â£ om~a uod~aaosqo oqa u! ~ iloa moaj sutmoa sosind ~ oq1 saunoa  iiaa aosa~a oq~ "~aomaou oqa jo ka!a~aa~ o~aoa~ oqa s~ ~sxn/i = ~ oaoqm  ~0g sÂ£i~di:iid >idv~t(ix~i ai:ioÂ£Â¥Â±idx~206 neuromorphic systems engineering  which shows a mildly less-than-exponential growth of number of code words  with bus width. we now turn to additional practical properties of the code.  bus  l  iii  figure 9.3 logic to decode address 11000, in a 5-wire bus example. the left and right  gates are equivalent but the right one is simpler and the nor gate can be implemented  with just a p-type transistor in the pull-up branch.  cells are addressed by monitoring and decoding the configuration of bits on  the napfm bus. an example of the decoding logic is given in figure 9.3 for  the code 11000. note that the and (or nor, depending on the chosen config-  uration) gate needs only 4 inputs; because of the way the code is constructed  and because no additional zero can be produced by a collision, a codeword is  completely characterized by the position of its "zeros". a check for a "one" in  any remaining wire is enough to determine the presence of an event. thus, for  this family of codes, the decoder logic needs only one more input as there are  "zeros" in the code.  9.4 a silicon retina equipped with the napfm  communication system  as an application of the napfm communication architecture, this section  presents a silicon retina that can be used as the first layer of a biomimetic  vision system. the retina has been successfully interfaced with another analog  chip, also equipped with napfm communication, implementing a cortical layer  for orientation enhancement. the chip is basically an array of hexagonally  arranged cells interconnected in such a way that the spatial high-pass filtering  of the image projected on the chip surface can be carried out and the results  of the computation passed over to the next layer using the napfm technique.  every cell contains part of a normalizer and a pulse generator connected to a  parallel bus in exactly the same way as described in [14] and [1]. this section's  main interest is to present conclusive measurements concerning the validity of  the assumptions in [63] and a larger-scale realization than in [14].  edge enhancement can be obtained by subtracting from the image its spa-  tially low-pass filtered version. this operation results in a center-surround  receptive field. a low-pass spatial filter can be realized by using a resistance-  conductance diffusive network implemented with an array of resistors, difficult  to integrate in cmos technology if large enough resistances must be used forexcitatory feedback circuits 207  low consumption. a solution to this problem is to use a transistor with a par-  ticular gate bias and to operate it in its conduction mode. a clever gate biasing  scheme has been proposed [8] to implement with mosfets a resistor-like con-  nection between two nodes. however, the range of linearity of this element's  conductance is limited and depends on its source/drain potentials in a satu-  rating fashion (the saturating non linearity, however, turns out to be useful in  segmentation tasks).  it has been shown that linear networks can be efficiently implemented using  cmos transistors instead of resistors. reference [14] gives full details about  the underlying principle which can be so stated:  the current distribution in the branches of a current-driven resistive net-  work is the same as that of a current driven mos network (all transistors  having the same gate voltage) where resistor terminals are replaced by  sources and drains and conductance ratios are replaced by w/l ratios.  grounded resistor terminals must be replaced by mos terminals at a  potential larger than their pinch-off potential vp (pseudo-ground)  which permits easy extraction of the current entering a pseudo-ground node by  means of a complementary current mirror.  in particular, if transistors are restricetd to operate in the weak inversion  domain, the analog of conductance in the resisitve network, the pseudo conduc-  tance [6], can be also tuned by adjusting the gate voltage. pseudo-conductance  becomes then a linear function of w/l and an exponential function of the gate  voltage va.  ~ inorm  v-i  i,lp  figure 9.4 schematic diagram of a pixel.  the schematic diagram of one pixel is shown in figure 9.4. the photodiode  is a n-well/substrate junction. the photogenerated current enters the pixel's  share tn1 -- tn2 -- tn3 of a chip-wide normalizer integrated using mos transis-  tors in weak inversion. although a "real" translinear network should be made  using bipolar or compatible lateral bipolar transistors [3, 17] the mos solu-208 neuromorphic systems engineering  tion was chosen for its smaller area even with rather large transistors in the  translinear loop and because exact normalization does not seem critical in this  application. the normalization current is directly related to the network's total  activity and can be used to tune bus occupation to the best value according  to equation (9.2). the normalizer has two outputs tn2 and tn3. the current  delivered by tn2 is injected in the pseudo-conductance network. the contribu-  tion to the network of each cell is a conductance ta to the pseudo-ground node  pg and three "outgoing" pseudo-resistances to nearest neighbors tr1, tr2 and  tr3. the three other pseudo-resistive connections are provided by remaining  neighbors. the current injected in the pseudo-resistive grid is spatially low-pass  filtered and the result of this operation, ilp, flows through tc, is collected by  the current mirror tm1 -- tm2, and subtracted from the current iim (a nor-  malized version of the original image) flowing through the second normalizer's  output tn3. the difference ihp, represents the local value of the spatially  high-pass filtered version of the image. it drives the pulse generator composed  by a schmitt trigger (transistors tt1, tt2, tt3 and tt4) and transistors td  and ts. the input node of the schmitt trigger (transistors tt1, tt2, tt3 and  tt4) is alternatively charged by iup and discharged by ides (imposed by dis-  charge transistor td when switch ts is on) between its upper threshold vh +  and its lower threshold v~. introducing av = vh + -- v~ , the pulse frequency  f depends only on the normalized activity current ihp : f ---- ihp/cav and its  duration 5 is controlled by ides assumed much larger than ihp : ~ ~ c/~y/xdis .  the network's total activity is:  ftot= i~o,.,~/cav (9.3)  and therefore, controlling the normalization current is enough to realize con-  dition (9.2). the cell encodes of course only positive values of the high-pass  filtered image. similarly to what has been described in [14], current pulses  generated by tc and tl reach a 7-wires column and a 7-wires row internal  bus. the row and column codes are 7-bits with 3 "ones" (35 possibilities). the  internal bus current configuration is converted into voltage pulses by the same  circuitry as in [14]. figure 9.5 shows a photograph of the chip.  9.5 noise measurements results  the most important theoretical result to test from the communication's point  of view is the dependence of relative error ~2 as a function of bus occupation  for a given chip activity. bus occupation depends only on chip total pulse rate  and is proportional to the normalization current.  to perform this measurement, pulses coming from a particular cell have  been accumulated by the timer of a microcontroller, which also provided ways  of storing the readings in a file. the chip was illuminated with a uniform  spot and transistor ta had its gate at v + to exclude filtering action. mea-  surements have been done for different values of the normalization current.  for each current, 35 readings of the pulse counts over different time windowsexcitatory feedback circuits 209  figure 9.5 die photograph of the silicon retina.  have been recorded. the average and standard deviation of the counts have  then been computed with a spreadsheet. average pulse counts for observation  times of 10, 20 and 30ms are shown in figure 9.6 as a function of normal-  ization current, directly related to the probability of collision, equation (9.1).  note the good agreement of the measurements with the expected behaviour  in i~o~-mexp(-i~or~) (intuitively, the number of received pulses is the product  of the number of emitted pulses, proportional to ino~-~, times the probability  of safe reception, which goes exponentially with -i~o~.,~) this is a first solid  confirmation of the validity of the poisson assumption.  the measured relative error s 2 is shown in figure 9.7 (only points for t -- 10  and 30ms are shown for clarity). to obtain an analytical fit, we used equa-  tion (8), of reference [63]:  s 2nÂ°zfÂ°6 -- 1 1/4 ~ - + -- (9.4) ofot (ofot) ~  in our measurements, t = 10, 20 and 30ms. with uniform illumination,  ofot = 1/nftott, with n = 35.35 = 1225 and ftot given by (9.3). to deter-  mine 6, the system's time resolution in our set-up, we increased the discharge  current of the pulse generators until a sharp decrease of the pulse count was  observed for a discharge current between 3 and 4#a. in this case the pulse du-210 neuromorphic systems engineering  100 t ~'~ â¢ t=10ms  80 / ~ "'â¢-â¢ ~ â¢ t=20ms  0 ,o 0t  ~- 4o  20  1 2 3 4 5 6 7 8 9 10  normalization current (i~a)  figure 9.6 pulse count coming from one cell as a function of the normalization current.  solid line: theoretical fit with an [norm â¢ ez.p(-[~o~.m) -like behaviour.  0.1  ~2  0.01 + t=10ms  + â¢ t=30~  -t- -t- ++-i- nl  ..~...~, â¢ â¢ -+ â¢ ,~...~" â¢ â¢ n n=~ ~  â¢  0.001 ....... , ......  0.1 1 10  normalization current (pa)  figure 9.7 variation of e 2 as a function of normalization current. squares: measured  values, solid line: theoretical prediction based on equation (9.4)excitatory feedback circuits 211  figure 9.8 signal recorded while illuminating the chip with a light bar. top: average  of 2 recordings, observation time 20 ms; bottom: difference of the two same recordings  (visualizes transmission noise).  ration is too small for the receiver logic to operate properly and a conservative  estimate for the largest allowable discharge current is 3#a (corresponding to  ~ ~ 300n8, which is not surprisingly large since the measuring set-up included  a cascade of 2 gate-array-logic chips as decoders, one and gate and a rather  adventurous connection to the controller mounted on a printed circuit). dedi-  cated receiver hardware can push system bandwidth much higher; an array of  analog pulse demodulators [12] has demonstrated 40n8 operation and a digital  receiver chip, soon to be fabricated, has also been designed to that purpose. all  the parameters have thus been estimated to fit the measurements with (9.4).  the result of the fitting is shown in figure 9.7. a good order-of-magnitude esti-  mate of the relative error is provided by the theory despite the many sources of  uncertainty (for instance over the determination of the number of active cells,212 neuromorphic systems engineering  over the real value of 5, the real value of av used to determine the chip pulse  rate ftot and especially real value of 0).  the same set-up was used to produce images with the chip. in this case  the cells were sequentially observed and pulses coming from each address ac-  cumulated over 20ms. figure 9.8 shows the result obtained by projecting a  light bar on the chip. two acquisitions were done to obtain a visual impression  of the fixed-pattern noise, and of the noise contributed by the communication  system. the average value of the two acquisitions is displayed on the top of  the figure (and can be interpreted as a representation of the light signal plus  fixed-pattern noise) while the absolute value of their difference, originating only  from transmission noise, is displayed on the bottom of the figure. as can be  observed, fixed pattern noise definitely dominates over transmission noise and  contributes high-frequency spatial components difficult to separate from actual  edges in the signal by a linear spatial filtering operation without other cues. it  is likely that an adaptation method like the one proposed in [8] is necessary to  auto-zero the system and reduce mismatch effects. as far as communication is  concerned, though, the results obtained with this chip are satisfactory.  9.6 an application of napfm communication to  cortical visual processing  the silicon retina described above has also been used as the input stage of a  multichip system. it feeds a second chip, where the electronic equivalent of an  axonal arborization is realized: incoming pulses determine the injection of a  current at the target location which is then diffused by a non-linear network.  the diffusion network has been originally designed as a building block in the  analog implementation of a kohonen map, reference [4] gives all the details on  how this functionality is achieved. it has been modified to be capable of forming  an activity bubble with controllable size, aspect ratio and orientation [15].  these geometrical parameters are easily set by global biases which, however,  entails the limitation that all bubbles have the same shape. at the edge of  the bubble, voltages at the nodes of the non-linear network decrease sharply  and a threshold can easily set the bubble's exact boundaries. thus the second  chip acts as if spanned by axonal arborizations is capable of distributing every  incoming pulse to several destinations arranged on the bubble. since cells of the  second chip receive inputs from several cells with a center-surround receptive  field, the possibility of implementing hubel and wiesel's [5] simple cell receptive  fields emerges from this architecture. the cortical chip also provides its output  in the napfm format. the operation of the complete chain is demonstrated in  figure 9.9 which shows the three steps of processing: the raw image appears on  the left, the edge-enhanced image in the center and the image with enhanced  vertical edges on the right. in all images the output has been taken from the  second chip, using it as a kind of repeater (by setting the bubbles' sizes to one  pixel) in the first two cases. note that it would be straightforward to plug onexcitatory feedback circuits 213  the retina's output bus to other cortical chips tuned for different orientations  to implement simultaneous enhancement of edges oriented in several directions.  ~g  figure 9.9 results of interfacing the silicon retina with the orientation enhancement chip:  left: photoreceptors response, center: retina on-center response; right: vertical edge en-  hancement by cortical chip. all responses recorded from cortical chip.  9.7 conclusion  this paper discussed the principles and presented recent results of the applica-  tion of a pulsed communication system for analog vlsi perceptive systems.the  theoretical grounds of the system's design have been experimentally verified.  as an example of application, a silicon retina interfaced with a cortical orien-  tation enhancing circuit has been presented. computation performed by the  cortical chip relies on the pulsed representation of the information stream it  receives. it is an example of the opportunistic approach to sensory processing  possible using analog techniques. further applications of the communication  system are foreseen in the design of a sensorimotor loop where it can be used to  set up non topology-conserving links between sensory and motor maps located  on the same chip.  acknowledgments  i wish to acknowledge the constant support of prof. eric vittoz under whose guidance  the project has started and developed and philippe venier, who designed the cortical  chip and kindly provided figure 9.9.  notes  1. editorial note: this scheme was first proposed by marienborg in [10] and called constant  weight coding214 neuromorphic systems engineering  references  [1] x. arreguit, e. a. vittoz, f. van schaik, and a. mortara. analog im-  plementation of low-level vision systems. in ecctd proceedings, davos,  switzerland, august 1993.  [2] j. e. dowling. the retina: an approachable part of the brain. technical  report, belknap harvard, 1987.  [3] b. gilbert. a monolithic 16-channel analog array normalizer. ieee jour-  nal of solid state circuits, sc-19(6):956-963, december 1984.  [4] p. heim, b. hochet, and e. a. vittoz. generation of learning neighbour-  hood on kohonen feature maps by means of a simple nonlinear network.  electronics letters, 27(3), january 1991.  [5] d. hubel and t. wiesel. brain mechanisms of vision. scientific american,  pages 130-144, 1980.  [6] o. landolt. personal communication.  [7] j. lazzaro, j. wawrzynek, m. mahowald, m. sivilotti, and d. gillespie.  silicon auditory processors as computer peripherals. ieee journal of  neural networks, 4(3):523 528, 1993.  [8] m. mahowald. silicon retina with adaptive photoreceptors. in spie's in-  ternational symposium on optical engineering and photonics in aerospace  sensing, orlando, fl, 1991.  [9] m. mahowald. vlsi analogs of neuronal visual processing: a synthesis of  form and function. computation and neural systems, california institute  of technology, 1992.  [10] j. marienborg, t. s. lande, m. hovin, and a. absuland. neuromorphic  analog communication. in proceedings of the ieee international con-  ference on neural networks, volume 2, pages 920-925, washington dc,  1996.  [11] a. mortara and e. a. vittoz. a communication architecture tailored for  analog vlsi neural networks: intrinsic performance and limitations. ieee  transactions on neural networks, tnn-5(3):459 466, may 1994.  [12] a. mortara and e. a. vittoz. a 12-transistor pfm demodulator for analog  neural networks communication. ieee transactions on neural networks,  tnn-6(5), september 1995.  [13] a. mortara, e. a. vittoz, and p. venier. a communication scheme for  analog vlsi perceptive systems. ieee journal of solid state circuits,  sc-30(6):660-669, june 1995.  [14] l. m. reyneri. a performance analysis of pulse stream neural and fuzzy  computing systems. ieee transactions on circuits and systems ii: ana-  log and digital signal processing, cas-42(10), october 1995.  [15] p. venier, a. mortara, x. arreguit, and e. a. vittoz. an integrated cortical  layer for orientation enhancement, submitted to ieee-jssc.excitatory feedback circuits 215  [16] e. vittoz and x. arreguit. linear networks based on transistors. elec-  tronics letters, 29(3):297 299, february 1993.  [17] e. a. vittoz. mos transistors operated in the lateral bipolar mode and  their application in cmos technology. ieee journal of solid state cir-  cuits, sc-18(3), june 1983.0 asynchronous  communication of 2d motion  information using  winner-takes-all arbitration  zaven kalayjian and andreas g. andreou  department of electrical and computer engineering;,  johns hopkins university,  baltimore, md 21218  zavenÂ©olym pus.ece.j hu.ed u  10.1 the asynchronous way  synchronous quantization (data conversion), transmission, and processing of  information are today common engineering practices. these practices have  evolved in an era where system components were designed and optimized inde-  pendantly of each other but with a standard interface.  recent trends towards parallel and distributed processing, as well as high  levels of system integration, have blurred the boundaries between computation  and communication. often, computation and communication resources are  shared by individual processor nodes. in a distributed processing system when  there is a-priori knowledge that not all nodes are likely to require computa-  tion/communication resources at the same time, a fixed time-slot (synchronous)  allocation of resources among all nodes is wasteful. if the demand for resources  is bursty, computation/communication can be done asynchronously. this is  also true for the nervous system, where neurons actively generate their own  output signals when they have salient information to transmit [3].  mahowald [51] and sivilotti [17], demonstrated this important aspect of  neural processing for inter-chip communication. they employed [11] binary-tree  arbitration using asynchronous digital circuits and a full handshaking protocol.218 neuromorphic systems engineering  a closed loop system (full handshaking) is one way to perform this task and  warrants, in theory, a zero probability of information loss.  if communication bit-rate requirements are low, an open loop system (no  handshaking) is a possible solution as well. open loop communication trades  off the zero probability of loosing relevant information to the simplicity in  encoder/decoder design. a neuromorphic free-for-all asynchronous communi-  cation scheme was proposed and experimentally demonstrated for one dimen-  sional arrays by andreou and edwards [14]. their aim was to communicate  the position of a bright spot of a slow moving object in a photoreceptor field  - a task which has very low bit rate requirements. in the centroid computa-  tion/communication system, the channel encoding was performed by employ-  ing an array of randomly firing neurons (relaxation oscillators firing in random  phase and frequency) which were modulated by the stimulus.  an alternative scheme was proposed and demonstrated experimentally in  two dimensions by mortara and vittoz [14], whose theoretical analysis showed  that we can trade off channel capacity for a simple encoding scheme that al-  lows the possibility of collisions and loss of information. boahen [1, 4] analyzed  the trade-offs between encoding complexity and bandwidth and concluded that  arbitered schemes are preferable. more importantly, he achieved reliable com-  munication with a cycle time of a few tens of nanoseconds in a reasonably  large-sized retinomorphic-vision architecture [3]. a communication architec-  ture for stochastically encoded signals was also developed by abusland and  colleagues [1].  figure ]0.]. photomicrograph of the ar chip.winner-takes-all arbitration 219  our asynchronous retina [10] (ar), based on a 2d array of motion encoding  phototransduction neurons, qua pixels, employs a full handshaking protocol  (figure 10.1). it is different from previous fully handshaked implementations  in that it uses winner-takes-all (wta) arbitration, a form of arbitration more  neuromorphic than binary-trees. a similar scheme was proposed independently  in marienborg and associates [13]. our ar comprises an arbitration circuit and  an array of pixels, and is connected to a macintosh computer that acts as an  off-chip receiving system.  10.2 ar system architecture  t.o.., oo,~o,  ~ ~  ....  reÂ¢olvor ~ ............................ ~ ..........  acknowledge  figure 10.2 system architecture. an array of phototransducers transmits motion data  asynchronously via winner-take-all arbiters.  the system architecture of the ar chip is depicted in figure 10.2. data  transmitted from the ar to the receiver constitutes a temporal stream of pixel  addresses (bundled data). the receiver reads the address of a communicat-  ing pixel on the address-output bus, and acknowledges receipt on the receiver  acknowledge line.  information processing in the pixels can be divided into three categories:  adaptive phototransduction, motion encoding/quantization, and communica-  tion processing. the adaptive phototransducer [8] senses temporal changes in  light intensity while maintaining relative immunity to the absolute radiance  from the imaged scene thus providing an invariant representation of the image.  the motion encoder circuit produces an output when local temporal intensity  changes are sensed, thus providing a rudimentary form of data encoding to  yield what is essentially a motion sensitive system.  the output of the motion encoding circuitry is a current i, ntn which feeds  into capacitor c,~m. a threshold is applied to the accumulated charge on220 neuromorphic systems engineering  c,~e,~ which reduces the signal to a binary value, qo~t. in this way, its function  is like the soma of a neuron which receives and integrates dendritic input and  produces a spike on the axon. the quantizer has memory (c,~e,~), which means  a spiking pixel will continue to vie for access to the address output bus until it  is finally selected and reset. a more detailed picture of the quantizer is found  in figure 10.3.  quantizer  cmem,f~[~ ~0 - ~ gout  qin -\  i imtn' w ( qreset  v motion encoder  figure 10.3 motion encoder and quantizer. the shaded elements (cfb and inverter) are  not present in the circuit discussed in this paper. they are part of an improved quantizer  design.  the communication processor (figure 10.4) is responsible for handshaking  with the wta arbiters and with the receiver. the communication processor  also broadcasts the state of a selected pixel - that is, whether the pixel is active  or has been reset - via the pixel state line.  wta arbiters in the periphery regulate pixel communication with the off-  chip receiver. arbitration ensures that only one pixel is selected to load its  address onto the address-output bus at any given time. whereas other asyn-  chronous systems have appealed to digital circuits in their design of arbiters  - for example, using binary trees of cross-coupled nand gates - wta ar-  biters represent a more neuromorphic approach to arbitration. the wta cells  are cascaded, voltage-mode, analog circuits (figure 10.5) based on the single-  transistor current-conveyor [2]. the second stage in the cascade is designed  with positive feedback from the output (transistor mfb), so that, once a pixelwinner-takes-all arbitration 221  a~  v~ v~ck,  hack: ih k, ~vrea, iil_)hrea, "~.~pixel l 2~eqi 2~qj ~ " state  @ =  ...........................  figure 10.4 circuit schematic of the communication processor for pixeli,j. each pixel in  a row (or column) is connected to the vreqÂ¢ (or hr~qj) line, but the pull-down transistors  in the shaded box are common to all pixles.  vdd~  req~ i\~~  f~ght~ ~'~]z f ~--t~  figure 10.5 fightb~  ~-~-,~--~0-~i acki  ~l__ wt~ reset  winner-take-all arbiter cell.  wins, competition is forcibly ended and no subsequently firing pixel can be se-  lected [99]. latching the wta arbiters ensures closed-loop operation; that is,  until the wta arbiters are reset, communication is locked.  cascading in the wta cells creates two layers of competition. in each  layer, the wta units are biased by a common resistive network but compete  on separate nodes, fighta and fightb. figure 10.6 shows a simulation of a  sixteen-unit arbiter. the outputs of the first and second stage of the wta222 neuromorphic systems engineering  ~.0_-- ......... ~ a  - i 2 1 ~ : : ~e~  tl ~ 0 _-- 3  ~ ~o ~ ..... .... ...... ~  n ~- .............. ~  ...... ,j  ~.8~5~ ~ inti~  v : ~ .... ~ a ~ ..... : ~ ~. ~ ~ o : ~ =.~:~..._:::~_:~:.~ ~  t - ~ ~ ........ _ ............ nt~  ~ 2 o ~ ....... : ~~~:-~-~ ......... ~ ....... ~ *~4~ .... -  1 0 : ........ : ~ ~'~-:~-~ ........ ....... = ~int~- - - - ~ .:%:: .... ~ ~ ......... ~ ...... z ~ ......  o.: ,  ~ ~ ~  ~ ~ ~c~4  n ~ck5  ~/~ = ~- ~ ~ ~ckg 1.0 ...... ~ : ~ ......  ~ : ~c~f  o i , , , ~ ,' , , ~ ~--, ~ ~ ~ ;* ..........  l ou 1 2ou i qou  ~oo o~ tih~ zli~] 1 ~ou  figure i0.6 simulation of a sixteen-unit wta arbiter. top: the input signal to all sixteen  units. middle: competition in the first layer of the wta arbiter. bottom: output of wta  arbiter.  cells are represented in panel two and panel three, respectively. the same  input signal, req, is given to all sixteen inputs of the wta arbiter. in the  first stage, the effect of the resistive-network biasing the wta units can be  seen separating the signals. rather than waiting for equally biased wta units  to slowly resolve a winner, this tilt in the biasing quickly sorts out a large  backlog of pixels requesting arbitration - for example, if the sensor undergoes  ego-motion causing nearly all the pixels to fire simultaneously. since the output  of the first layer of competition becomes the input to the second layer of wta  units, the second stage of the cascade chooses a winning pixel based on the  competition it sees from the first stage. a winner is decided by the second  stage long before competition in the first stage has ended.  a typical handshaking cycle with an off-chip receiver is as follows. pixel  (2,1) senses a temporal change in light intensity that activates its motion en-  coder/quantizer. qo~t of the pixel goes low, and the pixel makes a communi-  cation request to the vertical wta arbiter (vwta) along the v,-eq2 line. the  vwta arbitrates, acknowledges the requesting row by lowering line vack2, and  simultaneously communicates with the vertical-address encoder, which loadswinner-takes-all arbitration 223  the vertical coordinate (0010) of the firing pixel onto the address-output bus.  the communication processor of the active pixel acknowledges the vwta by  raising hrÂ¢ql, which initiates arbitration by the horizontal wta (hwta). the  hwta selects a winner, responds along the ha~kl line, and communicates with  the horizontal-address encoder, which loads the horizontal coordinate (0001) of  the firing pixel. at this point, a data-valid signal is generated by the address  encoders to alert the receiver that the complete address of the communicating  pixel has been loaded onto the address output bus.  the receiver reads the address and acknowledges the transmission through  the receiver-acknowledge port. the receiver-acknowledge is transmitted to all  pixels, but only the communicating pixel - the one receiving va~k and ha~  signals from the wta arbiters - can be reset. resetting a pixel consists of  charging up ct~ak, thus bringing qr~s~t low. a pixel that has been selected and  reset will remain dormant for as long as the capacitor ctÂ¢~k is charged - a time  determined by the leak bias. to ensure a complete loop, each pixel notifies  the array of its state via the pixel state line, which prevents the arbiters from  being reset before the pixel has been reset. once the pixel has been reset (pixel  state goes low), the pixel state signal ands with the receiver acknowledge  to reset the wta arbiters. when the wta arbiters are reset, they cease  communications with the address encoders, which remove the data from the  address output and reset the data-valid line. resetting the data-valid line  concludes the communication cycle, and the sender is ready to communicate  again.  10.3 experimental results  we fabricated two chips in 2#m-cmos processes through mosis, and assem-  bled a receiver system on a macintosh platform.  the first chip, a full ar, consists of an array of 108 (9 Ã 12) pixels fabri-  cated in an n-well process. the ar is capable of generating a video output  of the phototransducer activity, as well as asynchronously communicating with  a receiver system. the receiver consists of a macintosh quadra 840av fitted  with a national instruments lab-nb data-acquisition board. we developed  custom software that allows the lab-nb board to receive data from the test  chips, to display graphically the pixel address, and to return an acknowledge  signal. we measured asynchronous-communication speeds of 3khz between  ar and computer, which was the maximum rate given the receiver's hardware  and software limitations. snapshot images of the output in synchronous-video  and asynchronous-communication modes are shown in figure 10.7. in another  experiment, we removed the macintosh from the communication loop by feed-  ing back a slightly delayed version of the data-valid signal into the receiver-  acknowledge port of the chip using two inverters and a capacitor. with this  configuration, we recorded communication rates of 400khz (figure 10.8).  our circuit layout severely affected the functionality of the system. charge  pumping rendered control of the pixel's refractory states ineffectual. charge224 neuromorphic systems engineering  |  i  figure 10.7 sequence of images from the ar looking at a diagonal edge moving from  bottom right to top left. top row: video output of the adaptive phototransducers. bottom  row: asynchronous output. black squares indicate communication activity, gray squares  indicate pixels which have fired previously, white indicates pixels which have not fired.  15  10 handshaking with macintosh  dval  ack , . , ~_~  i  5 1 115 time (ms) 2.5  15  10  0 ~ 0 handshaking with inverters  , , , ,  ~ ~ dval ack  i i i ~  5 10 15 20 25  time (gs)  figure 10.8 measured handshaking data. ac/~ is the receiver acknowledge signal and  dval is data valid.winner-takes-all arbitration 225  stored on czeak was quickly depleted by transistor switching, so refractory  times were always negligible. furthermore, since the pixels were not completely  shielded by a metal layer, photogenerated leakage currents caused sporadic  firing in the pixel array. also, power consumption was unmeasurable due to  parasitic light-emitting structures on the chip [15].  we fabricated, in a p-well process, a second test chip that contained an  isolated 16-input wta circuit. we wrote diagnostic software to test 64 possible  combinations of wta inputs. the arbitration time of the 16-input wta was  approximately 18ns, which compares favorably with our simulations. of the  64 input combinations possible for this test setup, three invalid outputs were  produced at 18ns arbitration rates.  10.4 discussion  although our measured results were encouraging, considering the various  layout-related problems mentioned in section 10.3, there are two potential im-  provements to this design.  first, we can accelerate pixel firings by including feedback in the quantizer.  it is well known that biological neurons have a positive-feedback mechanism  (sodium channels) that forces rapid depolarization of the soma once the spiking  threshold has been crossed. we can implement a similar feedback mechanism  in the quantizer to speed up spiking of the pixel by including positive feedback  from qo~,t to q,.es~t. the shaded capacitor and inverter in figure 10.3 shows  the improved quantizer circuit.  second, pipelining of the communication process can greatly enhance perfor-  mance, as it has in other designs (c.f. boahen). in brief, we can use pipelining to  speed up communication in the following way: after arbitration by the vwta,  the entire selected row is loaded into a register, from which the hwta arbi-  trates. meanwhile, the vwta arbiter is available to the pixel array for another  arbitration cycle. additional communication circuitry between the pixels and  the wta arbiters, as well as between the wta arbiters and the receiver, will  be required.  10.5 conclusions  we have presented a new design of an asynchronous, fully handshaked, address-  event-based system. the ar uses adaptive phototransducers, and commu-  nicates asynchronously through a hierarchical arbitration scheme based on  voltage-mode wta arbiters. through the ar, we have shown that wta  arbiters - whose analog mode of operation resembles more closely the biologi-  cal communication system we are trying to emulate - can be used effectively in  an asynchronous environment. the wta arbiters also offer processing possi-  bilities that digital arbitration schemes cannot offer; for example, the ar can  implement attentional behaviors using distributed hysteretic feedback into the  wta inputs [9].226 neuromorphic systems engineering  acknowledgments  this work was supported by an onr multidisciplinary university research initiative  (muri) for automated vision and sensing systems n00014-95-1-0409. testing of  the system and preparation of this document were performed while the authors were  visiting the physics of computation laboratory at caltech. we thank professor  carver mead for providing a stimulating laboratory environment for this work.  references  [1] a. abusland, t. lande, and m. h0vin. a vlsi communication architec-  ture for stochastically pulse-encoded analog signals. in proceedings of the  ieee international symposium on circuits and systems, volume 3, pages  401 404, atlanta, ga, 1996.  [2] a. g. andreou and k. a. boahen. analog vlsi signal and information  processing. in m. ismail and t. fiez, editors, neural information process-  ing ii, pages 358-413. mcgraw-hill, new york, 1994.  [3] h. b. barlow. possible principles underlying the transformations of sensory  messages. in w.a. rosenblit, editor, sensory communications. mit press,  cambridge, ma, 1961.  [4] k. boahen. retinomorphic vision systems. in int. conf. on microelec-  tronics for neural networks, volume 16-5, pages 30 39, los alamitos, ca,  1996. epfl/csem/ieee.  [5] k. boahen. retinomorphic vision systems i: pixel design. in proceedings  of the ieee international symposium on circuits and systems, volu~ne  supplement, pages 14 19, atlanta, ga, may 1996.  [6] k. boahen. retinomorphic vision systems ii: communication channel  design. in proceedings of the ieee international symposium on circuits  and systems, volume supplement, pages 9-14, atlanta, ga, may 1996.  [7] g. cauwenberghs and v. pedroni. a charge-based cmos parallel analog  vector quantizer. in advances in neural information processing systems,  volume 7, pages 779-786, cambridge, ma, 1995. mit press.  [8] t. delbriick and c. a. mead. analog vlsi phototransduction. cns memo  no 30, may 1994.  [9] s. p. deweerth. analog vlsi circuits for stimulus localization and centroid  computation. international journal of computer vision, 8(3):191 202,  1992.  [10] z. kalayjian, j. waskiewicz, d. yochelson, and a. andreou. asynchronous  sampling of 2d arrays using winner takes all circuits. in proceedings of the  ieee international symposium on circuits and systems, volume 3, pages  393 396, atlanta, ga, 1996.  [11] j. lazzaro, j. wawrzynek, m. mahowald, m. sivilotti, and d. gillespie.  silicon auditory processors as computer peripherals. ieee journal of  neural networks, 4(3):523 528, 1993.winner-takes-all arbitration 227  [12] m. mahowald. vlsi analogs of neuronal visual processing: a synthesis of  form and function. computation and neural systems, california institute  of technology, 1992.  [13] j. marienborg, t. s. lande, /~. absuland, and m. hovin. an analog ap-  proach to 'neuromorphic' communication. in proceedings of the ieee in-  ternational symposium on circuits and systems, volume 3, pages 397-400,  atlanta, ga, 1996.  [14] a. mortara, e. a. vittoz, and p. venier. a communication scheme for  analog vlsi perceptive systems. ieee journal of solid state circuits,  sc-30(6):660-669, june 1995.  [15] a. obeidat, z. kalayjian, a. andreou, and j. khurgin. a model for visible  photon emission from reverse-biased silicon p-n juncions. applied physics  letters, 70(4):470 471, 1997.  [16] m. sivilotti. wiring considerations in analog vlsi systems with applica-  tions to field programmable networks. phd thesis, california institute of  technology, pasadena ca, 1991.11 communicating neuronal  ensembles between  neuromorphic chips  kwabena a. boahen  physics of computation laboratory, ms 136-93,  california institute of technology,  pasadena ca 91125  buste@pcmp.caltech.edu  11.1 time-division multiplexing  the small number of input-output connections available with standard chip-  packaging technology, and the small number of routing layers available in vlsi  technology, place severe limitations on the degree of intra- and interchip con-  nectivity that can be realized in multichip neuromorphic systems. inspired by  the success of time-division multiplexing in communications [16] and computer  networks [19], many researchers have adopted multiplexing to solve the con-  nectivity problem [12, 67, 17]. multiplexing is an effective way of leveraging  the 5 order-of-magnitude difference in bandwidth between a neuron (hundreds  of hz) and a digital bus (tens of megahertz), enabling us to replace dedicated  point-to-point connections among thousands of neurons with a handful of high-  speed connections and thousands of switches (transistors). this approach pays  off in vlsi technology because transistors take up a lot less area than wires,  and are becoming relatively more and more compact as the fabrication process  scales down to deep submicron feature sizes.  four important performance criteria for a communication channel that pro-  vides virtual point-to-point connections among arrays of cells are:  capacity: the maximum rate at which samples can be transmitted. it is  equal to the reciprocal of the minimum communication cycle period.230 neuromorphic systems engineering  latency: the mean time between sample generation in the sending popula-  tion and sample reception in the receiving population.  temporal dispersion: the standard deviation of the channel latency.  integrity: the fraction of samples that are delivered to the correct destina-  tion.  all four criteria together determine the throughput, which is defined as the  usable fraction of the channel capacity, because the load offered to the channel  must be reduced to achieve more stringent specifications for latency, temporal  dispersion, and integrity.  as far as neuromorphic systems [1] are concerned, a sample is generated  each time a neuronal spike occurs. these spikes carry information only in their  time of occurence, since the height and width of the spike is fixed. we must  make the time it takes to communicate the occurence of each spike as short as  possible, in order to maximize the throughput.  the latency of the sending neuron should not be confused with the channel  latency. neuronal latency is defined as the time interval between stimulus onset  and spiking; it is proportional to the strength of the stimulus. channel latency  is an undesirable systematic offset. similarly, neuronal dispersion should not  be confused with channel dispersion. neuronal dispersion is due to variabil-  ity between individual neurons; it is inversely proportional to the strength of  the stimulus. channel dispersion is additional variability introduced by uneven  access to the shared communication channel. hence, channel latency and chan-  nel dispersion add systematic and stochastic offsets to spike times and reduce  timing precision.  a growing body of evidence suggest that biological neurons have submil-  lisecond timing precision and synchronize their firing, making it imperative  to minimize channel latency and dispersion. although neuronal transmission  has been shown to be unreliable, with failure occuring at axonal branches and  synapses, it is most likely that each spike changes the local state of the axon or  synapse--even if it is rejected--and thereby determines the fate of subsequent  spikes. so the fact that communication in the nervous system is unreliable  does not give us license to build an imperfect communication channel, as the  decision whether or not to transmit a spike is not arbitrary.  there are several alternatives to using the timing of fixed-width/fixed-height  pulses to encode information, and several approaches to optimizing channel  performance ~s shown in table 11.1; the choices i have made are highlighted.  i attempt to justify my choices by introducing a simple population activity  model in section 11.2. i use this model to quantify the tradeoffs faced in  communication channel design in section 11.3. the model assumes that the  activity in the pixel array is whitened (i.e. activity is clustered in space and  in time). having motivated my approach to pixel-parallel communication, i  describe the implementation of a pipelined communication channel, and show  how a retinomorphic chip is interfaced with another neuromorphic chip in sec-  tion 11.4. the paper concludes with a discussion in section 11.5. parts of thisneuromorphic chips 231  table 11.1 time-multiplexed communication channel design options. the choices made  in this work are highlighted.  specification approaches remarks  activity  encoding pulse amplitude  pulse width  pulse code  pulse timing long settling time and static power  dissipation  channel capacity degrades with  increasing width  inefficient at low precision (< 6 bits)  uses minimum-width, rail-to-rail  pulses  latency polling  event-driven latency e( total number of neurons  latency e( number currently active  integrity collision rejection  arbitration collisions increase exponentially with  throughput  reorder events to prevent collisions  dispersion dumping  queueing new events are given priority ~ no  dispersion  dispersion e( 1/capacity, at  constant throughput  capacity hard-wired simple ~ short cycle time  pipelined cycle time set by a single stage  work have been described previously in conference proceedings [1, 4] and in a  magazine article [186].  11.2 population activity model  although a fairly general purpose implementation was sought, my primary  motivation for developing this communication channel is to read pulse trains off  a retinomorphic imager chip [1]. therefore, the channel design was optimized  for retinal population activity, and an efficient and robust solution that supports  adaptive pixel-parallel quantization was sought.  11.2.1 retinal processing  the retina converts spatiotemporal patterns of incident light into spike trains.  transmitted over the optic nerve, these discrete spikes are converted back into  continuous signals by dendritic integration of excitatory postsynaptic potentials  in the lateral geniculate nucleus of the thalamus. for human vision, contrast  thresholds of less than 1%, processing speeds of about 20 ms per stage, and232 neuromorphic systems engineering  temporal resolution in the submillisecond range are achieved, with spike rates  as low as a few hundred per second. no more than 10 spikes per input are  available during this time. the retina achieves such high peprformance by  minimizing redundancy and maximizing the information carried by each spike.  the retina must encode stimuli generated by all kinds of events efficiently,  over a large range of lighting conditions and stimulus velocities. these events  fall into three broad classes, listed in order of decreasing probability of occur-  rence:  1. static events: generate stable, long-lived stimuli; examples are buildings  or trees in the backdrop.  2. punctuated events: generate brief, short-lived stimuli; examples are a  door opening, a light turning on, or a rapid, short saccade.  3. dynamic events: generate time-varying, ongoing stimuli; examples are  a spinning wheel, grass vibrating in the wind, or a smooth-pursuit eye  movement.  in the absence of any preprocessing, the output activity mirrors the input  directly. changes in illumination, which influence large areas, are reflected  directly in the output of every single pixel in the region affected. static events,  such as a stable background, generate persistent activity in a large fraction  of the output cells, which transmit the same information over and over again.  punctuated events generate little activity and are transmitted without any  urgency. dynamic events generate activity over areas far out of proportion to  informative features in the stimulus, when the stimulus rapidly sweeps across  a large region of the retina. clearly, these output signals are highly correlated  over time and over space, resulting in a high degree of redundancy. hence,  reporting the raw intensity values makes poor use of the limited throughput of  the optic nerve.  the retina has evolved exquisite filtering and adaptation mechanisms to  improve coding efficiency, six of which are described briefly below[6, 7]:  1. local automatic gain control at the receptor level eliminates the depen-  dence on lighting--the receptors respond to only contrast--extending the  dynamic range of the retina's input without increasing its output range.  2. bandpass spatiotemporal filtering in the first stage of the retina (outer  plexiform layer or opl) attenuates signals that do not occur at a fine  spatial or a fine temporal scale, ameliorating redundant transmission of  low frequency signals and eliminating noisy high frequency signals.  3. highpass temporal and spatial filtering in the second stage of the retina  (inner plexiform layer or ipl) attenuates signals that do not occur at  a fine spatial scale and a fine temporal scale, eliminating the redundant  signals passed by the opl, which responds strongly to low temporal  frequencies that occur at high spatial frequencies (sustained response toneuromorphic chips 233  static edge) or to low spatial frequencies that occur at high temporal  frequencies (blurring of rapidly moving edge).  4. half-wave rectification, together with dual-channel encoding (on and  off output cell types), in the relay cells between the opl and the ipl  (bipolar cells) and the retina's output cells (ganglion cells) eliminates the  elevated quiescent neurotransmitter release rates and firing rates required  to signal both positive and negative excursions using a single channel.  5. phasic transient-sustained response in the ganglion cells avoids temporal  aliasing by transmitting rapid changes in the signal using a brief, high  frequency burst of spikes, and, at the same time, avoids redundant sam-  pling, by transmitting slow changes in the signal using modulation of a  low sustained firing rate.  6. foveated architecture, together with actively directing the gaze (saccades),  eliminates the need to sample all points in the scene at the highest spatial  and temporal resolution, while providing the illusion of doing so every-  where. the cell properties are optimized: smaller and more sustained in  the fovea (parvocellular or x cell type), where the image is stabilized by  tracking, and larger and more transient in the periphery (magnocellular  or y cell type), where motion occurs.  the resulting activity in the ganglion cells, which convert these preprocessed  signals to spikes, and transmit the spikes over the optic nerve, is rather different  from the stimulus pattern. for relatively long periods, the scene captured by  the retina is stable. these static events produce sparse activity in the opl's  output, since the opl does not respond to low spatial frequencies, and virtually  no activity in the ipl's output, since the ipl is selective for temporal frequency  as well as spatial frequency. the opl's sustained responses drive the 50,000, or  so, ganglion cells in the fovea, allowing the fine details of the stabilized object  to be analyzed. while the vast majority of the ganglion cells, about 1 million  in all, are driven by the ipl, and fire at extremely low quiescent rates of 10s/s  (spikes per sec), or less, in response to the static event.  when a punctuated event (e.g. a small light flash) occurs, the ipl responds  strongly, since both high temporal frequencies and high spatial frequencies are  present, and a minute subpopulation of the ganglion cells raise their firing rates,  briefly, to a few hundred spikes per second. a dynamic event, such as a spinning  windmill, or a flash that lights up an extended region, is effectively equivalent  to a sequence of punctuated events occurring at different locations in rapid  succession, and can potentially activate ganglion cells over an extended region  simultaneously. this is indeed the case for the opl-driven cells but is not true  for the ipl-driven cells, which cover most of the retina, because the low spatial  frequencies produced in the opl's output by such a stimulus prevent the ipl  from responding.  in summary, the activity in the optic nerve is clustered in space and time  (whitened spectrum), consisting of sporadic short bursts of rapid firing, trig-234 neuromorphic systems engineering  gered by punctuated and dynamic events, overlaid on a low, steady background  firing rate, driven by static events.  11.2.2 the neuronal ensemble  we can describe the activity of a neuronal population by an ordered list of  locations in spacetime  ~ = {(~:olto),(:~l;tl),...(~:~;td,...};  to <tl <'"t~ < ...,  where each coordinate specifies the occurrence of a spike at a particular loca-  tion, at a particular time. the same location can occur in the list several times  but a particular time can occur only once--assuming time is measured with  infinite resolution.  there is no need to record time explicitly if the system that is logging this  activity operates on it in real-tim~only the location is recorded and time  represents itself. in that case, the representation is simply  ~" = {x0, xl,...xi,...}; to < tl < "''ti < "''.  this real-time representation is called the address-event representation (aer)  [12, 17]. i shall present more details about aer later. at present, my goal  is to develop a simple model of the probability distribution of the neuronal  population activity described by g.  g has a great deal of underlying structure that arises from events occurring  in the real world, to which the neurons are responding. the elements of g  are clustered at temporal locations where these events occur, and are clustered  at spatial locations determined by the shape of the stimulus. information  about stimulus timing and shape can therefore be obtained by extracting these  clusters, g also has an unstructured component that arises from noise in the  signal, from noise in the system, and from differences in gain and state among  the neurons. this stochastic component limits the precision with which the  neurons can encode information about the stimulus.  we can use a much more compact probabilistic description for g if we know  the probability distributions of the spatial and temporal components of the  noise. in that case, each cluster can be described explicitly by its mean spatial  configuration and its mean temporal location, and the associated standard  deviations.  ~ '~ { (~0, 17x0; ~0, crt0), (~1, (tx 1 ; ~i, (tt 1),  â¢ .., (~, ~; ~, ~td, â¢ â¢ â¢, };  to <tl...t~ <....  i shall call these statistically-defined clusters neuronal ensembles. the ith neu-  ronal ensemble is defined by a probability density function pi(z; t) with param-  eters â¢ ~, axi, ~, ~ti. the probability that a spike generated at location x at  time t is a member of the ith ensemble is then obtained by computing pi(x; t).neuromorphic chips 235  in what follows, i will assume that the distribution of the temporal compo-  nent of the noise is gaussian and the latency (the mean minus the stimulus  onset time) and the standard deviation are inversely proportional to the stim-  ulus strength. we can measure the parameters of this distribution by fitting  a gaussian to the normalized poststimulus time histogram, as shown in fig-  ure ll.la. here, latency refers to the time it takes to bring a neuron to spiking  threshold; this neuronal latency is distinct from the channel latency that i  defined earlier. the gaussian distribution arises from independent random  variations in the characteristics of these neurons--whether carbon or silicon  based--as dictated by the central limit theorem.  .~-~, probability density  :~ ~  1 2 ,/:/  Â¢---'~ ~/3a  o" "/~-'2~" i "~ "/,j2y ~ t o "/,-'2~" t'~ "/,j2y  (~) (b) â¢ t  figure 11.1 the hypothetical distribution, in time, of samples generated by a subpopula-  tion of neurons triggered by the same stimulus at t -= 0. (a) normalized poststimulus time  histogram with gaussian fit: 8 is the number of samples per bin, /k is the bin-width, and n  is the total number of samples; # and (7 are the mean and standard deviation of the fitted  (~aussian. (b) rectangular distribution with the same mean and standard deviation as the  gaussian. after redistributing samples uniformly, the sampling rate is only 72% of the peak  rate reached in the original distribution, yielding the minimum capacity required to transmit  the burst without dispersion.  the ratio between the standard deviation and the latency, which is called  the coefficient of variation (cov), is used here as a measure of the variability  among the neurons. other researchers have used the cov to measure mismatch  among transistors and the variability in the steady firing rates of neurons; i use  it to measure variability in latency across a neuronal ensemble. as the neuronal  latency decreases, the neuronal temporal dispersion decreases proportionally,  and hence the cov remains constant. the cov is constant because the charac-  teristics that vary between individual neurons, such as membrane capacitance,  channel conductance, and threshold voltage, determine the slope of the latency  versus input current curves rather than the absolute latency or firing rate.  11.3 design tradeoffs  several options are available to the communication channel designer. should  he preallocate the channel capactiy, giving a fixed amount to each node, or  allocate capacity dynamically, matching each nodes allocation to its current  needs? should she allow the users to transmit at will, or implement elaborate236 neuromorphic systems engineering  mechanisms to regulate access to the channel? and how does the distribution  of activity over time and over space impact these choices? can he assume that  nodes act randomly, or are there significant correlations between the activities  of these nodes? i shed some light on these questions in this section, and provide  some definitive answers.  two-layer network  itt  inputs . _~ii 1 3  222 "  â¢ ~ iiii _~ ~o p ,o ~  _~11 i1 k.  layer 1 layer 2/ \/\/  outputs  chip 1  .~ii i  2 2 2  .~ iiii 9 ~ .o  _~ii  i t t layer 1  inputs two-chip implementation  chip 2  321322123 1-~  l , time ii i xÂ¢ n ~  iii i ~Â¢ n ~Â¢  ii i 1 ,~  layer 2z k  outputs  figure 11.2 two-chip implementation of a two-layer spiking neural network (adapted  from [12]). in the two-layer network, the origin of each spike is infered from the line on  which it arrives. this parallel transmission uses a labeled-line representation. in the two-  chip implementation, the neurons share a common set of lines (digital bus) and the origin  of each spike is preserved by broadcasting one of the labels. this serial transmission uses  an address-event representation (aer). no information about the height or width of  the spike is transmitted, thus information is carried by spike timing only. a shared address-  event bus can transparently replace dedicated labeled lines if the encoding, transmission,  and decoding processes cycle in less than a/n seconds, where/k is the desired spike timing  precision and n is the maximum number of neurons that are active during this time.neuromorphic chips 237  11.3.1 allocation: dynamic or static?  we are given a desired sampling rate fnyq, and an array of n signals to be  quantized. we may use adaptive, 1-bit, quantizers that sample at fnyq when  the signal is changing, and sample at fnyq/z when the signal is static. let  the probability that a given quantizer samples at fnyq be a. that is, a is the  fraction of the quantizers whose inputs are changing. then, each quantizer  generates bits at the rate  fbits ~- fnyq(a ~- (1 -- a)/z)log 2 n,  because a percent of the time, it samples at fnyq; the remaining (1 -a) percent  of the time, it samples at fnyq/z. furthermore, each time that it samples,  log 2 n bits are sent to encode the location, using the aforemention address-  event representation (aer) [12, 17]. aer is a fairly general scheme for trans-  mitting information between arrays of neurons on separate chips, as shown in  figure 11.2.  on the other hand, we may use more conventional qunatizers that sample  every location at fnyq, and do not locally adapt the sampling rate. in that  case, there is no need to encode location explicitly; we simply cycle through  all n locations, according to a fixed squence order, and infer the origin of each  sample from its position in the sequence. for a constant sampling rate, the bit  rate per quantizer is simply fnyq"  adaptive sampling produces a lower bit rate than fixed sampling if  a < (z/(z - 1))(1/log2 n - l/z).  for example, in a 64 Ã 64 array with sampling-rate attenuation, z, of 40, the  active fraction, a, must be less than 6.1 percent. in a retinomorphic system, the  adaptive neuron circuit performs sampling-rate attenuation (z will be equal to  the firing rate attenuation factor "y) [6], and the spatiotemporal bandpass filter  makes the output activity sparse [6], resulting in a low active fraction a.  it may be more important to minimize the number of samples produced per  second, instead of minimizing the bit rate, as there are usually sufficient i/o  pins to transmit all the bits in each sample in parallel. in that case, it is the  number of samples per second that is fixed by the channel capacity.  given a certain fixed channel throughput (fchan), in samples per second, we  may compare the effective sampling rates, fnyq, acheived by the two strate-  gies. for adaptive quantization, channel throughput is allocated dynamically  in the ratio a : (1 - a)/z between the active and passive fractions of the node  population. hence,  fnyq = fchan/(a "-b (1 - a)/z) (11.1)  where fchan = f~han/n is the throughput per node. in contrast, fixed quan-  tization achieves only f~han- for instance, if fchan ~- 100s/s, adaptive quan-  tization achieves fnyq = 1.36ks/s, with an active fraction of 5 percent and a  sampling-rate attenuation factor of 40. thus, a fourteen-fold increase in tem-  poral bandwidth is achieved under these conditions; the channel latency also is  reduced by the same factor.238 neuromorphic systems engineering  11.3.2 access: arbitration or ~"ree-for-all?  if we provide random access to the shared communication channel, in order  to support adaptive pixel-level quantization [186], we have to deal with con-  tention for channel access, which occurs when two or more pixels attempt to  transmit simultaneously. we can introduce an arbitration mechanism to re-  solve contention and a queueing mechanism to allow nodes to wait for their  turn. however, arbitration lengthens the communication cycle period, reduc-  ing the channel capacity, and queuing causes temporal dispersion, corrupting  timing information. on the other hand, if we simply allow collisions to occur,  and discard the corrupted samples so generated [14], we may achieve a shorter  cycle period and reduced dispersion, but sample loss will increase as the load  increases.  we may quantify this tradeoff using the following well-known result for the  collision probability [16]:  pcoll = 1 - e -2c, (11.2)  where g is the offered load, expressed as a fraction of the channel capacity. 1  for u < 1, u is the probability that a sample is generated during the commu-  nication cycle period.  if we arbitrate, we will achieve a certain cycle time, and a corresponding  channel capacity, in a given vlsi technology. an arbitered channel can operate  at close to 100-percent capacity because the 0.86 collision probability for g = 1  is not a problem--users just wait their turn. now, if we do not arbitrate, we  will achieve a shorter cycle time, with a proportionate increase in capacity. let  us assume that the cycle time is reduced by a factor of 10, which is optimistic.  for the same offered load, we have g = 0.1, and find that pco~l = 18 percent.  thus, the simple nonarbitered channel can handle more spikes per second only  if collision rates higher than 18 percent are acceptable. for lower collision rates,  the complex, arbitered channel offers more throughput, even though its cycle  period is 1 order of magnitude longer, because the nonarbitered channel can  utilize only 10 percent of its capacity.  indeed, the arbiterless channel must operate at high error rates to maximize  utilization of the channel capacity. the throughput is ge -2c [16], since the  probability of a successful transmission (i.e., no collision), is e -2a. throughput  reaches a maximum when the success rate is 36 percent (e -~) and the collision  rate is 64 percent. at maximum throughput, the load, u, is 50 percent and,  hence, the peak channel throughput is only 18 percent. increasing the load  beyond the 50% level lowers the channel utilization because the success rate  falls more rapidly than the load increases.  in summary, the simple, free-for-all design offers higher throughput if high  data-loss rates are tolerable, whereas the complex, arbitered design offers higher  throughput when low-data loss rates are desired. and, due to the fact that the  maximum channel untilization for the free-for-all channel is only 18 percent,  the free-for-all channel will not be competitive at all unless it can achieve a  cycle time that is five times shorter than that of the arbitered channel.neuromorphic chips 239  indeed, the free-for-all protocol was first developed at the university of  hawaii in the 1970s to provide multiple access between computer terminals  and a time-shared mainframe over a wireless link with an extremely short cycle  time; it is known as the aloha protocol [16]. in this application, a vanish-  ingly small fraction of the wireless link's tens of megahertzs of bandwidth is  utilized by people typing away at tens of characters per second on a few hun-  dred computer terminals, and hence the error rates are negligible. however,  in a neuromorphic system where we wish to service hundreds of thousands of  neurons, efficient utilization of the channel capacity is of paramount concern.  the inefficiency of aloha h~s been long recognized, and researchers have  developed more efficient communication protocols. one popular approach is  csma (carrier sense, multiple access), where each user monitors the channel  and does not transmit when the channel is busy. the ethernet protocol uses  this technique, and most local-area-networks work this way. making available  information about the state of the channel to all users greatly reduces the num-  ber of collisions. a collision occurs only when two users attempt to transmit  within a time interval that is shorter than the time it takes to update the in-  formation about the channel state. hence, the collision rate drops if the time  that users spend transmitting data is longer than the round trip delay; if not,  csma's performance is no better than aloha's [16]. lande's group at the  cs-dept., univ. of oslo, norway is developing a csma-like protocol for neuro-  morphic communication to avoid the queueing associated with arbitration [1].  what about the timing errors introduced by queueing in the arbitered chan-  nel? it is only fair to ask whether these timing errors are not worse than the  data-loss errors. the best way to make this comparison is to express the chan-  nel's latency and temporal dispersion as fractions of the neuronal latency and  temporal dispersion, respectively. if the neuron fires at a rate fnyq, we may as-  sume, for simplicity, that its latencies are uniformly distributed between 0 and  tnyq = 1/fnyq. hence, the neuronal latency is # -- tnyq/2, and the neuronal  temporal dispersion a = tnyq/(2v/3). for this fiat distribution, the coefficient  of variation is c -- 1/v/3, which equals 58 percent.  to find the latency and temporal dispersion introduced by the queue, we use  a well-known result from queueing theory which gives the moments of the time  spent waiting in the queue, ~, as a function of the moments of the service  time, ~ [9]:  ax 2 --  w --  2(1 - g)'  --  ax 3  w--~ = 2~2+ 3(1)g ~;-  where a is the arrival rate of the samples. 2 an interesting property of the queue,  which is evident from these results, is that the first moment of the waiting time  increases linearly with the second moment of the service time. similarly, the240 neuromorphic systems engineering  second moment of the waiting time increases linearly with the third moment  of the service time.  in our case, we may assume that the service time, a, is fixed; hence x -~ = an.  in that case, the mean and the variance of the number of cycles spent waiting  are given by  ~ g ~ -- --- (11.3) zx 2(1-g)'  __  2 w 2 _ ~2 _ ~2 2  ~m -- a~ + gin. (11.4)  ~or example, at 95-percent capacity, a sample spends 9.5 cycles in the queue,  on average. this result agrees with intuition: as every twentieth slot is empty~  one must wait anywhere from 0 to 19 cycles to be serviced~ which averages out  to 9.5. hence the latency is 10.5 cycles, including the mdditional cycle required  for service~ ~nd the temporal dispersion is 9.8 cycles~virtually equal to the  latency. in general, the temporal dispersion will be approximately equal to the  latency whenever the latency is much larger than one cycle.  if there are a tot~l of ~ neurons~ the cycle time is ~ = o/(n~chan), where  ~ is the normalized load~ and the timing error due to channel l~tency will be  (m + 1)a ~nyq m + i % ~ - 2g  ~ f~h~n ~  using the expression for the number of cycles spent w~iting (equation 11.3),  and the relationship between ~nyq and f~h~ (equation 11.1)~ we obtain  2g 1 1 - g/2  e,= n a+(1-a)/z 1-g  for example, at 95 percent load and at 5 percent active fraction, with a sampling  rate attenuation of 40 and with a population of size 4096 (64 x 64), the latency  error is 7 percent. the error introduced by the temporal dispersion in the  channel will be similar, as the temporal dispersion is more or less equal to the  latency.  notice that the timing error is inversely proportional to n. this scaling  occurs because channel capacity must grow with the number of neurons. hence,  the cycle time decreases, and there is a proportionate decrease in queueing time,  even though the number of cycles spent queueing remains the same~for the  same normalized load. in contrast, the collision rate remains unchanged for  the same normalized load. hence, the arbitered channel scales much better  than the nonarbitered one as technology improves and shorter cycle times are  achieved.  11.3.3 trat~c: random or correlated?  correlated spike activity occurs when external stimuli trigger synchronous ac-  tivity in several neurons. such structure, which is captured by the neuronal en-neuromorphic chips 241  semble concept, is much more plausible than totally randomized spike times--  especially if neurons are driven by sharply defined object features (high spatial  and temporal frequencies) and adapt to the background (low spatial and tem-  poral frequencies). if there are correlations among firing times, the poisson  distribution does not apply to the spike times, but, making a few reasonable  assumptions, it may be used to describe the relative timing of spikes within a  burst.  the distribution of sample times within each neuronal ensemble is best de-  scribed by a gaussian distribution, centered at the mean time of arrival, as  shown in figure ll.la. the mean of the gaussian, #, is taken to be the de-  lay between the time the stimulus occurred and the mean sample time, and  the standard deviation of the gaussian, ~r, is assumed to scale with the mean,  i.e. the coefficient of variation (cov) c -- or/#, is constant.  the minimum capacity, per quantizer, required to transmit a neuronal en-  semble without temporal dispersion is given by  fbur~t 1  fburst ~ -- -- --  -]vburs t 2~/3c#'  using the equivalent uniform distribution shown in figure ll.lb. this simple  model predicts that shorter latencies or less variability can be had only by  paying for a proportionate increase in channel throughput. for instance, a  latency of 2ms and a cov of 10% requires 1440 s/s per quantizer. this result  assumes that the neurons' interspike intervals are large enough that there is no  overlap in time between successive bursts; this is indeed the case if c < l/x/3  or 58%.  there is a strong tendency to conclude that the minimum throughput spec-  ification is simply equal to the mean sampling rate. this is the case only if  sampling times are totally random. random samples are distributed uniformly  over the period t = l/f, where f is the quantizer's mean sampling rate, as-  sumed to be the same for all nodes that are part of the ensemble; the latency  is # = t/2, and the temporal dispersion is a = t/(2v/3 ). hence, the cov is  c = l/v/3 =58% and equation 11.3.3 yields fburst = l/t, as expected. for  a latency of 2ms and a cov of 58%, the required throughput is 250 s/s per  quantizer compared to 1440 s/s when the cov is 10%.  the rectangular (uniform) approximation to the gaussian, shown in fig-  ure ll.lb, may be used to calculate the number of collisions that occur during  a burst. we simply use equation 11.2, and set the rate of the poisson process to  the uniform sampling rate of the rectangular distribution. the result is plotted  in figure 11.3.  11.3.4 throughput requirements  by including the background activity of the neurons that do not participate in  the neuronal ensemble, we obtain the total throughput requirement  ftotm = afburst -~ (1 -- a)ffire/z ,242 neuromorphic systems engineering  0.8  ~0.6  ~ 0.4  0.2 collision probability vs bandwidth  1 ,  0 i0 20 30 40 50  bandwidth/minimum  figure 11.3 theoretical collision probability versus channel capacity (bandwidth) for gaus-  sian bursts. actual capacity is normalized to the minimum capacity requirement. thus, the  numbers on the bandwidth/minimum axes are the reciprocal of the offered channel load  ~y. for collision probabilities below 18 percent, capacities ten or more times the minimum  are required. the dots represent results of a numerical computation based on the gaussian  and binomial distributions for sample occurrence within a burst, and the number of sam-  ples in each time-slot, respectively. the line represents results of a simplified, analytically  tractable, approximation based on the equivalent rectangular and poisson distributions, re-  spectively. the simplified model overestimates the collision probability by no more than 8%;  the estimation error drops below 4.4% for capacities greater than i0 times the theoretical  minimum.  where a is the fraction of the population that participates in the burst; ffire/z is  the firing rate of the remaining quantizers, expressed as an attenuation, by the  factor z, of the sampling rate of the active quantizers. assuming f~re = 1/(2#),  we have ffire/fburst = v/3c, and  ftotal ~ a + ~/3c(1 - a)/z  2~3c# '  per quantizer. for a 2ms latency, a 10 percent coy, a 5 percent active fraction,  and an attenuation factor of 40, the result is ftotal = 78.1s/s per quantizer. for  these parameter values, a 15.6 percent temporal dispersion error is incurred,  assuming a channel loading of 95 percent and a population size of 4096 (64 by  64 array).  this result is only valid if samples are not delayed for more than the duration  of the burst (i.e. errors less than 1/v/3 = 58%). for larger delays, the temporal  dispersion grows linearly--instead of hyperbolically--because the burst is dis-  tributed over an interval no greater than (~burst/fchan)/tburst, when the sampleneuromorphic chips 243  rate /~burst exceeds the channel capacity, fchan, where tburs t is the duration of  the burst.  11.4 pipelined communication channel  in this section, i describe an arbitered, random-access communication channel  design that supports asynchronous pixel-level analog-to-digital conversion. as  discussed in the previous section, arbitration is the best choice for neuromor-  phic systems whose activity is sparse in space and in time, because it allows  us to trade an exponential increase in collisions for a linear increase in tempo-  ral dispersion. furthermore, for the same percentage channel utilization, the  temporal dispersion decreases as the technology improves, and we build larger  systems with shorter cycle times, whereas the collision probability remains the  same.  the downside of arbitration is that this process lengthens the communication  cycle, reducing channel capacity. i have achieved improvements in throughput  over previous arbitered designs [12] by adopting three strategies that shorten  the average cycle time:  1. allow several address-events to be in various stages of transmission at the  same time. this well-known approach to increasing throughput is called  pipelining; it involves breaking the communication cycle into a series of  steps and overlapping the execution of these steps as much as possible.  2. exploit locality in the arbiter tree. that is, do not arbitrate among all  the inputs every time; doing so would require spanning all log 2 (n) levels  of the tree. instead, find the smallest subtree that has a pair of active  inputs, and arbitrate between those inputs; this approach minimizes the  number of levels spanned.  . exploit locality in the row-column architecture. that is, do not redo both  the row arbitration and the column arbitration for each address-event.  instead, service all requesting pixels in the selected row, redoing only  the column arbitration, and redo the row arbitration only when no more  requests remain in the selected row.  this work builds on the pioneering contributions of mahowald [12] and  sivilotti [17]. like their original design, my implementation is completely self-  timed: every communication consists of a full four-phase handshaking sequence  on a pair of wires, as shown in figure 11.4. self-timed operation makes queue-  ing and pipelining straightforward [18]: you stall a stage of the pipeline or make  a pixel wait simply by refusing to acknowledge it. lazzaro et.al, have also im-  proved on the original design [11], and have used their improved interface in a  silicon auditory model [15].244 neuromorphic systems engineering  j ~[req 22  sender _ receiver  i el  oa,a i da a  i -1  (a/ ....  r~q  m\  (b)  figure 11.4 self-timed data-transmission protocol using a four-phase handshake. (a)  data-bus (data) and data-transfer control signals (req and ack). (b) handshake protocol  on control lines. the sender initiates the sequence by driving its data onto the bus and  taking req high. the receiver reads the data when req goes high, and drives ack high  when it is done. the sender widthdraws its data and takes req low when ack goes high.  the receiver terminates the sequence by taking ack low after req goes low, returning the  bus to its original state. as data is latched on ack ~', the designer can ensure that the setup  and hold times of the receiver's input latch are satisfied by delaying; req ~', relative to the  data, and by delaying widthdrawing the data after ack ~.  11.4.1 communication cycle sequence  the operations involved in a complete communication cycle are outlined in this  subsection. this description refers to the block diagram of the channel archi-  tecture in figure 11.5; the circuits are described in the next two subsections. at  the beginning of a communication cycle, the request and acknowledge signals  are both low.  on the sender side, spiking neurons first make requests to the y arbiter,  which selects only one row at a time. all spiking neurons in the selected row  then make requests to the x arbiter. at the same time, the y address encoder  drives the address of the selected row onto the bus. when the x arbiter selects  a column, the neuron in that particular column, and in the row selected earier,  resets itself and withdraws its column and row requests. at the same time, the  x address encoder drives the addresses of the selected column on to the bus,  and takes req high.  when ack goes high, the select signals that propagate down the arbiter tree  are disabled by the and gates at the top of the x and y arbiters. as a result,  the arbiter inactivates the select signals sent to the pixels and to the address-  encoders. consequently, the sender withdraws the addresses and the request  signal req.  when it is necessary, the handshake circuit (also known as a c-element [18])  between the arbiters and the rows or columns will delay inactivating the select  signals that drive the pixel, and the encoders, to give the sending pixel enough  time to reset. the sender's handshake circuit is designed to stall the communi-  cation cycle by keeping req high until the pixel withdraws its row and columnneuromorphic chips 245  ~' ..... ~o ~" ~ vreset[ i " i- x i i pixack  i vspk ~  ~ ~  ~,,,,~, , ...... / .............. ~ ..... ~  ~ . lreqorpu ,~ ~  '~~!l..~'b ~"~'~ i~-~.~ set  c-element  driver c-eh,meut  figure 11.5 pipelined addess-event channel. the block diagram describes the channel  architecture; the logic circuits for each block also are shown. sender chip: the row and  column arbiter circuits are identical; the row and column handshaking circuits (c-elements)  are also identical. the arbiter is built from a tree of two-input arbiter cells that send a  request signal to, and receive a select signal from, the next level of the tree. the sending  neuron's logic circuit (upper-left) interfaces between the adaptive neuron circuit, the row  c-element (lower-left: ry --~ rpix, apix -~ ay), and the column c-element (lower-left:  rx --~ rpix, apix --+ ax). the pull-down chains in the pixel--tied to pull-up elements at  the right and at the top of the array--and the column and row request lines form wired-or  gates. the c-elements talk to the column and row arbiters (detailed circuit not shown)  and drive the address encoders (detailed circuit not shown). the encoders generate the  row address (y), the column address (x), and the chip request (req). receiver chip:  the receiver's c-element (lower-right) acknowledges the sender, strobes the latches (lower-  middle), and enables the address decoders (detailed circuit not shown). the receiver's  c-element also monitors the sender's request (req) and the receiving neuron's acknowledge  (apix). the receiving neuron's logic circuit (upper-right) interfaces between the row and  column selects (ry and rx) and the post-synaptic integrator, and generates the receiving  neuron's acknowledge (apix). the pull-down path in the pixel--tied to pull-up elements  at the left of the array--and the row acknowledge lines form wired-or gates. an extra  wired-or gate, that runs up the left edge of the array, combines the row acknowledges into  a single acknowledge signal that goes to the c-element.246 neuromorphic systems engineering  sending sender pixel c-element arbiter reciever recieving  c-element pixel  <t---t---:  ~'~" ~  â¢ "~ l~i~~  ~ set phase ' "~" "~,.. ,..~ ~,,.~  ,  ~ reset phase  figure 11.6 pipelined communication cycle sequence for arbitration in one dimension,  showing four-phase minicycles among five elements. the boxes indicate the duration of the  current cycle, which may overlap with the reset phase of the preceding cycle and the set  phase of the succeeding cycle. (steps associated with the preceding and succeeding cycles  are shown with dashed-lines.) thus three address-events may be at different stages in the  communication channel at the same time. the cycle consists of three smaller interwoven  minicycles: sending pixel to c-element, c-element to c-element, and c-element to receiving  pixel. the c-elements--also known as handshake circuits---ensure that the minicycles occur  in lock-step, synchronizing the activity of the sending pixel, the arbiter, and the receiving  pixel.  requests, confirming that the pixel has reset. the exact sequencing of these  events is shown in figure 11.6.  on the receiver side, as soon as req goes high, the address bits are latched  and ack goes high. at the same time, the address decoders are enabled and,  while the sender chip is deactivating its internal request and select signals, the  receiver decodes the addresses and selects the corresponding pixel. when the  sender takes req low, the receiver responds by taking ack low, disabling the  decoders and making the latches transparent again.  when it is necessary, the receiver's handshake circuit, which monitors the  sender's request (req) and the acknowledge from the receiving pixel (apix), will  delay disabling the address-decoders to give the receiving pixel enough time to  read the spike and generate a post-synaptic potential. the reciever's handshake  circuit is designed to stall the communication cycle by keeping ack high untilneuromorphic chips 247  or gate flip-flop steering circuit  figure 11.7 arbiter cell circuitry. the arbiter cell consists of an or gate, a flip-flop, and a  steering circuit. the or gate propagates the two incoming active-high requests, f~i and [~2,  to the next level of the tree by driving f~out. the flip-flop is built from a pair of cross-coupled  nand gates. its active-low set and reset inputs are driven by the incoming requests, f~i  and r2, and its active-high outputs, q1 and q2, control the steering circuit. the steering  circuit propagates the incoming active-high select signal, ain, down the appropriate branch  of the tree by driving either a1 or a2, depending on the state of the flip-flop. this circuitry  is reproduced from mahowald 1994 [12].  the pixel acknowledges, confirming that the pixel did indeed recieve the spike.  the exact sequencing of these events also is shown in figure 11.6.  11.4.2 arbiter operation  the arbiter works in a hierarchical fashion, using a tree of two-way decision  cells [11, 12, 17], as shown in figure 11.5. thus, arbitration between n inputs  requires only n - 1 two-input cells. the n-input arbiter is layed out as a  (n - 1) Ã 1 array of cells, positioned along the edge of the pixel array, with  inputs from the pixels coming in on one side and wiring between the cells  running along the other side.  the core of the two-input arbiter cell is a flip-flop with complementary inputs  and outputs, as shown in figure 11.7; these circuits were designed by sivilotti  and mahowald [12, 17]. that is, both the set and reset controls of the flip-  flop (tied to r1 and r2) are normally active (i.e., low), forcing both of the  flip-flops outputs (tied to q1 and q2) to be active (i.e., high). when one  of the two incoming requests (r1 or r2) becomes active, the corresponding  control (either set or reset) is inactivated, and that request is selected when the  corresponding output (q1 or q2) becomes inactive. in case both of the cell's  incoming requests become active simultaneously, the flip-fiop's set and reset  controls are both inactivated, and the flip-flop randomly settles into one of its  stable states, with one output active and the other inactive. hence, only one  request is selected.248 neuromorphic systems engineering  before sending a select signal (a1 or a2) to the lower level, however, the cell  sends a request signal (rout) up the tree and waits until a select signal (ain)  is received from the upper level. at the top of the tree, the request signal is  simply fed back in, and becomes the select signal that propagates down the  tree.  as the arbiter cell continues to select a branch so long as there is an active  request from that branch, we can keep a row selected, until all the active pixels  in that row are serviced, simply by oring together all the requests from that  row to generate the request to the y arbiter. similarly, as the request passed  to the next level of the tree is simply the or of the two incoming requests, a  subtree will remain selected as long as there are active requests in that part  of the arbiter tree. thus, each subtree will service all its daughters once it  is selected. using the arbiter in this way minimizes the number of levels of  arbitration performed the input that requires the smallest number of levels  to be crossed is selected.  to reset the select signals fed into the array--and to the encoder previous  designs removed the in-coming requests at the bottom of the arbiter tree [11,  12, 17]. hence, the state of all the flip-flops were erased, and a full log2(n)-level  row arbitration and a full column arbitration had to be performed for every  cycle. in my design, i reset the row/column select signals by removing the  select signal from the top of the arbiter tree; thus, the request signals fed in  at the bottom are undisturbed, and the state of arbiter is preserved, allowing  locality in the array and the arbiter tree to be fully exploited.  i achieve a shorter average cycle time by exploiting locality; this opportuinis-  tic approach trades fairness for efficiency. instead of allowing every active pixel  to bid for the next cycle, or granting service on a strictly first-come first-served  basis, i take the travelling-salesman approach, and service the customer that  is closest. making the average service time as short as possible to maximize  channel capacity--is my paramount concern, because the wait time goes to  infinity when the channel capacity is exceeded.  11.4.3 logic circuits and latches  in this subsection, i describe the address-bit latches and the four remaining  asynchronous logic gates in the communication pathway. namely, the inter-  faces in the sending and receiving neurons and the c-elements in the sender  and receiver chips. the interactions between these gates, the neurons, and  the arbiter--and the sequencing constraints that these gates are designed to  enforce--are depicted graphically in figure 11.6.  the logic circuit in the sending neuron is shown in figure 11.5; it is similar  to that described in [11, 12]. the neuron takes vspk high when it spikes, and  pulls the row request line ry low. the column request line rx is pulled low  when the row select line ay goes high, provided vspk is also high. finally, ireset  is turned on when the column select line ax goes high, and the neuron is reset.neuromorphic chips 249  vadpt is also pulled low to dump charge on the feedback integrator in order to  adapt the firing rate [3].  i added a third transistor, driven by vspk, to the reset chain to turn off lreset  as soon as the neuron is reset, i.e. vspk goes low. thus, ~reset does not continue  to discharge the input capacitance while we are waiting for ax and ay to go  low, making the reset pulse width depend on only the delay of elements inside  the pixel.  the sender's c-element circuit is shown in the lower-left corner of fig-  ure 11.5. it has a flip-flop whose output (apix) drives the column or row select  line. this flip-flop is set when aarb goes high; which happens when the arbiter  selects that particular row or column. the flip-flop is reset when rpix goes  high, which happens when two conditions are satisfied: (i) aarb is low, and  (ii) all the pixels tied to the wired-or line rpix are reset. thus the wired-or  serves three functions in this circuit: (i) it detects when there is a request in  that row or column, passing on the request to the arbiter by taking r~rb high;  (ii) it detects when the receiver acknowledges, by watching for a~rb to go low;  and (iii) it detects when the pixel(s) in its row or column are reset.  there are two differences between my handshaking circuit and the hand-  shaking circuit of lazzaro et.al. [11].  first, lazzaro et.al, disable all the arbiter's inputs to prevent it from granting  another request while ack is high. by using the and gate at the top of  the arbiter tree to disable the arbiter's outputs (aarb) when ack is high, my  design leaves the arbiter's inputs undisturbed. as i explained in the previous  subsection, my approach enables us to exploit locality in the arbiter tree and  in the array.  second, lazzaro et.al, assume that the selected pixel will withdraw its re-  quest before the receiver acknowledges. this timing assumption may not hold  if the receiver is pipelined. when the assumption fails, the row or column select  lines may be cleared before the pixel has been reset. in my circuit, the row  and column select signals (apix) are reset only if aarb is low, indicating that  the receiver has acknowledged, and no current is being drawn from rpix by the  array, indicating that the pixel has been reset.  mahowald's original design used a similar trick to ensure that the select  lines were not cleared prematurely. however, her handshaking circuit used  dynamic state-holding elements which were susceptible to charge-pumping and  to leakage currents due to minority carriers injected into the substrate when  devices are switched off. my design uses fully static stateholding elements.  the receiver's c-element (it is slightly different from the sender's) is shown in  the lower-right corner of figure 11.5. the c-element's output signal drives the  chip acknowledge (ack), strobes the latches, and activates the address decoders.  the flip-flop that determines the state of ack is set if req is high, indicating  that there is a request, and apix is low, indicating that the decoders' outputs  and the wired-or outputs have been cleared. it is reset if req is low, indicating  that the sender has read the acknowledge and apix is high, indicating that the  receiving neuron got the spike.250 neuromorphic systems engineering  the address-bit latch and the logic inside the receiving pixel are also shown  in the figure (middle of lower row and upper-right corner, respectively). the  latch is opaque when ack is high, and is transparent when ack is low. the pixel  logic produces an active low spike whose duration depends on the delay of the  wired-or and the decoder, and on the duration of the sender's reset phase.  circuits for the blocks that are not described here--namely, the address  encoder and the address decoder--are given in [11, 12].  11.4.4 performance and improvements  in this subsection, i characterize the behavior of the channel and present mea-  surements of cycle times and the results of a timing analysis in this section. for  unacessible elements, i have calculated estimates for the delays using the device  and capacitance parameters supplied by mosis for the fabrication process.  figure 11.8 shows plots of address-event streams that where read out from  the sender under two vastly different conditions. in one case, the load was  less than 5% of the channel capacity. in the other case, the load exceeded the  channel capacity. for small loads, the row arbiter rearranges the address--events  as it attempts to scan through the rows, going to the nearest row that is active.  scanning behavior is not evident in the x addresses because no more than 3 or  4 neurons are active simultaneously within the same row. for large loads, the  row arbiter concentrates on one half, or one quarter, of the rows, as it attempts  to keep up with the data rate. and the column arbiter services all the pixels  in the selected row, scanning across the row. sometimes the addresses are  transposed because each arbiter cell chooses randomly between its left branch  (lower half of its range) or its right branch (upper half of its range).  figure 11.9a shows the relative timing of req, ack, the x-address bit xad0,  and the acknowledge from the pixel that received the address-event apix. the  cycle breaks down as follows. the 18ns delay between req $ and ack $ is due  to the receiver's c-element (8.9ns) and the pad (9.1as); the same holds for the  delay between req j" and ack $. the 57ns delay between ack $ and apix $ is due  to the decoder (13ns), the row wired-or (41ns), and the second wired-or that  runs up the left edge of the array (3.4ns). the 57ns delay between ack j" and  apix ]', breaks down similarly: decoder (38ns), row wired-or (14ns), left-edge  wired-or (3.4ns). the 94ns and 170ns delays between apix and req are due  to the sender. the x-address bits need not be delayed because the c-element's  8.9ns delay is sufficient set-up time for the latches.  figure 11.9b shows the relative timing of req, the y-address bits yad0, and  the select-enable signals (xsel, ysel) fed in at the top of the arbiter trees; Ãsel  and ysel are disabled by ack. the first half of the first cycle breaks down  as follows. the 48ns delay between req $ and ysel ~" is due to the receiver  (18ns) and the and circuit (30ns)--it is a chain of two 74hc04 inverters and  a 74hc02 nor gate. 3 the 120ns delay between ysel ? and yadr0 j" is due to  the arbiter (propagating a high-going select down the six-level tree takes 19ns  per stage and 6.tns for the bottom stage), the row/column c-element (5.2ns),neuromorphic chips 251  y ~,: q~ ~,  â¢ : o!/ , ~ ~ ; ;  .~* ~ â¢ . ~ ~  ~ ~ ~. ~ ~.oo ~  ,,. ~.,.~ , ~ ~.~  ~ ~ ~ ~ ~ ~ ~  â¢ â¢ a, .:* ~'~ ; 0"" ,~ ~ ~ : *~ ~  . ~'~ , â¢ .~ .,~..  â¢ . ~. ~ .~ ~ ~" .. â¢ . ~. ~ ~ . .~  ~o =~. ~ ~ ~ ) ~. .~  o ~' ~ ~ , ." .~. ~ .  o ..~ ." ", 01: :f ~,0 ~  ~ ~ ~ ~ ~  y~  io~a ~ ~ ~ ~ ~ ~ ~ x a~= ~ l~p~  ?0  ":" :':~i'::.:" ".':-: " :'. ":"" .* . â¢ Ã·.'* ... â¢ Ã· .... ~. â¢ ~. â¢:*. ..... **.** ... .  , ."~./â¢ "-.':: .:.. :. ". :.:'...  : +:. + ~+. /.~ :-.~..." . .~  : .'- ./~-.. : ..': -...'.:  :~."~. . ..... ~'..'-'.: ,'..  .~.. ".~ .~... ~ ::.. ".." . :.:..:"  â¢ ~ . +.~ ~ . +* . ~ ~..~  ~ ~. ~ . + ... *~* ~ /.'+.. :..~/ ~ "-..-" ~ â¢ ~ "-~.~ ~.. ~  + â¢ ~ , ~ ~ , a, **+ ~+, . .,  1~ ~ ~ ~ ~  x~:~  ~0 .  q q  ~ ~ ~ l  ~ /~ ), ~  , s ~  ~ ~ ~ ~ ~ ~ ~ '~  ~ 4 ~ z ~'~ /, ~ ,, io , ~  ~ ,~t ~  ~ ,~ , z,  ~ i~ ll~ ii~ i~ i~  figure 11.8 recorded address-event streams for small (queue empty) and large (queue  full) channel loads. x and y addresses are plotted on the vertical axes, and their position in  the sequence is plotted on the horizontal axes. queue empty (top row): the y addresses  tend to increase with sequence number, but the row arbiter sometimes remains at the same  row, servicing up to 4 neurons, and sometimes picks up a row that is far away from the  previous row. whereas the x addresses are distributed randomly. queue full (bottom row):  the y addresses tend to be concentrated in the top or bottom half of the range, or in the  third quarter, and so on, and the column arbiter services all 64 neurons in the selected row.  the x addresses tend to decrease with sequence number, as all the neurons are serviced  progressively, except for transpositions that occur over regions whose width equals a power  of 2. note that the horizontal scale has been magnified by a factor of 30 for the x addresses.252 neuromorphic systems engineering  receiver timing sender timing  _ ~~ 5z,. j~.. .  s 15 2 ~5 3 5,l~0,' ..... Â°,0, 2 , 0 lo~,l ,0  (a) (b) ~ x ~o ~  figure 11.9 measured address-event channel timing. all the delays given are measured  from the preceding transition: (a) timing of req and ack signals relative to x-address bit  (xad0) and receiver pixers acknowledge (apix). pipelining shaves a total of 113ns off the  cycle time (twice the duration between ack and apix). (b) timing of req signal relative  to the select signals fed into the top of the arbiter trees (ysel and xsel, disabled when ack  is high), and the y-address bit (yad0). arbitration occurs in both the y and x dimensions  during the first cycle, but only in the x dimension during the second cycle; the cycle time is  730 ns for the first cycle and 420 ns for the second.  the address encoder (3.2ns), and the pad (tns). the same applies to the delay  between xsel j" and req . the 190ns delay between yad0 j" and xsel ~" is due  to the column wired-or (120ns), the arbiter (propagating a high-going request  up the six-level tree takes 4.sns per stage and 11ns for the top stage), the pad  (tns), and the and circuit (30ns). the slow events are propagating a high-  going select down the arbiter (100ns total) and propagating a request from the  pixel through the column wired-or (120ns).  the second half of the first cycle breaks down as follows. the 49ns delay  between req ]" and xsel $, ysel $ is identical to that for the opposite transitions.  the 200ns delay between xsel $, ysel $ and req $ is due the arbiter (propagating  a low-going select signal down the six-level tree takes 6.sns per stage and 2.5ns  for the bottom stage), the column and row wired-ors (120ns), the handshake  circuit (5.2ns), the encoder (32ns), and the pad (tns). the slow events are  propagating a low-going select down the arbiter (36ns total), restoring the  wired-or line (120ns), and restoring the address-lines (32ns).  the second cycle is identical to the first one except that there is no row  arbitration. hence, xsel goes high immediately after req goes low, eliminating  the 310ns it takes to propagate a select signal down the y-arbiter, to select a  row, to get the column request from the pixel, and to propagate a request up  the x-arbiter.  this channel design achieves a peak throughput of 2.5ms/s (million spikes  per second), for a 64 x 64 array in 2#m cmos technology. the cycle time isneuromorphic chips 253  730ns if arbitration is performed in both dimensions and 420ns when arbitration  is performed in only the x dimension (i.e., the pixel sent is from the same row  as was the previous pixel). these cycle times represent a threefold to fivefold  improvement over the 2#s cycle times reported in the original work [12], and are  comparable to the shortest cycle time of 500ns reported for a much smaller 10 x  10 nonarbitered array fabricated in 1.6#m technology [14]. lazzaro et.al, report  cycle times in the 100-140ns range for their arbitered design, but the array size  and the chip size are a lot smaller.  pipelining the receiver shaves a total of 113ns off the cycle time--the time  saved by latching the address-event, instead of waiting for the receiving pixel  to acknowledge. pipelining the sending pixel's reset phase did not make much  difference because most of the time is spent waiting for the row and column  wired-or request lines to reset, once the pixel itself is reset. unfortunately, i  did not pipeline reseting these request lines: i wait until the receiver's acknowl-  edge disables the select signals that propagate down the arbiter tree, and arb  goes low, before releasing the column and row wired-or request line. propa-  gating these high-going and low-going select signals down the six-level arbiter  tree (ll0ns+40ns) and resetting the column and row request lines (120as) adds  a total of 270ns to the cycle time, when arbitration is performed in only the x  dimension, and adds 400ns when arbitration is performed in both the x and y  dimensions.  the imapct of the critical paths revealed by my timing analysis can be  reduced significantly by making three architectural modifications to the sender  chip:  1. moving the and gate that disables the arbiter's select signals to the  bottom of the tree would shave off a total of 144ns; this change requires  an and gate for each row and each column. 4  2. removing the input to the column and row wired-or from the arbiter  would allow the column and row request lines to be cleared as soon as  the pixel is reset; this requires adding some logic to reset the flip-flop in  the sender's c-elemet when rpix is high and aarb is low.  3. doubling the drive of the two series devices in the pixel that pull down  the column line would reduce the delay of the wired-or gate to 60as.  these modifications, taken together, allow us to hide reseting the column and  row select lines in the 59ns it takes for apix? ~ req~" ~ ack$ ~ xsel$, shaving  off a total of 120ns, when arbitration occurs in only the x dimension, and  180ns when arbitration occurs in both dimensions. these changes, together,  will reduce the cycle time to 156ns with arbitration in one dimension, and to  406ns with arbitration in both dimensions. purther gains may be made by  optimizing the sizes of the devices in the arbiter for speed and adding some  buffers where necessary.254 neuromorphic systems engineering  11.4.5 bugs and fixes  in this section, i describe the bugs i discovered in the channel design and  propose some ways to fix them.  during testing, i found that the sender occasionally generates illegitimate  addresses, i.e. outside the 1 to 64 range of pixel locations. in particular, row  (y) addresses higher than 64 were observed. this occurs when row 64 and one  other row, or more, are selected simultaneously. i traced this problem to the  sender's c-element (lower-left of figure 11.5).  after ack goes high and aarb goes low, the pull-up starts charging up rpiÃ.  when rpiÃ crosses the threshold of the inverter, rarb goes low. when the  arbiter sees rarb go low it selects another row. however, if rpix has not crossed  the threshold for resetting the flip-flop, the flip-flop remains set and keeps the  previous row selected. hence, two rows will be selected at the same time, and  the encoder will or their addresses together.  this scenario is plausible because the threshold of the inverter that drives  rarb is lower than that of the flip-flop's reset input; i calculated 2.83v and  3.27v, respectively. if any neuron in the previously selected row spikes while  rpix is between these two values, rpix will be pulled back low, and the flip-  flop will not be reset. rpix spends about 0.44/2.5 Ã 120ns = 21ns in this  critical window. at a total spike rate of 100khz, we expect a collision rate of  0.05hz, just for row sixty-four alone. i observed a rate of 0.06hz; the higher  rate observed may be due to correlations in firing times. to eliminate these  collisions, we should disable neurons from firing while their row is selected (i.e.,  ay is high). that way, rpix will remain low until apix goes low, ensuring that  the flip-flop is reset. 5  iw ~~~scanout  ~. ~  (al r _  ::Â°ol  nand gaw intcg~alor  (b)  figure 11.10 comparison between my (a) first and (b) second circuit designs for a receiver  pixel. the second design eliminated charge-pumping and capacitive turn-on which plagued  the first design, as explained in the text.neuromorphic chips 255  i also had to redesign the receiver pixel to eliminate charge-pumping and  capacitive turn-on, which plagued the first pixel i designed, and to reduce cross-  talk between the digital and analog parts by careful layout. two generations  of receiver pixel circuit designs are shown in figure 11.10.  the pair of transistors controlled by the row and column select lines, r~, and  rÃ, pump charge to ground when non-overlapping pulses occur on the select  lines. in my first design, this charge-pump could supply current directly to the  integrators--irrespective of whether or not that pixel had been selected. the  pump currents are significant as, on average, a pixel's row or column is selected  64 times more often than the pixel itself. for a parasitic capacitance of 20ff  on the node between the transistors, an average spike rate of 100hz per pixel,  and a voltage drop of 0.5v, the current is 64pa. this current swamps out the  subpicoamp current levels we must maintain in the diode-capacitor integrator  to obtain time constants greater than 10ms using a tiny 300ff capacitor.  i solved this problem in my second design by adding a pull-up to implement  an nmos-style nand gate. the pull-up can supply a fraction of a milliamp,  easily overwhelming the pump current. the nand gate turns on the transis-  tor that supplies current to the integrator by swinging its source terminal from  vdd to gnd. as demonstrated by cauwenberghs [6], this technique can meter  very minute quantities of charge onto the capacitor. in addition to eliminat-  ing charge-pumping, this technique circumvents another problem we encounter  when we attempt to switch a current source on and off: capacitive turn-on.  rapid voltage swings on the select line are transmitted to the source termi-  nal of the current-source transistor by the gate-drain overlap capacitor of the  switching transistor. in the first design, where this terminal's voltage was close  to ghld, these transients could drive the source terminal a few tenths of a volt  below gsid. as a result, the current source would pass a fraction of a picoamp  even when iw was tied to gsid. in the new design, the pull-up holds this node  up at vdd and supplies the capacitive current, preventing the node from being  discharged.  11.5 discussion  since technological limitations precluded the use of dedicated lines, i developed  a time-multiplexed channel that communicates neuronal ensembles between  chips, taking advantage of the fact that the bandwidth of a metal wire is several  orders of magnitude greater than that of a nerve axon. thus, we can reduce the  number of wires by sharing wires among neurons. we replaced thousands of  dedicated lines with a handfull of wires and thousands of switches (transistors).  this approach paid off well because transistors take up much less real estate  on the chip than wires do.  i presented three compelling re~sons to provide random access to the shared  channel, using event-driven communication, and to resolve contention by ar-  bitration, providing a queue where pixels wait their turn. these choices are256 neuromorphic systems engineering  based on the assumption that activity in neuromorphic systems is clustered in  time and in space.  first, unlike sequential polling, which rigidly allocates a fixed fraction of the  channel capacity to each quantizer, an event-driven channel does not service  inactive quantizers. instead, it dynamically reallocates the channel capacity  to active quantizers and allows them to samples more frequently. despite the  fact that random access comes at the cost of using log 2 n wires to transmit  addresses, instead of just one wire to indicate whether a polled node is active  or not, the event-driven approach results in a lower bit rate and a much higher  peak sampling rate when activity is sparse.  second, an arbiterless channel achieves a maximum throughput of only 18%  of the channel capacity, with an extremely high collision rate of 64 percent.  whereas an arbitered channel can operate at 95% capacity without any losses  due to collisions--but its latency and temporal dispersion is 10 times the cycle  period. thus, unless the cycle-time of the arbiterless channel is 5 times shorter,  the arbitered channel will offer higher performance in terms of the number of  spikes that get through per second. furthermore, the cycle-time of the ar-  biterless channel must be even shorter if low error rates are desired, as failure  probabilities of 5 percent require it to operate at only 2.5 percent of its capac-  ity. a comparable error in timing precision due to temporal dispersion in the  arbitered channel occurs at 84.8% of the channel capacity, using the numbers  given in section 11.3.3.  and third, the arbitered channel scales much better than the nonarbitered  one as the technology goes to finer feature sizes, yielding higher levels of in-  tegration and faster operation. as the number of neurons grows, the cycle  time must decrease proportionately in order to obtain the desired throughput.  hence, there is a proportionate decrease in queueing time and in temporal  dispersion--even though the number of cycles spent queueing remains the un-  changed when the same fraction of the channel capacity is in use. whereas the  collision probability remains unchanged under the same conditions.  i described the design and operation of an event-driven, arbitered interchip  communication channel that reads out pulse trains from a 64 x 64 array of  neurons on one chip and transmits them to corresponding locations on a 64 x 64  array of neurons on a second chip. this design acheived a threefold to fivefold  improvement over the first-generation design [12] by introducing three new  enhancements.  first, the channel used a three-stage pipeline, which allowed up to three  address-events to be processed concurrently. second, the channel exploited  locality in the arbiter tree by picking the input that was closest to the previously  selected input--spanning the smallest number of levels in the tree. and third,  the channel exploited locality in the row-column organization by sending all  requests in the selected row without redoing the arbitration between columns.  i identified three inefficiencies and one bug in my implementation, and i  suggested circuit modifications to address these issues. first, to reduce the  propagation delay of the acknowledge signal, we must remove the and gateneuromorphic chips 257  at the top of the arbiter tree, and disable the select signals at the bottom of  the arbiter tree instead. second, to reset the column and row wired-or lines  while the receiver is latching the address-event and activating the acknowledge  signal, we must remove the input to the wired-or from the arbiter and redesign  the row/column c-element. third, to decrease the time the selected row takes  to drive its requests out on the column lines, we must double the size of the  pull-down devices in the pixel. and fourth, to fix the multiple-row-selection  bug, we must guarantee that the row request signal is stable by disabling all  the neurons in a row whenever that row is selected.  these modifications will provide error-free operation and will push the ca-  pacity up two and a half times, to 6.4ms/s. according to my calculations, for  neurons with a mean latency of 2ms, a coefficient of variation of 10%, and an  firing-rate attenuation factor of 40, this capacity will be enough to service a  population of up to 82,000 neurons.  however, as the number of neurons increases, the time it takes for the pixels  in the selected row to drive the column lines increases proportionately. as this  interval is a significant fraction of present design's cycle time (38% in the mod-  ified design), the desired scaling will not be acheived unless the ratio between  the unit current and the unit capacitance increases linearly with integration  density. sram and dram scaling trends indicate that this ratio increases  sublinearly, and hence the present architecture will not scale well. we need to  develop new communication channel architectures to address this issue.  acknowledgments  this work was partially supported by the office of naval research; darpa; the  beckman foundation; the center for neuromorphic systems engineering, as part  of the national sceince foundation engineering research center program; and the  california trade and commerce agency, office of strategic technology.  i thank my thesis advisor, carver mead, for sharing his insights into the operation  of the nervous system. i also thank misha mahowald for making available layouts  of the arbiter, the address encoders, and the address decoders; john lazzaro, alain  martin, jose tierno, and tor (bassen) lande for helpful discussions on address events  and asynchronous vlsi; tobi delbriick for help with the macintosh address-event  interface; and jeff dickson for help with pcb design.  notes  1. this result is derived by assuming independent firing probabilities and approximating  the resulting binomial distribution with the poisson distribution.  2. this result is also based on the assumption that samples arrive according to a poisson  process.  3. xsel does not go high at this point because the x-arbiter has not received any requests,  as a row has not yet been selected.  4. this modification was suggested to me by tobi delbriick.  5. this solution was suggested to me by jose tierno.258 neuromorphic systems engineering  references  [1] /~. abusland, t. lande, and m. hovin. a vlsi communication architec-  ture for stochastically pulse-encoded analog signals. in proceedings of the  ieee international symposium on circuits and systems, volume 3, pages  401-404, atlanta, ga, 1996.  [2] k. boahen. retinomorphic vision systems. in int. conf. on microelec-  tronics for neural networks, volume 16-5, pages 30-39, los alamitos, ca,  1996. epfl/csem/ieee.  [3] k. boahen. retinomorphic vision systems i: pixel design. in proceedings  of the ieee international symposium on circuits and systems, volume  supplement, pages 14-19, atlanta, ga, may 1996.  [4] k. boahen. retinomorphic vision systems ii: communication channel  design. in proceedings of the ieee international symposium on circuits  and systems, volume supplement, pages 9 14, atlanta, ga, may 1996.  [5] k. a. boahen. a retinomorphic vision system. ieee micro, 16(5):30-39,  october 1996.  [6] k. a. boahen. the retinomnorphic approach: pixel-parallel adaptive am-  plification, filtering, and quantization. analog integr. circ. and sig. proc.,  13:53-68, 1997.  [7] k. a. boahen. retinomorphic vision systems: reverse engineering  the vertebrate retina. phd thesis, california institute of technology,  pasadena ca, 1997.  [8] g. cauwenberghs. a micropower cmos algorithmic a/d/a converter.  ieee transactions on circuits and systems i: fundamental theory and  applications, 42(11):913-919, 1995.  [9] l. kleinrock. queueing systems. wiley, new york ny, 1976.  [10] j. lazzaro, j. wawrzynek, , and a. kramer. systems technologies for  silicon auditory models. ieee micro, 14(3):7-15, june 1994.  [11] j. lazzaro, j. wawrzynek, m. mahowald, m. sivilotti, and d. gillespie.  silicon auditory processors as computer peripherals. ieee journal of  neural networks, 4(3):523-528, 1993.  [12] m. mahowald. an analog vlsi stereoscopic vision system. kluwer aca-  demic pub., boston, ma, 1994.  [13] c. a. mead. neuromorphic electronic systems. in proceedings of the ieee,  volume 78-10, pages 1629-1639, 1990.  [14] a. mortara, e. a. vittoz, and p. venier. a communication scheme for  analog vlsi perceptive systems. ieee journal of solid state circuits,  sc-30(6):660-669, june 1995.  [15] a. f. murray and l. tarassenko. analogue neural vlsi: a pulse stream  approach. chapman and hall, london, england, 1994.neuromorphic chips 259  [16] m. schwartz. telecommunication networks: protocols, modeling, and  analysis. addison-wesley, reading, ma, 1987.  [17] m. sivilotti. wiring considerations in analog vlsi systems with applica-  tions to field programmable networks. phd thesis, california institute of  technology, pasadena ca, 1991.  [18] i. e. sutherland. micropipelines. communications of the acm, 32(6):720-  738, 1989.  [19] a. s. tanenbaum. computer networks. prentice-hall international, 2  edition, 1989.iv neuromorphic technologysnoonb~ u~ u! sau~sqmoua p!d![ ~u!sn dq 'a~a~qa o~uoa~aia s~ u~uoo ol sza[~  -a~q x~aua ~[[m~s s~aaaa ma~sds snoaaa~ a~& "uo~aun[ ud ~ u~ aa~az~q x~aaua  ara ao 'ap~xo~p uoa~w pu~ uoa~i f uaa~aaq aauaaa~[p uo~aaunj-~ao~ ara smsn  dq 'asavqa a~uoaaaai a aqa u~muoa oa saa~aa~q gsaaua aaaaa a~ 'sa~uoaaaala aoaanp  -uoa~mas u i 'pa~nq~a~s~p uu~mz~[o a oar osiv sa~aaua a!aq~ ~ssu!punoaans a~aqa  qa~ mn~aq[i[nba l~maaql u~ aa~ sum 'anss~a aaaou u i "paanq[aas~p uuvmzaioa  oar sa~$aaua a!oqa ~ssu~punoaans apqa qa~ mn!aq!i!nba fmaaqa u! aa~ suoaaaoia  'sa~uoa~aolo aoaanpuoa~mas u i -uo~aaoguoa osa~qa 3o s~svq oq~ no dllvddu:ad  paa~lnd[u~m s[ uo~avmaoju[ 'onsg!a snoaaau pu~ sa~naa d paa~asaau~ qloq ui  â¢ saoandmoa im~$~p ano pi~nq oa xoldmo am a~qa sa[uoaaaola  soaanpuoa~mas oq~ sa~iaapun safxqd oa~aop a~i~mss v "pumsaapun pay ~ou~  am a~qa safdqd aa~aap ~u~dlaapun u~ ~msn os saop a~ 'aaios oa moq mou~ aou  op a~ l~qa smalqoad saalos anss~l snoaaau q~noq~iv "~u~ssaaoad uo~avmao~u~  fanau ~o said~au~ad ar~ pumsaapun a~ aauo 'sa~uoaaaala ra~ a~inma aouuva  o~ a~qa maasxs snoaaau aqa u~ ouop s~ ~q~ ~u~q~ou s~ oaaqa a~qa oao~laq a~  â¢ uo!a~andmoa jo maq auop~o oaom  Â£dsva - vavp pouo~a~puoa Â£iaood uo - puv ~uaaa~p ~ ~noq~ Â£~olo~qoanou moaj  ua~o i u~a am a~qa saldpu[ad i~auam~punj aa~ aaaqÂ£ "pu~asaopun aou op 'saoou  -tsua pu~ sas~auads ~ 'am lvql sÂ£~ ut os op ~oqa ~u~ua~o i puv 'ioaauoa aoaom  '~u~ssaaoad qaaads pu~ a~m~ us smaiqoad pasod-ii ~ 'am~ faa u~ 'aalos smaas*s  snoaaa n 'smstu~ao ~ud~ i jo smaas*s snoaaau oqa u~ posn sald~au[ad fuo~a~z~u  -~$ao pu~ i~uo~avandmoa aqa xoidmo l~qa smaasxs a~uoaaaoio pi[nq oa s~ ivo$ an 0  noi.l3noo~i.lni ['~[  n pa" uo~u!qse~'s~)opo!p  0Â§~-Â§61:86 vaa 'ala.:l-eas 0s~Â§Â£ xo8 'lleh ~a~s ~t  uo~u~qse~ jo ~s~aa~u r aq$  ~u~jaau~u 3 pue a~uaps ja~nd~o d jo ~ua~eda o aql  o!~o!(1 sphd  gi264 neuromorphic systems engineering  solution. in both systems, when the height of the energy barrier is modulated,  the resulting current flow is an exponential function of the applied voltage.  both systems use this principle to produce devices that exhibit signal gain.  transistors use populations of electrons to change their channel conductance,  in much the same way that neurons use populations of ionic channels to change  their membrane conductance.  we believe that the disparity between the computations that can be done by  a digital computer and those that can be done by the nervous system is a conse-  quence of the way that the underlying physics is used to effect the computation.  the state variables in both electronic and nervous systems are analog. they  are represented in electronic systems by electric charge, and in nervous systems  by electric charge or by chemical concentrations. the mechanisms by which  each systems manipulates its state variables to do computation, however, are  vastly different. in a digital computer, we ignore most of the available states  in favor of the two binary-valued endpoints: we achieve noise immunity at the  expense of dynamic range. the nervous system retains the analog dynamic  range, achieving noise immunity by adjusting the signal-detection threshold  adaptively. digital machines quantize their analog inputs, and use restoring  logic at every computational step. nervous systems perform primarily analog  computations, and quantize the computed result.  unfortunately, we do not know what computational primitives neural sys-  tems use, how they represent information, or what their organizing principles  are. however, because semiconductor electronics allows us to apply, at a high  level of integration, a device physics similar to that used by neural tissue,  we conclude that we should be able to build electronic circuits that mimic the  computational primitives of nervous systems, and that we should be able to use  these circuits to explore the organizational principles employed by neurobiology.  we call the approach silicon neuroscience: the development of neurobiologically  inspired silicon learning systems.  our predecessors began these investigations by modeling two of the sensory  organs available to neural systems: the retina and the cochlea [1, 6, 7, 24].  the silicon retina and cochlea are now well developed and mimic some of the  sensory preprocessing performed by living organisms. the papers in this part  both extend the existing retina and cochlea models, and chart new pathways  for investigation.  in "winner-take-all networks with lateral excitation," by giacomo indiveri,  the author describes an extension to the class of winner-take-all networks orig-  inally presented by j. lazzaro, et. al., in [43]. this improved circuit is certain  to become a valuable building block in neuromorphic vlsi design, and has  particular relevance for retinal image-processing systems. in "a low-power  wide-linear-range transconductance amplifier," by rahul sarpeshkar, richard  f. lyon, and carver mead, the authors describe and analyze a wide-range  transconductance amplifier applicable to analog-vlsi silicon cochleae. this  amplifier permits the construction of artificial, neurally inspired silicon cochleae  with a much wider dynamic range than was possible previously.from neurobiology to silicon 265  other reseoxchers, myself included, are beginning to model what is perhaps  the most remarkable aspect of living organisms - their abilities to adapt and to  learn. in "floating-gate mos synapse transistors," by chris diorio, bradley a.  minch, paul hasler, and carver mead, we describe compact, single-transistor  devices that, like neural synapses [4], implement long-term nonvolatile analog  memory, allow bidirectional memory updates, and learn from an input signal  without interrupting the ongoing computation [7, 60]. although we do not  believe that a single device can model the complex behavior of a neural synapse  completely, our synapse transistors do implement a local learning function.  finally, still other researchers are exploring neural function in silicon inte-  grated circuits. neurons are the nervous system's primary computing elelnents:  they appear to perform and to learn from spatio-temporal input-signal cor-  relations. in "neuromorphic synapses for artificial dendrites," by wayne c.  westerman, david p. m. northmore, and john g. elias, the authors describe  variable-weight neuromorphic synapses on artificial dendrites. the synapses,  dendrites, and the associated interface circuitry permit the investigation and  comparison of correlative adaptation mechanisms in a mixed software-hardware  model of neural function. by fabricating and testing structures such as these,  the authors may provide valuable insights into adaptation and learning in bio-  logical and artificial neural systems.  as a research community, my colleagues and i generally believe that if we can  understand the principles on which biological information-processing systems  operate, then we can build circuits and systems that deal naturally with real-  world data. our goal, therefore, is to consider the computational principles on  which neural systems operate, and to model and understand those principles in  the silicon medium. we consider these papers to represent a small step toward  achieving our goal.  references  [1] k. boahen. retinomorphic vision systems. in int. conf. on microelectronics  for neural networks, volume 16-5, pages 30-39, los alamitos, ca, 1996.  epfl/csem/ieee.  [2] c. diorio, p. hasler, b. a. minch, and c. mead. a single-transistor silicon  synapse. ieee trans. electron devices, 43(11):1972-1980, 1996.  [3] p. hasler, c. diorio, b. a. minch, and c. mead. single transistor learning  synapses. in advances in neural information processing systems 7, pages  817-824. mit press, cambridge, ma, 1995.  [4] c. koch. computation and the single neuron. nature, 385(6613):207-210,  january 1997.  [5] j. p. lazzaro and c. mead. circuit models of sensory transduction in  the cochlea. in mead and ismail, editors, analog vlsi implementation of  neural systems, pages 85-101. kluwer academic publishers, norwell, ma,  1989.266 neuromorphic systems engineering  [6] m. mahowald and c. mead. the silicon retina. scientific american,  264(5):76-82, 1991.  [7] c. a. mead. analog vlsi and neural systems, pages 179-192, 279-302.  addison-wesley, reading, ma, 1989.  [8] r. sarpeshkar, lyon r. f., and c. a. mead. an analog vlsi cochlea with  new transconductance amplifiers and nonlinear gain control. in proc. ieee  intl. conf. on circuits and systems, volume 3, pages 292-295, atlanta, may  1996.3 a low-power  wide-linear-range  transconductance amplifier  rahul sarpeshkar i, richard f. lyon 2, and carver mead 3  1department of biological computation,  bell laboratories, murray hill, nj 07974  ra h ul~physics.bell-labs.com  :~foveonics inc., i0131-b bubb rd., cupertino ca 95014  3physics of computation laboratory, california institute of technology  13.1 introduction  in the past few years, engineers have improved the linearity of mos transcon-  ductor circuits [2, 5, 10, 11, 19, 20, 26, 28, 29, 32]. these advances have been pri-  marily in the area of above-threshold, high-power, high-frequency, continuous-  time filters. although it is possible to implement auditory filters (20hz-20khz)  with these techniques, it is inefficient to do so. the transconductance and cur-  rent levels in above-threshold operation are so high that large capacitances or  transistors with very low w/l are required to create low-frequency poles, and  area and power are wasted. in addition, it is difficult to span 3 orders of mag-  nitude of transconductance with a square law, unless we use transistors with  ungainly aspect ratios. however, it is easy to obtain a wide linear range above  threshold.  in above-threshold operation, identities such as (x - a) e - (x - b) e = (b -  a) (2x - a - b) are used to increase the wide linear range even further. in bipo-  lar devices where the nonlinearity is exponential, rather than second-order, it  is much more difficult to completely eliminate the nonlinearity. the standard  solution has been to use the feedback technique of emitter degeneration, which  achieves wide linear range by reducing transconductance, and is described by  gray [7]. a clever scheme for widening the linear range of a bipolar transcon-  ductor that cancels all nonlinearities up to fifth order, without reducing the268 neuromorphic systems engineering  fi v.  w  gm  ]1 n  3g ~ vo s  i  i <m  i lout  >~  i<m  figure 13.1 the wide linear range transconductance amplifier. the figure shows the  transconductance amplifier with v+ and v_ well-input voltages and the tiilot~t current output.  the voltage vb sets the bias current of the amplifier and the voltage vos allows a fine  adjustment of its offset if necessary. the s transistors reduce the transconductance of the  differential-pair w transistors through source degeneration. the gm transistors reduce the  transconductance of the w transistors through gate degeneration. they also serve to mirror  the differential-pair currents to the output of the amplifier. the b transistors implement  bump linearization. the m transistors form parts of the current mirrors in the circuit.  transconductance, has been proposed by wilson [31]. a method for getting  perfect linearity in a bipolar transconductor by using a translinear circuit and  a resistor has been demonstrated by chung [3]. both of the latter methods,  however, require the use of resistors, and ultimately derive their linearity from  the presence of a linear element in the circuit. resistors, however, cannot  be tuned electronically, and require special process steps. various authors  have used an mos device as the resistive element in an emitter-degeneration  scheme to make a bicmos transconductor--for example, the scheme proposed  by sanchez [22]. another bicmos technique, reported by weimin [13], uses  an above-threshold differential pair to get wide linearity, and scales down the  output currents via a bipolar gilbert gain cell, to levels more appropriate for au-  ditory frequencies. above-threshold differential pairs, however, result in lower  open-loop gain and higher voltage offset, and techniques such as cascode mir-  rors are required to improve these characteristics. cascode mirrors, however,a low-power wide-linear-range transconductance amplifier 269  degrade dc output-voltage operating range and consume area. in addition,  above-threshold operation results in higher power dissipation.  subthreshold mos technology, like bipolar technology, is based on expo-  nential nonlinearities. thus, it is natural to employ source-degeneration tech-  niques. methods for getting wider linear range that exploit the early effect in  conjunction with a source-degeneration method are described by arreguit [1].  the early voltage is, however, a parameter with high variance across transis-  tots; thus, we cannot expect to get good transconductance matching in this  method. further, such schemes are highly offset prone, because any current  mismatch manifests itself as a large voltage mismatch due to the exceptionally  low transconductance.  the simple technique of using a diode as a source-degeneration element  extends the linear range of a differential pair to about +150 mv, as described  by watts [90]. however, it is difficult to increase this linear range further  by using two stacked diodes in series as the degeneration element--the wider  linear range that is achieved is obtained at the expense of a large loss in dc-  input operating range. if we operate within a 0 to 5v supply, the signal levels  remain constrained to take on small values, because of the inadequate dc input  operating range.  three techniques for improving the linear range of subthreshold differen-  tial pairs have been described in [6]. the authors define the linear range  to be the point where the transconductance drops by 1%. by that defini-  tion, the linear range of a conventional transconductance amplifier described  by iout = is tanh (x/vl) is vl/5. their best technique achieved a value of  vl = 584 mv, and involved expensive common-mode biasing circuitry. in con-  trast, our technique yields a vl of 1.7 v, and involves no additional biasing  circuitry.  in [21], a 21-transistor subthreshold transconductance amplifier is described.  from visual inspection of their data, the amplifier has a vl of about 700 mv.  they estimate from simple theoretical calculations that the effective number  of shot-noise sources in their circuit is about 20. in contrast, our 13-transistor  circuit has a vl of 1.7 v, and the effective number of shot-noise sources in our  circuit is around 5.3 (theory) or 7.5 (experiment).  we can solve the problem of getting wider linear range by interposing a  capacitive divider between each input from the outside world and each input  to the amplifier. some form of slow adaptation is necessary to ensure that the  de value of each floating input to the amplifier is constrained. this approach,  as used in an electronic cochlea, is described in [14]; it did not work well in  practice because of its sensitivity to circuit parasitics. as we shall see later,  capacitive-divider schemes bear some similarity to our scheme. we shall discuss  capacitive-divider techniques in section 13.5.4.  to get low transconductance, we begin by picking an input terminal that is  gifted with low transconductance from birth: the well. we reduce the transcon-  ductance further by using source degeneration, and a new negative-feedback  technique, which we call gate degeneration. finally, we use a novel technique,270 neuromorphic systems engineering  which we call bump linearization, to extend the linear range even more; bump  circuits have been described in [4]. the amplifier circuit that incorporates all  four techniques is shown in figure 13.1.  in section 13.2, we present all the essential ideas and first-order effects that  describe the operation of the amplifier. we describe second-order effects, such  as the common-mode and gain characteristics, in section 13.3. we discuss the  operation of this amplifier as a follower-integrator filter in section 13.4. we  elaborate on noise and dynamic range in section 13.5. in section 13.6, we con-  clude by summarizing our contributions. appendix a contains a quantitative  treatment of common-mode effects on the amplifier's transconductance. sec-  tion a.1 describes the effects of changing transconductance; section a.2 is on  the effects of parasitic bipolar transistors present in our well-input amplifier.  normally, the amplifier operates in the 1v to 5v range, where these bipolar  transistors are inactive.  13.2 first-order effects  we begin by expressing basic transistor relationships in a form that will be  useful in our paper. we use standard ieee convention for large-signal (ids),  dc (ids), and small-signal (iris) variables.  13.2.1 basic transistor relationships  the current in a subthreshold mos well transistor in saturation is given by  (_kvcs~ ( (1-k)vws) (13.1) ids "= io exp ut ] exp -- ~t ]'  where vgs and vws are the gate-to-source and well-to-source voltage, respec-  tively; ~ is the subthreshold exponential coefficient; i0 is the subthreshold  current-scaling parameter; ut ---- kt/q is the thermal voltage; and vds >> 5ut.  eq. (13.1) illustrates that the gate affects the current through a ~ expo-  nential term, whereas the well affects the current through a 1 - ~ exponential  term. thus, when the gate is effective in modulating the current, the well is  ineffective, and vice versa. by differentiating eq. (13.1), we can easily show  that the gate, well, and source transconductances are  oids __ iris - ids  9gt = ovg -- fg = --~ ut '  oids _~ = _(1_~)~ (13.2)  gwl -= ovw -- v~. ut '  oids __ iris ids  bs ~---- ors -- v~ ~-- ut '  respectively. thus if and only if ~ > 0.5--which is almost always the case--then  the well transconductance has a lower magnitude than the gate transconduc-  tance, and the well is preferable over the gate as a low-transconductance input.  it is convenient to work with dimensionless, small-signal variables: if id and  vd are arbitrary small-signal variables, and we define the dimensionless variables  i = id/id, v = vd/ut, then a relation such as id = gdvd = ~idvd/ut takes thea low-power wide-linear-range transconductance amplifier 271  v s  (  v s ?,,o, e  ~i e~s (v,-v 1 - ~)i ~s (v,-v ~ )  vd v d )  +  vg  (a) (b) (c)  figure 13.2 signal-flow diagram for a saturated well transistor. (a) a well transistor  with marked voltages and currents. (b) the small-signal equivalent circuit for the well  transistor. (c) the signal-flow diagram represents the dimensionless relationships between  the small-signal variables of the transistor. in our paper it shall prove to be the most  useful and insightful way of analyzing transconductance relationships, rather than the more  conventional circuit representation shown alongside it. in the signal-flow diagram, it is  understood that all currents are normalized by ids, all voltages are normalized by ut, and  all transconductances are normalized by ids/ut. thus, the small-signal relationship of  the transistor takes the simple form ids = vs -- evg -- (1 -- e)v w.  simple form i = ~v. we notice then that ~ plays the role of a dimensionless  transconductance; that is, ~ = 9d/(id/ut) is the dimensionless transconduc-  tance that we obtain by dividing the real transconductance gd by id/ut. we  shall use the dimensionless variable forms to do most of our calculations, and  then shall convert them back to the real forms. for convenience, we denote the  dimensionless variable by the same name as that of the variable from which it  is derived. thus, eq. (13.2) when converted to its dimensionless form, simply  reads ggt = -n, gwl = -(1 - ~), andros = 1.  figure 13.2 shows a well transistor, its small-signal-equivalent circuit, and a  signal-flow diagram that represents its small-signal relations. in this paper, we  shall ignore the capacitances that would be represented in a complete small-  signal model of the transistor.  13.2.2 transconductance reduction through degeneration  the technique of source degeneration is well known, and was first used in  vacuum-tube design; there it was referred to as cathode degeneration, and was  described by landee [12]. later, it was used in bipolar design, where it is  referred to as emitter degeneration [7]. the idea behind source degeneration  is to convert the current flowing through a transistor into a voltage through a  resistor or diode, and then to feed this voltage back to the emitter or source of  the transistor to decrease its current.272 neuromorphic systems engineering  v c  ~ ids 1/~p, -  i~  ~ (1-,<)(v~-v  ~,ia~  1/~ n  (a) (b) v s  w)  â¢ â¢  vg  (c)  figure 13.3 transconductance reduction through degeneration. (a) a half circuit for  one differential arm of our amplifier, if we ignore the b transistors of figure 13.1 for now.  (b) a small-signal equivalent circuit for this configuration. (c) a signal-flow diagram for  this configuration. the signal-flow diagram shows that the transconductance reduction is  achieved through two parallel feedback loops. the top loop is due to source degeneration  and the bottom loop is due to gate degeneration. a more detailed description of the feedback  concepts assosciated with transconductance reduction may be found in section 13.2.2.  gate degeneration has never been reported in the literature to our knowl-  edge. this lacuna probably occurs because most designs use the gate as an  input and thus never have it free to degenerate. the vacuum-tube literature,  however, shows familiarity with a similar concept, called screen degeneration,  as described in landee [12]. the idea behind gate degeneration is to convert the  current flowing through a transistor into a voltage through a diode, and then  to feed this voltage back to the gate of the transistor to decrease its current.  figure 13.3a shows a half-circuit for one differential arm of the amplifier of  figure 13.1, if we neglect the b transistors for the time being. the source-  degeneration diode is the pfet connected to the source of the well-input tran-  sistor; the gate-degeneration diode is the nfet connected to the drain of the  well-input transistor. the gate-degeneration diode is essentially free in our  circuit, because it is part of the current mirror that feeds the differential-arm  currents to the output. the voltage vc represents the common-node voltage  of the differential arms. in differential-mode ac analysis, the common-node  voltage is grounded, as explained in the following paragraph.  in figure 13.1, if v+ = v_, and the amplifier is perfectly matched, then a  quiescent current of !~ flows through each branch of the amplifier and iout  will be 0. if we now vary the differential voltage, vd = v+ -- v_, by ~t small  amount, the current changes by io~,t = gvd, where g is the transconductancea low-power wide-linear-range transconductance amplifier 273  of the amplifier. we would like to compute g. if we apply vd, such that  v+ changes by +~ and v_ changes by -~ then the common node of the 2'  two differential halves (the source of the s transistors) does not change in  voltage. for the purposes of small-signal analysis, we can treat the common  node as a virtual ground. thus, if gh is the transconductance of the half-circuit  shown in figure 13.3a, the output current is gh~ - (-gh~ t) = ghvd. hence,  the transconductance of the half-circuit, biased to the current level of ib/2, is  the transconductance of the amplifier.  the circuit of figure 13.3a yields the small-signal circuit of figure 13.3b: the  source-degeneration diode is represented by a dimensionless resistor of value  1/np, the gate-degeneration diode is represented by a dimensionless resistor  of value 1/nn, the gate-controlled current source of figure 13.2 is represented  by a dimensionless resistor of value 1/n (the gate is tied to the drain), and  the well-controlled current source of figure 13.2 is represented by a dependent  source, as shown.  the left half of figure 13.3c represents the signal-flow diagram for the well-  input transistor, as derived in figure 13.2. the right half of figure 13.3c  represents the blocks due to the source or gate degeneration diodes feeding  back to the source or gate. thus, we have two negative-feedback loops acting  in parallel to reduce the transconductance. one loop feeds back the output  current to the source via a -1/e;p block; the other loop feeds back the output  current to the gate via a 1/nn block. since the magnitude of the loop gains of  ~ and a~- '~ the source-degeneration and gate-degeneration loops are a8 = ~ - ~--~  respectively, the well transconductance is attenuated by 1 â¢ that is to l+as+ag ~  say, the transconductance is  1--~ .. (13.3) g--l+~-~+--~  we multiply the dimensionless transconductance thus computed by ~ to get 2ut  the actual transconductance, since ids in each differential arm is !~ 2"  the w, s, and g transistors of each differential arm may be regarded as a  single transistor with i-v characteristics given by  i (x e -(vs-gvw)/vr.  by following the steps outlined by mead [17], we can easily derive that  (g(v+_nv_)~, (13.4) iour = ib tanh \ 2ut ]  where g is given by eq. (13.3).  13.2.3 bump linearization  bump linearization is a technique for linearizing a tanh and extending the linear  range of a subthreshold differential pair [15]. we shall first explain how it works274 neuromorphic systems engineering  for the simple differential pair; then, we shall extend this explanation to our  amplifier in a straightforward fashion.  a bump differential pair has, in addition to its two outer arms, a central  arm containing two series-connected transistors [4]. the current through the  transistors in the central arm is a bump-shaped function of the differential  voltage, so we call these transistors bump transistors. thus, the differential  output current from the outer two arms, i, is the usual tanh-like function of  the differential voltage, v, except for a region near the origin, where the bump  transistors steal current. by ratioing the w/l of the bump transistors to be w  with respect to the transistors in the outer arms, we can control the properties  of this i-v curve. a small w will essentially leave it unchanged. a large w will  cause a fiat zone near the origin, where there is current stealing. the larger  the w, the larger the width and flatness of the zone. at intermediate values  of w, the expansive properties of the curve due to the bump compete with the  compressive properties of the curve due to the tanh, and a curve that is more  linear than is a tanh is obtained. at w = 2, the curve is maximally linear.  for a simple subthreshold bump amplifier with bump scaling w, the differ-  ential output current can be shown to be  where sinh x  io~t - fl + cos lhx' (13.5)  w z = l+g, (13.6)  q~(v+ - v_)  x - kt (la.7)  the bias current is assumed to be 1, without loss of generality. simple calculus  shows that the first and second derivatives of eq. (13.5) are  diout 1 + ~ cosh x  --  dx (/~ + cosh x) 2'  d2iout _ sinhx(/~2-zcoshx-2)  dx 2 (~ + coshx) 3 (13.s)  (f12 _ flcoshx- 2) ~ 0 (13.9)  for all x. note that in deriving eq. (13.9), we have used the facts that the  denominator term of eq. (13.8) is always positive, and that sinh x changes sign  at only the origin. the worst possible case for not meeting the constraint given if we require that the iout-vs.-x curve have no points of inflection in it except  at the origin, then it must always be convex or concave in the first or third  quadrant; this requirement is necessary to ensure that there are no strange  kinks in the i-v curve. in eq. (13.8), we must then havea low-power wide-linear-range transconductance amplifier 275  by eq. (13.9) occurs when coshx is at its minimum value of 1 at x = 0. if we  set cosh x = 1 and solve the resulting quadratic for 3, we obtain  3_<2,  =~w < 2.  --  thus, at w = 2, we are just assured of satisfying this constraint. at w = 2,  the iout-vs.-x curve is maximally linear: if we taylor expand eq. (13.5) at  3 = w = 2, we find that it has no cubic-distortion term. in comparison, the  function tanhx/2, which is to what eq. (13.5) reduces when w = 0 or 3 = 1,  has cubic distortion.  tanh x x x 3 x 5 17x 7  -- + -- + .. (13.10) 2 2 24 240 40320  sinh x x x 5 x 7 x 9  -- -- + -- + .. (13.11) 2 + cosh x 3 540 4536 77760  at x = 1, the tanh function has cubic distortion of approximately 8%, as com-  pared to the linearized tanh function which has no cubic distortion whatsoever,  but has only fifth-harmonic distortion of less than 1%. thus, we have linearized  the tanh. we shall show later, in section 13.5, that bump linearization is a par-  ticularly useful technique because it increases the linear range of an amplifier  without increasing that amplifier's noise.  our circuit topology in figure 13.1 implements a wide-linear-range bump  differential pair by ratioing the w/l of the the nfet b transistors to be w with  respect to the w/l of the nfet gm transistors. however, the mathematical  analysis of the circuit is identical to that of a simple bump differential pair if  we replace the n of eq. (13.7) with the g of eq. (13.3).  13.2.4 experimental data  in summary, from eqs. (13.3) and (13.4), we get the transfer function of the  amplifier with the b transistors absent:  io~t = ib tanh(vd/vÂ¢), (13.12)  vd = v+-v_,  vÂ¢ : (2 t/q)(1 + . (13.13)  prom eqs. (13.3), (13.4), (13.5), (13.6), and (13.7), the transfer function of the  overall amplifier has the form  sinh(2x) (13.14)  io~t = ib ~ + w/2 + cosh(2x)'  where x = vd/vÂ¢. the w/l of the b transistors is w times the w/l of the  gm transistors.276 neuromorphic systems engineering  we fabricated our transconductance amplifier in a standard 2#m cmos n-  well process, and obtained data from it. we have also used the amplifier in  a working silicon cochlea [24, 25]. figure 13.4a shows experimental data for  our amplifier for values of w = 0, 1.86, and 66, fitted to eq. (13.14). note  that, even at rather large w, the nonlinear characteristics are still gentle. this  property ensures that even fairly large mismatches in w do not degrade the  operation of the amplifier significantly. for w ~ 2, figure 13.4b shows that  the simple tanh fit of eq. (13.12) is a good approxinmtion to eq. (13.14) if  vl ~ (3/2)vl--that is to say, from 1.16 v for w = 0 to 1.7 v for w -- 2.  considering the leading-order terms of eqs. (13.10) and (13.11), the factor of  3/2 seems natural. however, we shall show, from the follower-integrator data  of section 13.4, that the tanh approximation is inadequate for large differential  inputs, because the curve is more linear than is that of a tanh.  we verify eq. (13.13) experimentally in section a.1. since ~ varies with  the well-to-gate voltage, and consequently with the common-mode voltage, the  verification is involved; we have relegated the details to the appendix.  13.3 second-order effects  we shall now discuss several second-order effects in our amplifier. we begin by  describing the common-mode characteristics.  13.3.1 common-mode characteristics  below an input voltage of 1v, the well-to-source junction becomes forward  biased, and the parasitic bipolar transistors, which are part of every well tran-  sistor, shunt the current of the amplifer to ground. thus, as figure 13.5a shows,  the output current of the amplifier falls. figure 13.5b illustrates the same ef-  fects, but from the perspective of varying the differential-mode voltage, while  fixing the common-mode voltage. in this figure, we can see that changes in  ~ increase the transconductance of the amplifier as the common-mode voltage  is lowered. the increase is exhibited as a rise in the slope at the origin. at  very low common-mode voltages, at a particular differential voltage, the input  voltage for one arm or the other falls below 1v, and the bipolar transistors in  that arm begin to shunt the current to ground. the lower the common-mode  voltage, the smaller the differential voltage at which shunting begins. thus,  at low common-mode voltages the current starts to fall at small magnitudes of  differential voltage.  we clarify the bipolar effect in figure 13.6: figure 13.6a shows a verti-  cal crosssection of a well transistor in an n-well process; figure 13.6b shows  that the equivalent circuit of this well transistor contains two parasitic bipo-  lar transistors. typically, the well-to-source and well-to-drain junctions are  always reverse biased, so that these bipolar transistors are turned off. for our  amplifier, and in most cases, the source-to-drain voltage is sufficiently positive  that the bipolar transistor at the source is the one that is turned on, if any is,  whereas the one at the drain is hardly turned on. thus, in figure 13.6c, wea low-power wide-linear-range transconductance amplifier 277  2.5 x 10 .8  1.5  1  'e 0.5  ~ 0  â¢ . -0.5  n -1  o , , ,  j x--- w=66  -1.5  -2  -2.5 -~ ~ ~  differential voltage (v)  ,3  o x 10 .8  x --- w = 1.86; v l = 1.72 v ~x~  1  0.50 ~  -0.5  -1  -1.5  -2  -2.5 _~ d ~  differential voltage (v)  figure 13.4 i-v curves for the amplifier. (a) experimental data for our amplifier for  values of t~ = 0, 1.86, and 66, fitted to eq. (13.14). (b) for w ~ 9., the simple tanh-fit  of eq. (13.12) is a good approximation to eq. (13.14), if vl --~ (3/2)vl  have indicated the bipolar transistor at only the source. the bipolar effect is  described in quantitative detail in section a.2. typically, we operate the  amplifier at a common-mode voltage of about 3v, where s~nall dc offsets do not  significantly affect its transconductance, and where the action of the bipolar  transistors is negligible. when the bipolar transistors do turn on, there is no  danger of latchup, because the current that is fed to the substrate is at most  the tiny subthreshold bias current of the amplifier. our input operating range  of 1v to 5v is about the same as that of a simple subthreshold nfet differen-278 neuromorphic systems engineering  10-~0 16 , , ,  14 (a) ~,  12 differential voltage =  ~ 10  ~ ~  ~ bipolar i  0 shunting "i 14 ~ changes  -o ~ ~ ~ ~ornmen no6e v~lta~e (v)  2.5 x 10 8  2[ b 2.5 v~.~.~.  ~ o~  ~'0st \ ~//// -'i  "2"5-2 -1 0 1  differential voltage  figure 13.5 differential and common-mode curves. (a) the data shows that the current  rises as the common-mode voltage is decreased from 5v to 1v, because changes in t~ in-  crease the transconductance of the amplifier. below 1v, the well-to-source junction becomes  forward biased, and the parasitic bipolar transistors, which are part of every well transistor,  shunt the current of the amplifer to ground. (b) the same effects as in (a) but from the  perspective of varying the dif[erential-mode voltage, while fixing the common-mode voltage.  further details may be found in section 13.3.1.  tial amplifier. these amplifiers also show transconductance changes, due to n  changes, that are most abrupt at low common-mode voltages.  as the common-mode input voltage of our amplifier is decreased, the well-  to-gate voltage falls, the depletion region beneath the channel of the transistor  shrinks, n decreases, the transconductance of the amplifier rises from eq. (13.3),  and, at a given differential voltage, there is more current. thus, the data  of figure 13.5a show that the current rises as the common-mode voltage is  decreased from 5v to 1v. note also that the transconductance changes are  greatest near 1v where the depletion region is thin, and are least near 5v,a low-power wide-linear-range transconductance amplifier 279  v w v s v g v d  (a) v w v s vg v d  n- nwell  p- substrate  (b)  (c) vbez>-- vso v g  j !  1 ovd  0 vw  v+ ~ vo s  iout  __>_~  figure 13.6 the effect of the parasitic bipolar transistor. (a) a vertical crosssection of a  well transistor in an n-well process. (b) the equivalent circuit of this well transistor contains  two parasitic bipolar transistors. (c) the bipolar transistor at the source is responsible for  shunting the amplifier's current to ground at low common-mode voltages, while the bipolar  transistor at the drain plays a negligible part.  where the depletion region is thick. the n changes are described in quantitative  detail in section a.1.280 neuromorphic systems engineering  13.3.2 bias-current characteristics  figure 13.7a shows the bias-current characteristics of our amplifier as a func-  tion of the bias voltage vb. the amplifier is capable of operating with little  loss in common-mode range from bias currents of pa to #a. figure 13.7b illus-  trates that the linear range begins to increase at above-threshold current levels,  because of an increase in the i/gm ratio of a transistor at these levels. that is  to say, the characteristic scaling voltage in eq. (13.13) begins to increase above  2kt/q.  note that the changes in ~ observed with bias current and common-mode  voltage are not unique to our amplifier: they occur in every subthreshold  mos transistor. by measuring the slope of the i-v curve in a simple nfet  differential pair, we obtained changes in ~ with common-mode voltage and  with bias current. figure 13.8 shows these data. note that ~ changes are  most abrupt at low common-mode voltages--that is to say, at low gate-to-bulk  voltages; ~ also decreases with increasing bias current.  13.3.3 gain characteristics  the voltage gain of the amplifier is determined by the ratio of its transconduc-  tance gm to its output conductance go. when w = 0, the transconductance  is is/vl. the output conductance is is/2vo, where v0 is the effective early  voltage of the output m transistors in figure 13.1. the effective early volt-  age of the output m transistors is the parallel combination v~v~/(v~ + v~),  where v0 ~ and v0 ~ are the early voltages of the n and p output transistors,  respectively. when w = 2, g,~ is given by is/(3/2vl), since the linear range  increases by a factor of 3/2. the value of go is given by is/3vo, as the out-  put current in each arm falls from is/2 to is/3. thus, effectively, the w = 2  case corresponds to the w = 0 case, with is being replaced by 2ib/3. so the  gain is unchanged, because g,~ and go change proportionately. independent of  whether w = 0 or w = 2, the gain is 2vo/vl. figure 13.9a shows that the con-  clusions of the previous paragraph are borne out by experimental data. as the  common-mode voltage is lowered, the gain increases because of the increasing  transconductance, or decreasing vl. the w = 0 and w = 2 amplifiers have al-  most identical gain. figure 13.9b illustrates that, as a function of bias current,  the gain initially rises, because the early voltage increases with current. as the  bias current starts to go above threshold, however, vl drops faster than v0 in-  creases, and the gain starts to fall. from figure 13.7b and figure 13.9b, we see  that the location of the gain peak is at a bias current (10 na) where the linear  range starts to change significantly, as we would expect. figure 13.9b shows,  however, that the w = 0 and w = 2 cases do not have identical gains at all bias  currents, although the gains are similar. we attribute these small differences  to differences in g,~ and go at bias-current levels of ib/2 versus is/3.  for the amplifier for which data were taken, the output transistors had a  channel length of 16#m in a 2#m n-well orbit analog process. if higher gain is  desired, these channel lengths should be increased, or the positive and negativea low-power wide-linear-range transconductance amplifier 281  10 +6  10 -7  10 .8  Â£  ~ 10-9  ~9  .~ 10 1Â°  r~  10 -11  i0-,~' 0.6 (a) ~o o  ~ 10 = 3,2 x~,. k = 0"67 1019 a  0:7 0:8 0:9 ~ t:l bias voltage (v)  (vdd- vs) 1.2  ._  e 3  (b)  ~.5  2  x  1.5  i o  0.5 10 -12 o---w= 0  x---w= 1.86  x  x x x x x  0 0 0 0 0 0 x  x  o  o  10l10 ' 10. 8  bias current (a) x o  o  o  1 -6  figure 13.7 linear range vs. bias current. (a) the bias current ib as a function of the  bias voltage vb. (b) the linear range of the amplifier vl as a function of the bias current  ib.  output currents can be cascoded via pfet and nfet transistors respectively.  13.3.4 offset characteristics  a current offset of ik can be compensated for by a voltage offset of vz = ik/g,~,  where g,~ is the transconductance of the ik -- vl relation (ik and vl are  arbitrary variables). therefore, if the fractional current offset caused by a282 neuromorphic systems engineering  common-mode voltage (y) 0.80 -  0.78 -  0.76 --  0.74 --  0.72 ~  0.70  0.68  0.66  2.2 ?  ~/ 10 7 ,,.0 0 (9~"  io s ~ ~  figure 13.8 the changes in n in a simple nfet differential pair. the changes in n  are most abrupt at low common-mode voltages, i.e., at low gate-to-bulk voltages; n also  decreases with increasing bias current.  threshold or geometry mismatch is ik/ik = ~, then the voltage offset vt is  ik/g,~ = (ik/g,~)(ik/ik) = (ik/g,~)~. the ik/gm ratio is thus the scale  factor that converts percentage mismatches to voltage offsets. a transconduc-  tance amplifier has an ib/gm ratio of of 2ut/g. thus, the same percentage  mismatch results in a larger voltage offset for an amplifier with a lower g. this  general and well-known result causes the lowering of transconductance to be  assosciated with an increase in voltage offset. however, we show next that,  although this increase in voltage offset is fundamental and unavoidable, certain  transistor mismatches matter more than do others. by designing the amplifier  carefully, such that more area is expended where mismatches are more crucial,  we can minimize the offset. in our amplifier, mismatches may arise in the two  nfet gm m mirrors, in the pfet m mirror, or between the w, s, and gm  transistors in the two arms. in a first-order analysis, the b transistors do not  affect the voltage offset, except that g is replaced by (2/3)g. we shall therefore  start our analysis with the case of w = 0, and then extend our analysis to the  w = 2 case. suppose the net mismatch due to all mirror mismatches is such  ~m ) that i+/i_ = ~-~(1 Ã· ~)/l~(1 - ~- ~ 1 Ã· ~m. the mismatch in currents  may be referred back to the input as a voltage offset of Ã·vm/2 on one input,  and a voltage offset of --vm/2 on the other input:  2 -  \ 2u:~ /a low-power wide-linear-range transconductance amplifier 283  220  200  180  -~ 160 e <  ~8 140  .-~  ~ 120  ~.)  100  80  140  120 $  e  < 100  ._ ~g  ~.~  c~ 80  60 1040 o---w= 0  x---w~2  ib=35x 10-9 a  common mode voltage (v) (a)  5  o---w = 0;  x---w~ 2; x (b) vcm = 3.0 v  o x  x  o x  x o  x o  x o x o  o  10 .9 10-8 10 .7 10-s  bias current (a)  figure 13.9 gain characteristics. (a) the gain of our amplifier as a function of the  common-mode voltage. at low common-mode voltages, the gain increases because of the  rising transconductance of the amplifier. (b) the gain of our amplifier as a function of the  bias current. initially, the gain rises because of the increasing early voltage of the amplifier,  and then falls because of the increasing linear range. a more detailed discussion may be  found in section 13.3.3.  u~r ~vm =(~m--. g  since ~ is the ik/gm ratio of the w transistors, a mismatch of 6w between  those transistors introduces a voltage offset of ~v_-~6w on the well inputs. be-  cause ut is the ik/g,~ ratio of the s transistors, a mismatch of 5s between t~n284 neuromorphic systems engineering  those transistors results in a voltage offset of ~ss on their gates. this volt-  age offset is fed back to the sources of the w transistors and is amplified by  1/(1 - ~) when referred back to the those transistors' well-inputs. similarly, a  mismatch of 5g between the g transistors causes a voltage offset of v~ on the  /~n gates of the gm transistors that is amplified by ~/(1 - ~) when referred back  to the well inputs. the total offset, vof, is just the sum of the voltage offsets  introduced by all the mismatches, and is given by  ) 1 5w + -- 5s + vof = ut 5m + 1-- ~ ~p l  ) 1 --Â£a â¢  if we use the expression from eq. (13.3) to substitute for 9, then (13.15)  - -- -- + ~ + ~w + vÂ°f 1 - ~ ~  "  we notice that the greatest contributor to the offset is the mirror mismatch  5m. thus, it is important that all mirror transistors in the amplifier be big.  the matching of the s transistors is more important than is the matching  of the w transistors since 1/~p > 1. the matching of the gm transistors is  more important than is that of the w transistors if ~/~n > 1, which is usually  the case; for ~/~ ratios exceeding 1/~p, it is also more important than the  matching of the s transistors. the amplifier is thus laid out with big pfet m  tranistors, moderate gm transistors, moderate nfet m transistors (to match  the gm transistors), moderate s transistors and small w transistors. it is  interesting that the transistors that matter the least for matching are the input  transistors.  if the b transistors are present, a similar analysis shows that the matching  of the m transistors becomes even more important, because the g in eq. (13.15)  is replaced by (2/3)g. in eq. (13.16), we then have 5m ~ (3/2)5m.  the sizes of the transistors that we used for the amplifier were 12/12 for the  w transistors, 12/12 for the bias transistor, 29/12 for the s transistors, 14/16  for the gm and nfet m transistors, and 26/16 for the pfet m transistors.  the total dimensions of the amplifier were 85~m x 190~m in a 2~m process.  our random offset is about 5 to 10 inv.  due to early voltage effects, the c~caded gains of the gm-m mirror and the  pfet m mirror in the positive-output-current path, exceed those of the gm-  m mirror in the negative-output-current path. hence, with vos = vdd, the  amplifier typically has a systematic positive voltage offset that is on the order  of 10 to 20 inv. the voltage vos is operated approximately 0.5 mv below vdd,  to cancel this offset. we have also built automatic offset-adaptation circuitsa low-power wide-linear-range transconductance amplifier  1.5 285  ..~ 1 >  v  ~ 0.5  0 >  ~ 0 o  ---- -0.5:  0 ~. vcm = 1.5 v, 3.0 v ,,~,,,~  v b = 4.0 v . i~~  ~ " vc,= ].s~,,. v (a)  ~ 0.05  offset control voltage (v)  (vdd - v os)  1.5  ..-.. 1 >  v  --~ 0.5 0 >  o  0 = -0.5 2 vdc = 3.0 v  v b = 4.20 v (top curve) ~  v b = 4.00 v (middle curve) jjj  ~ ,  (b)  (~ 0.05  offset control voltage (v)  (vdd -v os)  figure 13.10 offset characteristics. (a) the offset characteristics of the amplifier at  two different common-mode voltages. (b) the same characteristics at three different bias  currents. more details may be found in section 13.3.4.  to control the offset of our amplifiers; we shall not elaborate on those schemes  here.  the data of figure 13.10a show the offset voltage of a follower built with this  amplifier as a function of the vos voltage. the two current curves correspond  to two different common-mode voltages. the slope of the input-output relation286 neuromorphic systems engineering  is the ratio of the source transconductance at the vos-input 9s to the transcon-  ductance of the overall amplifier g. at high common-mode voltages, the offset  is more sensitive to the vos voltage, because g decreases with common-mode  voltage. figure 13.10b shows that the offset voltage is less sensitive to the vos  voltage at high bias-current levels, because gs decreases faster than does g with  bias current.  13.4 follower-integrator characteristics  one of the most common uses of transconductance amplifiers is in building fil-  ters. thus, it is important to study the properties of a follower-integrator. the  follower-integrator is the simplest filter that can be built out of a transconduc-  tance amplifier; it is a first-order, lowpass filter. figure 13.11a shows the basic  configuration. the voltage vr is identical to vb in figure 13.1, and, with the  capacitance c determines the corner frequency (cf) of the filter. figure 13.11b  shows the de characteristics of such a filter built with our amplifier. except  for very low input voltages, where the parasitic bipolar transistors shunt the  amplifier's current to ground, the output is a faithful replica of the input. fig-  ure 13.12 reveals the effects of the parasitic bipolar transistors in detail. at  input voltages that are within the normal range of operation of the amplifer  (1v to 5v), such as those shown in the top portion of the figure, the output  of the follower is a simple lowpass-filtered version of the input. in the bottom  portion of the figure, we see that, as long as the input voltage is below about  0.7 v, the amplifier's bias current is shunted to ground, and the output voltage  barely changes. the constancy of the output voltage occurs because there is no  current at the output of the amplifier to charge or discharge the capacitor such  that it barely changes. when the input voltage is outside this range, the output  voltage tries to follow the input voltage by normal follower action. we show  these data to illustrate the consistency of our earlier results with the behavior  of the follower-integrator; we do not recommend operation of the filter in this  regime. we shall now describe more useful linear and nonlinear characteristics  of the follower integrator in detail.  13.4.1 linear and nonlinear characteristics  figure 13.13a and figure 13.13b show data for the gain and phase character-  istics of a follower integrator. the phase curve is much more sensitive to the  presence of parasitics than is the gain curve, which remains ideal for a much  wider range of frequencies. the higher sensitivity of the phase curve to para-  sitics is reasonable, because the effect of a parasitic pole or zero on the gain is  appreciable only at or beyond the pole or zero frequency location. the effect  of the same pole or zero on the phase is significant a full decade before the  frequency location of the pole or zero. we have studied the origin of the para-  sitic capacitances in detail, but we shall not delve into this subject here. the  parasitics are caused by the large sizes of transistors that we use to reduce our  1if noise and offset; the dominant parasitics are the well-to-drain capacitancea low-power wide-linear-range transconductance amplifier 287  in  > out > [-->  c (a)  5  4.5 offset = 1 5 mv . ,..~~,  4 slope = 0.981 .~,~x,--. ~.>2~ a.53 ~  ~ 2.5  2  ~ 1.5  1:~ (b)  0.5  o~ i 2 3 4  dc input voltage (v)  figure 13.11 follower dc characteristics. (a) the basic circuit for a follower-integrator.  (b) the output is a faithful replica of the input except at very low input voltages.  of the w transistor, which causes a right-half-plane zero, and the gate-to-bulk  capacitance of the gm transistors, which causes a left-half-plane pole. rather  than make detailed models of the parasitics, we model their effect by simply  having a different corner frequency (cf) for the gain and phase curves. as  figure 13.13a and figure 13.13b show, the phase cf is slightly lower than the  gain cf, because of the excess phase due to the parasitics.  it is possible to reduce the influence of parasitics in our amplifier by having  the nfet gm-m mirrors be attenuating. then, the differential-arm para-  sitics are at higher frequencies, compared with the cf of the filter. another  alternative is to use a larger output capacitor. however, for cochlear-filtering288 neuromorphic systems engineering  o >  o 3.5  2,5  2  1.51 ~  0.5  04 ~ ~ ~ ~ 5 time (secs) x 10 .4  figure 13.12 bipolar effects in the follower-integrator. the top portion of the figure  shows normal follower-integrator operation, i.e., the filtering of a sinewave input. the output  waveform is attenuated and phase-shifted with respect to the input waveform. the bottom  portion of the figure shows operation of the filter at low input voltages. an explanation for  the strange output curve is given in section 13.4, and is due to the effects of the parasitic  bipolar transistors in our amplifier.  applications, for which this amplifier is mainly intended, there is a steep ampli-  tude rolloff beyond the cf of the lowpass filter, and fine phase effects are not  important at these frequencies. in addition, the second-order filter, of which  this filter is a part, is only an approximation to the more complicated filtering  that occurs in a real cochlea. thus, we have not expended energy optimizing  the parasitics to get an ideal first-order filter.  figure 13.13c shows the first, second, and third harmonic rms output am-  plitudes when the input rms amplitude is 1v (peak-to-peak of 2.8v). the data  were collected with an egg3502 lockin amplifier. because of the wide linear  range of our amplifiers, there is little distortion even at these large amplitudes.  at frequencies that are within a decade of the cf, the second harmonic is less  than 4 x 10 .2 the amplitude of the first harmonic. the third-harmonic dis-  tortion is negligible at these frequencies. the total harmonic distortion is less  than 4%.  figure 13.14 plots the relative amplitude distortion--that is to say, the ratio  of the magnitude of the second and third harmonic amplitude to that of the  first. note that the x-axis is plotted in multiples of the cf, and that we  are uninterested in effects more than a decade beyond the 1 point. the chief  features of interest are that the addition of the bump transistors reduces third-  harmonic distortion as we expect from eq. (13.10) and eq. (13.11). however,  it also increases the second-harmonic distortion at frequencies just below thea low-power wide-linear-range transconductance amplifier 289  cf, for reasons unknown to us. usually, second-harmonic distortion occurs  along with dc shifts. figure 13.15 shows these shifts for an input signal with an  rms input amplitude of 1v. these shifts exert only a mild influence in cochlear  designs, because input-signal amplitudes far beyond the cf are small due to  successive filtering from prior stages. the dc shifts are not significant for small  inputs. the dc shifts are observed in ota-c follower-integrators as well; they  are typically positive for amplifiers with nfet differential pairs and negative  for amplifiers with pfet differential pairs.  figure 13.16a shows the shifts in gain and phase cf for the w = 0 and w = 2  cases as a function of the input rms amplitude. note that the gain and phase  cf shifts are similar. the addition of the b transistors decreases the amount  of cf shift due to the linearization of the tanh. figure 13.16b shows the shifts  in cf as a function of the input dc voltage to the follower-integrator. as we  expect from prior data on the amplifier (figure 13.5a), the cf shifts track the  shifts in transconductance with dc input voltage. the tracking is seen for gain  and phase cf curves, and for the w = 0 and w = 2 cases.  13.5 noise and dynamic range  the largest signal that a filter can process without excessive distortion is de-  termined by the linear range of that filter. the smallest signal that a filter  can process is one whose rms amplitude is just above the filter's input-referred  noise. the dynamic range of a filter is given by the ratio of the power of the  largest signal to the power of the smallest signal. we have already discussed  linear range and distortion in the follower-integrator. now, we focus on the  other half of the story: noise in the follower-integrator. after we compute  and measure the noise, we can tell whether there has been an improvement in  the dynamic range of the follower-integrator. since there are no free lunches  in nature, we also want to discover what price we have paid in our design, in  terms of the increase in power and area. we shall discuss noise and dynamic  range in our design in sections 13.5.1-13.5.3. we shall discuss the dynamic  range of capacitive-divider schemes in section 13.5.4, since those schemes bear  similarity to our technique of using the well as an input.  13.5.1 theoretical computations of noise in the amplifier and  follower-integrator  we now compute the noise in a follower-integrator built with our amplifier.  we tie the v+ and v_ to a. common constant voltage source, and the output to  another constant voltage source. then, we replace each of the 13 transistors in  figure 13.1 with its small-signal equivalent circuit. we introduce a noise current  source between each transistor's drain and source terminals in the small-signal  equivalent circuit. for each noise source, we compute the ac transfer function  between its current and the differential output current. we sum the noise  contributions from each of the 13 sources incoherently; that is to say, the square  of the net output current noise is the sum of the squares of the current noise290 neuromorphic systems engineering  ~ 10 .2 > .  ~ 103 .- ~-  e <  ~ 10.4 ~czczzec:~ inputrms=4.84mv  magnitude -~  corner frequency = 1 1 4 hz ~ oo~-o.o.e~  (a)  ~ 10i~0~ r~ 102 103 104  frequency (hz)  o. ~e-~.m~^._. â¢ â¢ â¢ â¢, ........ , ........  :  ~,~ -i00 Â°ooooo  ~ phase Â°oooo Â°  ~ corner frequency = 95.3 hz  ~ -200 Â°ooo  ~_ oo ~ (b) oo  0"300101 ....... i~ 2 fre~que~cy(~zi 'i~ 3 ....... io 4  i0Â°~.o ...... , ........ , ........  ~ ~u~u ~000000000  0000 input rms amplitude =  o o iv  o o  10-1 o o  ~ o0 0  xxxxxxxxxx 00  ~ xx x x o00  x x  ~.10. 2 x 000000000000 x x  <~ â¢ â¢ ~:~ ~:~ xxx  ~ ~ )~ xxxx  ~ ~ ~ n( ~ xxxxxxxxxxx ~)~ x x ~ ~ ~ 0 ~ xxx xx  ~ xx  ~10 -3 ~ ~)~ )~)~ ~)~)~  ~ o -- fundamental ~ ~ ~  x -- second harmonic ~  â¢ -- third harmonic (c)  10 4 ~ , , , , ,~1 ........ ~ , , .... ,,  1 0 ~ 10 ~ frequency (~z) 10 a 10 4  figure 13.13 follower-lntegrator frequency response at a dc-input voltage of 3v. (a)  and (b) show the gain and phase characteristics of the follower-integrator along with fits to  lowpass filter transfer functions. the phase characteristics are more sensitive to parasitics  and we model their effect by fitting the phase curves to a different corner frequency than  the gain curves. (c) the first, second, and third harmonic rms output amplitudes at an  input rms amplitude of lv (2.8 v peak-to-peak). because of the wide linear range of the  amplifier, the total harmonic distortion is low even at this amplitude.  from each source. the input-referred voltage noise per unit bandwidth is the  output current noise per unit bandwidth divided by the transconductance ofa low-power wide-linear-range transconductance amplifier 291  o  o  ,_  e <  o > ,_ 10 Â°  10 -1  1 0i20_ 1 second harmonic distortion  x---w~2 (a)  0 --- w = 0 x~<o xo  o xo  x  o x o  xo  o xo  x  x x xo  xoxox x x x x o >$)  xo o xo  o o o  o x Â° o x  10 0 101 10 2 10 3 0 4  multiple of corner frequency  o  o  e <  > ._ 10 0  10 -~  1 0 -2 third harmonic distortion  x---w~2  o---w = o (b)  ooo ~o o o  xx x o Ã xo xo  o n ~  Ã Ã  o Ã  Ã xox~  xo  x  10-3 -110 . o ........................................ 10 o 101 10 2 10 3 10 4  multiple of corner frequency  figure 13.14 follower-integrator distortion characteristics. (a) the ratio of magnitude  of the second harmonic to that of the first harmonic vs. normalized frequency, i.e., frequency  normalized by the cf. (b) the same as (a) except that the third harmonic is plotted instead.  note that the addition of the b transistors decreases third harmonic distortion but increases  second harmonic distortion.  the amplifier. the total voltage noise is the voltage noise per unit bandwidth  integrated over the bandwidth of the follower integrator. the bandwidth of the  follower-integrator is determined by the transconductance of the amplifier and  the capacitance. if the amplifier is used in a system, where the bandwidth is292 neuromorphic systems engineering  -0.5  (j  -1.5 â¢ --;---~,,.58x~ ~ ;~ .............  x x oo Â°  x  x o  o x o---w= 0  x--w~2  x o  x o  x  ~xxa~o~o~x  ~0" ' ..... i; Â° ...... i;' ' ..... i; 2 ...... 4; 3 ...... 40'  multiple of corner frequency  figure 13.15 dc shifts in the follower-integrator. the shifts are shown at an input  rms amplitude of 1v. these shifts exert only a mild influence in cochlear designsâ¢ see  section 13.4.1 for details.  determined by some other part of the system, then this bandwidth determines  the interval of integration. the parasitic capacitances in the amplifier set an  upper bound on the maximum possible bandwidth.  although there are 13 transistors in figure 13.1, we do not get 13 transistors'  worth of current noise at the output. our calculations will show that we get  about 5.3 transistors' worth of current noise. this figure is only slightly higher  than the 4 transistors' worth of noise obtained from a 5-transistor ordinary  transconductance amplifier (ota). the reduction in noise occurs for three  reasons. first, for each noise source, there is a total or partial cancellation of  its noise current at the output, due to opposing contributions from the two  circuit arms. as an example, the noise current from the bias transistor in  figure 13.1 (with gate voltage vb) makes no contribution to the output noise  current, because it branches into equal portions in the two differential arms,  which cancel each other at the output. similarly, other noise sources, such  as those from the b transistors, contribute no noise. the sources from the  gm transistors have a partial noise cancellation. second, some of the noise  current from each source is prevented from contributing to the output by local  shunting circuit impedances. as an example, the noise currents ti'om the w  transistors contribute only 0.16 transistors' worth of current noise to the output  because most of the noise current gets shunted by the w transistors themselves.  third, when we compute the incoherent noise average across many sources, a  given source's contribution to the net noise is proportional to the square of its  fractional current gain; that is to say, to the square of the output noise current  divided by source current. therefore, weak contributions are weakened further.  if we define c~b, cts, ~w, c~, and am to be the current gain between the  output current of the amplifier and the input drain-to-source noise current ofa low-power wide-linear-range transconductance amplifier 293  120  ~" 100  >~ 8  =~ g 80  ~-  ~ 60 (a)  gain o 0 0 0  phase o o o o o  o---w= 0  x---w~ 2  gain x x x x x x  phase x x x x x x  4?0-3 ....... ...... i'6'  input rms amplitude (v)  (b)  180 i  160 ~'o  -  ~,~n 140 Â° Â°  ~' 120 ~o Â°Â°o Â° o---w= 0  ~ j Â°o Â°Â° o x---w~2 o  ~ 100 xxÂ°o Â° o  ~x x"Â°Â°Â°~ e ~ '~xx x o o o o o ~ 80i/ xxx x x x xx o x  6o~ ÃÃÃÃÃÃ Ã Ã ÃÃÃÃÃ Ã  40.~ 1:5 ~ 2:5 ~ /  input dc voltage (v) o  o  o o o  o  x x  x x  10 o  o o  o  o  x ~ o 0  x x ~  x x x x  3:5 ~ 4.5  figure 13.16 corner-frequency shifts in the follower-lntegrator. (a) cf-shifts vs. input  rms amplitude and (b) cf-shifts vs. input dc voltage. the lower curve is the phase cf for  the w ---- 0 and the w = 2 cases.  a b,s,w,g, or m transistor, respectively, we can show that  o~b ---- o,  ~;n o~ s ~  oz g -~ i';n -f t~n t~p -f i':,k,p '  i~n l~ p  t~ n ~- i'~nl'; p -~ t~t~p '  i~ n ~ i~n l~ p  t~ n ~- t'~nl';, p ~- n,l'~p '294 neuromorphic systems engineering  since each transistor has the same dc current flowing through it (i.e. ib/2 in  the w = 0 case, or ib/3 in the w = 2 case), the magnitude of the noise current  source across each transistor is the same. thus, the output-current noise of the  amplifier is the noise current due to one transistor times an effective number  of transistorsn, where, n is given by  .n = 2as 2 4- 2ctw 2 -k 2aa 2 + 4o~m 2.  for our amplifier with ~ ~ 0.85, ~ ~ 0.7, and ~p ~ 0.75, the numbers work  out such that n = 5.3. the dominant contribution to n comes from the four  m transistors which contribute a whole transistor each. the two g transistors  contribute 0.865 transistors. the two s transistors contribute 0.28 transis-  tors. the two w transistors contribute 0.16 transistors. the b transistors and  the bias transistor contribute no noise. the most noise-free linearization tech-  niques, in decreasing order of merit, are bump linearization, the use of the well  as an input, source degeneration, and gate degeneration. bump linearization  is the only technique of the four that adds no noise whatsoever. note that,  depending on the circuit configuration, the relative noise efficiencies of the use  of the well as an input, source degeneration, and gate degeneration mayvary.  for example, in a well-input amplifier with source degeneration but no gate  degeneration, aw = as = ~/(~ + ~,). in the latter case, the use of the well  as an input and gate degeneration each contribute 0.41 transistors' worth of  noise.  the magnitudes of the individual noise current sources depend on the dc  current flowing through the transistor, and are well described by a white-noise  term for low subthreshold currents [23]. at high subthreshold currents, there  is also a 1/f-noise germ. our experimental data in section 13.5.3 reveal the  amount of the 1if contribution; we shall model this term empirically, because  no satisfactory theory for 1if noise currently exists.  in the first paragraph of this section, we explained the procedure for calcu-  lating the noise. we shall now perform the calculations. as usual, we begin by  analyzing the c~e for w = 0 and then extend our analysis to the w = 2 case.  the output-current noise of the amplifier ~ is given by  ig = x } + (la. z)  l  where the first and second terms in the integral correspond to white noise and  1if noise, respectively; i~ is the bias current, and k is the 1/f noise coe~cient.  we also assume that there is low-frequency adaptation in the amplifier, so that  frequencies below f~ are not passed through. this assumption is necessary if we  are to prevent the 1if noise from growing without bound at low frequencies.  in our amplifier, we have an offset-adaptation circuit that keeps f~ around 1  hz. also, typically the k for pfets is smaller than is that for nfets, anda low-power wide-linear-range transconductance amplifier 295  scales inversely with the channel area of the transistor. however, we assume a  transistor-invariant k, for simplicity.  the corner frequency of the follower integrator fc is  ib  fc- 2~rcvl' (13.18)  --  2 is so from eq. (13.17), the input-referred voltage noise v~  ( / --v~ = ~ n qi~ + ~ ] 1 ~ ~ (~b/v~l ~ ~ + (~/~  ( nqv~) ~f~ + nkv~ in (1+ ~ iz ] ~ ~ (f~/ft)2/~  _ nqv~ nkvc~ in 1 +  4~ + ~ 2uf~vi  (13.19)  in evaluating eq. (13.19), we computed two integrals:  fo ~ dx _ ~  l+x 2 2'  ~ dx _ lln(1 + 1 ).  , ~(1+ ~) ~ ~  note that the 1if noise rises with bias current because of the increasing band-  width. the magnitude of the 1if noise depends on the bandwidth; that is  to say, it depends on ib/cvl. the white noise is current invariant, because  the noise per unit bandwidth and the bandwidth scale as 1lib and ib, re-  spectively. the white noise increases linearly with vl, whereas the 1if noise  increases quadratically with vl.  13.5.2 theoretical computations of dynamic range  the dynamic range dr is defined to be the ratio of rms input power in a  signal with amplitude vl to the input-referred noise power; this definition is  implicitly based on our willingness to accept the distortion present at this  amplitude, which from figure 13.13 is seen to be reasonable. if we want to be  more conservative with respect to distortion, then we simple scale vl by the  necessary fractional amount. so, we have from eq. (13.19) that  dr = v~/2_  2 v i  v~/2  nqvl ~ ln (1+ ~ ~ ~ ~ ~c + ~ k k ~cv~ ] jas~ou p~jzajaz-~ndu~, lua!~uop! oa~q Â£aii: 1 l~ii~ ~oqs u~a ~ uaq~ 'qlpi~puuq  om~$ aq~ aa~q ao~an~ul-za~oiio ~ e = m ~ pu~ 0 = ~ ~ li "~a(u/~) ~ ~a  ~a~u~ zuouii puu q~pl~puuq aq~ zo~ "~/~i ~ g/al 'sao~sisu~z~ aq~ q~nozq~  s~uo~n3 aq~ ~o d 'di~vilmls spaooo~d ~u puv a~a ~oj slsdi~u~ aq~ '8 = m ji  __  â¢ s~aa~ a~a~ i q1~a  saoasssu~al ash asnm o~ 'aas~o pu~ os~ou//i aanpaa o& :o~u~a a~m~udp ~aaxa  qa~ pol~dsoss~ asoa ~aa~ u~ osi~ s~ aaaq& "sopn1~idmv o~av i pu~ aoo~ astou  aaq~q ~ oa~q soaiosmaqa l~qa sandu~ oa pal~ns oq o~ polvas uaoq oa~q sloa0 i  os~ou pug ivu$~s aqa avql Â£ioaom s! uo~a~na~s ~ qans u~ Â£a~aeou~i jo asmu~ape  oq& 'qap~pu~q avqa xluo uo spuodop qap~pueq uoai$ v aoao os~ou ivaoa oqa  'o~ "au~avau~ auoaana s~ pu~ ~a xiuo uo spuadop q~p~pu~q l~un aod as~ou  oqa asn~aaq sosu~ aau~avau~ qap~pu~q aqz "~u~aeau! qap~pu~q s~ aaq~a  anq 'iu~a~au~ auaaana aou st as~ou f/i asn~aoq ~a o~i sos~aaaut ostou oqa 'aaao  -~oq 'anita ivu~$~ao sa~ oa paaoasaa uaaq s~q qlp~pu~q aqa ao~v "(gi'gi) "ha  oa ~u~paoaae 'papuadxo oq oa svq 7 a o1 i~uo~aaodoad aa~od uoqa 'poaaosaad  oq oa s~ qap~pueq ji "va/i o~i soleas qap~pu~q aqa puv '~a a~i soi~as  qap~pu~q a~un aod as~ou or& "a6uw a}mvu~p aq~ laaffv 1ou saop a6uva avau}l  ap}~ 'sa,vu}~op as}ou f /i ~ 'snqi "~a Â°~i sas~aaam ao~od i~u~s mnm~m  oq~ "~a Â°~i sos~aaaut aa~od os~ou-//i oqa avqa (6i'~i) 'ba moaj oos a~  â¢ aomod u~ os~aaaut  u~ jo oatad oqa ae poua~o uoaq s~q osu~a a~mvujp ~aaxo oqÂ£ xlaa~aaodsoa '~i  pu~ ai/i o~i oi~as qap~mpuvq oql pu~ qap~mpu~q a~un aad os~ou oqa osn~aoq  sosue oaue~avau~ ~uaaana aqÂ£ 'au~aeau~ auoaana st ostou i~maoqa asn~aoq anita  om~s oql av summoa os~ou aqÂ£ "7 a oa i~uo!aaodoad anita ~ av sut~moa lids  os~ou oqa 'aoao~oq 'anita [~u~$~ao sa~ oa paaoasaa uooq svq qap!~puvq oqa aoa  -jg "(~i'~i) 'ha o~ su!paoaae 'popuodxo oq oa ~q 7 a oa ivuo!aaodoad aa~od  uoql 'paaaosaad oq o1 s ! qlp!~pu~q ji '7a o~!i soivas os~ou ii~a0ao 0qa a~qa  qans 7a/i a~i i soi~as qapi~puuq aqa anq '~a o~ii soi~as qapi~puuq aiun aod  ostou aq~ "~a}paad (ig'gi) "bg ~v 'asuva ai~vu~p aao~ ~aild~ } asuva avau}l  ap}~ 'sa~vui~op asiou a~iy~ /} 'sng~ '~a a~i so~oaam aa~od i~u$~s mnm~x~m  oq~ "7 a a~! i sas~oaam ao~od as!ou-oa~q~ oqa l~qa (6i'~i) "ha moaj oos o~  bn  (iugi) jada = uo  â¢ ia~ou s~  osaka a~mvudp aqa '(0g'gi) "ba moaj 'snq& "aold avou~i-a~ou!i ~ uo os~ou oa!q~  ~q paleu!~op s! aoid oqa aopun eaa~ lou aqa '~auonbaaj snsaoa opna~idmv as!ou  3o ~old $oi-$o i ~ uo aueaaodm~ oq oa savadd~ os!ou//i oqa oaoq~ 'sauoaana s~!q  q$~q a~ uoaa "sloaoi auoaana ploqsoaqaqns ~o i oqa a~ q$!q xiaa~a~ioa ii~ls s~ os~ou  aa~q~ asneaaq pu~ 'uo~a~ad~pe aas~o xq 1no paaaai~ aae zhi ~olo q soduonboa j  osnvaoq 'saoas~su~aa &add ji~som 'osa~ i ano u~ x jo onlva ilvms aqa jo osnvaoq  'maaa//i aqa uvqa aa~a~ i qanm xii~a~dx 1 s~ mao~ as!ou-oa~q~ oqa '(0g'~i) "ba ui  (() ) vaog @,  (ov o = i  di'Â¢ihh~inidn~ ~iaihk~x$ dihdhobiohflhn 96ga low-power wide-linear-range transconductance amplifier 297  spectra. for the bandwidths for the w = 2 and w -- 0 cases to be equal,  however, the bias current for the w = 2 case has to be increased by a factor  of (3/2) over that for the w = 0 case. theoretically, the dynamic range of the  w = 2 case is (3/2) 2 times the dynamic range of the w = 0 case because of  the increase in linear range. in practice, however, if the linear range is already  large in the w = 0 case, the increased linearity in the w = 2 case does not  reduce distortion further. the distortion is typically limited by some other  mechanism. for example, in our amplifier the dominant distortion is due to ~  shifts caused by the nonlinear well-to-channel capacitance. nevertheless, the  increased linearity does reduce third-harmonic distortion and cf shifts with  input amplitude, as shown in figure 13.14b and figure 13.16b.  13.5.3 experimental noise curves  figure 13.17a shows noise spectra at low bias-current levels for a follower-  integrator with a capacitance c -- 1.02 pf. the data were taken with a  hp3582a spectrum analyzer. from eq. /13.17), we would expect white or  thermal noise to be dominant at low bias currents. we observe from the figure  that, even at the lowest frequencies, no 1if noise is visible. we were able to  fit the data with lowpass-filter transfer functions as shown. note that, for the  low bias currents of figure 13.17a, where 1/f noise is hard to discern, we did  not use any 1/f terms in our fit. the terms in the integral of eq. (13.19)  predict that the noise spectra reveal relatively more 1/f noise at high bias cur-  rent levels because the white-noise term decreases and the 1if term remains  roughly constant. the data of figure 13.17b illustrates that this prediction is  approximately borne out. however, we were able to empirically fit the voltage  2 noise per unit bandwidth vif more accurately by a term of the form  ) vii = ~ +a . (13.22)  1 + (fife) ~  figure 13.18a shows a typical fit to the data in more detail. from the first line of  eq. (13.19), we would expect k8 = v~k/4, n = 1, and a = vl~nq/ib. since  ks and n are empirical, they do not yield substantial theoretical information,  although they are useful practically. in the following paragraph, we show how  to extract the value of n of eq. (13.21) from the value of a.  from the value of f~ obtained from the fit to the data, from our knowledge  of c, from measurements of im and from eq. (13.18) we obtain vl. given  vl and the fit parameter a, we obtain n, the effective number of transistors  contributing shot noise. figure 13.18b shows a plot of n versus the bias current  i8. we observe that n is roughly 7.5 in subthreshold, and decreases as the bias  current goes above threshold and space-charge smoothing sets in. the value of  n in subthreshold is within a factor of 1.4 of our theoreticm prediction of 5.3.  above threshold, the space-charge smoothing, that is to say, the modulation of  the mobile charge concentration by the mobile charges themselves, reduces the  noise to a value below what we would expect from shot noise.298 neuromorphic systems engineering  figure 13.19 shows a plot of how the ks and n of eq. (13.22) vary with bias  current is. since kb = kv~/4, part of the increase in ks arises from the  increase in vl and part of it arises from the increase in k. it is also interesting  that, as the bias current increases, the 1if noise power systematically rises  from about 0.67 to about 0.95.  the noise measurements of figure 13.17 through 13.19 were taken for an  amplifier with w = 0. we also experimentally confirmed that the noise in  a w = 2 amplifier was identical to that in a w = 0 amplifier of the same  bandwidth.  13.5.4 capacitive-divider techniques  our use of the well as an input implicitly involves a capacitive-divider technique:  the gate, surface potential, and well form three terminals of a capacitive di-  vider. we chose the well as an input because coupling ratio of the well to the  surface potential, 1 - ~, is smaller than the coupling ratio of the gate to the  surface potential, ~. the advantage of this implicit capacitive-divider scheme  is that the divider is inherently part of the transistor; so, we exploit a parasitic  capacitance rather than avoiding one. also, no additional floating-gate adapta-  tion circuits or control voltages are needed. the disadvantage of the technique  is that the capacitive-divider ratio is fixed by the physical parameters of the  process, and is slightly nonlinear. if the divider ratio is not as small as desired,  we must use other circuit techniques like source degeneration, gate degenera-  tion or bump linearization to obtain wider linear range. luckily, in our circuit,  the additional transistors used to widen the linear range do not increase the  noise greatly, but they do cost more area. it is logical to ask whether we can do  better in area consumption with circuits that have explicit capacitive dividers.  we shall discuss two simple schemes where capacitive dividers are explicitly  used around ota's. we assume that any floating-gate inputs of the amplifiers  are held at a proper dc value by low-frequency adaptation circuits. we further  assume that the adaptation circuits do not affect noise in the amplifier's tran-  sistors or in the circuit. in practice, this assumption may not be true of certain  adaptive schemes.  figure 13.20a shows a simple scheme. the voltage vt determines the bias  current in ~he ota. in practice, parasitic capacitances between ~he output  and inpu~ ~erminals of the ota can hurt ~he design significantly. if v~ is the  linear range of the ota, and ~ is the effective number of noise-contributing  ~ransistors in ~he ota, ~hen it can be shown tha~ the dynamic range da is  2 (co~ + c~+c2 ~ dr = (13.23) nq  the analysis leading to the previous equation is similar to that preceding  eq. (13.21). we assume that thermal noise dominates. ~om eq. (13.23), we  see that co,t needs to be moderately large. if not, any improvement in dynamic  range over that of an ota-c follower-integrator arises only at the expense ofa low-power wide-linear-range transconductance amplifier 299  10 .3  ,~- 10-4'  i 1e  >~ 10 s  .8  z  n 10 -e  o >  ~ 0 -~  0 (a) 18 = 50pa  noise is almost exclusively ~  thermal  -1-  ~_105 >~  .8 o z  &  #  ~108  8 1Â°1oÂ° ...... i~' ' ..... i~ 2 ...... i~ 3 ...... i~' ' ..... io'  frequency (hz)  10 .4 ............................................  (b)  ~ 18~ 6.5 na  "'~~ = 24n~ ~ 00na  at high bias-current levels,  progressively more 1/f noise  is revealed  1Â°~'oÂ° ...... i;' ' ..... i; 2 ...... i; 3 ...... ;;' " ..... lo s  frequency (hz)  figure 13.17 noise spectra at various current levels. (a) at low bias currents the noise is  almost solely white or thermal. the bold lines are iowpass-filter fits to the data. (b) at high  bias currents, there is relatively more 1/f noise. nonetheless, the dominant contribution to  the noise, which is the area under this curve in a linear-linear plot, remains thermal. the  bold lines are fits to the sum of a 1/f term and a white-noise term.  an extremely large value of c2. for example, if co~t were 0, we would need  c2 to be approximately 15 pf to get a dynamic range improvement of 15 over  that of an ota c follower-integrator with lpf.300 neuromorphic systems engineering  10 .4  ~10-5  >~  ~o  ~ lo-s  0 i b = 12na  l/f noise l/f noise  e noise  ~Â°(oÂ° ...... ig' ...... ;g' ...... i~ ~ ...... i~' ...... io Â° frequency(hz)  8  o  0  z o o o o o  o o  o  o o o  (b)  (  ~oo.,, ..... i.~,.,o ..... i.~. ~ ..... ,~,~;., , ~o ~ ~o-o  bias current (a)  figure 13.18 basic noise characteristics. (a) a typical noise spectrum. (b) the effective  number of transistors contributing shot noise in our circuit as a function of the bias current.  as the bias current goes above threshold, the ef[ective number of transistors contributing  shot noise decreases because of space-charge smoothing.  similarly, for the inverting configuration of figure 13.20b, we get  c~(c~.+c~) ~ ( c~+c~n+c~ 2 (co'~' + ~ ] t c, ) vl  dr = nq  once again, we observe that co~t must be moderately large. if not, any im-  provement in dynamic range arises only at the cost of an extremely large value  of (cin + c1). this configuration also introduces an rhp zero. if the effects ofa low-power wide-linear-range transconductance amplifier  x 10 .9  4 ........ , ........ , ...... ~. 301  3.5  'e 3  "~ 2.5  o  o  .~ ~ 2 ~-  ~.  t.5  0"150-9 o  o  o  o  10 -8 10"7  bias current (a) (a)  10 "6  1  0.95  0.9  o  0.7  o ~ 0.85  o i~.  m~ 0'8  Â¢-  ~.  ~ 0.75 o  o  , , , , , ,,,, , , . , , ,,,, 0'6~' 0-9 10-8 107  frequency(hz) o  (b)  10 .6  figure 13.19 characteristics of 1/f noise. (a) the i/f noise coefficient -ft'b, used in  eq. (13.22), as a function of the bias current _~b. (b) the i/f noise power n as a function  of the bias current _/~.302 neuromorphic systems engineering  this zero are to occur at frequencies well past the cf of the follower integrator,  then  cout ( c2 + cm + ca ) c2 + cin + ca >> c2.  parasitic capacitances can also hurt this design significantly, especially if the  explicit capacitors in the circuit are small.  actually, the circuit of figure 13.20b does not even need an ota, as the  reference input of the ota is not really used. the ota can be replaced by a  two-transistor amplifier, but, in that case, vl also should be replaced by vl/2.  thus, from eq. (13.21), as in a normal ota, n is still effectively 4.  theoretically, by making capacitive-divider ratios appropriately small, and  by spending power, the dynamic range may be increased to values beyond that  attained in our design. a floating-gate adaptation scheme combined with a  two-transistor version of figure 13.20b is being explored [8, 12].  13.5 conclusions  we conclude by summarizing our key findings:  . if the amplifier's noise is predominantly thermal, then an increase in  its linear range increases the follower-integrator's dynamic range. if  the amplifier's noise is predominantly i/f, then an increase in its linear  range has no effect on the follower integrator's dynamic range. to pre-  serve follower-integrator bandwidth, power consumption increases pro-  portionately with an increase in the amplifier's linear range according to  eq. (13.18).  2. in subthreshold, the noise is predominantly due to thermal noise, even at  high bias currents, where some 1if noise is present. the theory described  in [23] accurately modeled our thermal noise. empirical expressions in  the paper modelled our 1if noise.  . in subthreshold circuits where thermal noise dominates, a simple formula  for the dynamic range of a follower-integrator is d/~ = 2cvl/nq. the  capacitance of the follower-integrator is c, the linear range of the am-  plifier is vl, the charge on the electron is q, and the effective number of  noise-contributing transistors in the amplifier is n. a more complicated  formula that includes 1if noise is given by eq. (13.20).  . experimentally, we obtained a dynamic range of 65.2 db in a follower-  integrator with a capacitance of lpf. a signal with an input rms am-  plitude of 1v yielded 4% total harmonic distortion. the total measured  noise of the follower-integrator was 0.55 mv. a simple ota c follower  integrator has a theoretical linear range of 75 mv, and a theoretical noise  floor of 110 #v. thus, we obtained a dynamic range improvement of ata low-power wide-linear-range transconductance amplifier 303  in ~> ~ i__ ~~~ (a) out  i l  in ~> c2 (b)  i k i  ou out >[~>  figure 13.20 capacitive-divider schemes for widening the linear range. (a) and (b)  show two different schemes. section 13.5.4 contains further details.  .  .  .  .  . least 8.5 db over the ota c follower-integrator. in practice, due to of-  fchip noise floors on the order of 0.5-1 mv, the improvement can be as  much as 20 db.  bump linearization is our most efficient linearization technique, because  it increases the linear range of our amplifier without increasing its noise.  gate degeneration is a useful transconductance-reduction technique. it  can be generalized to the notion of the current increase from one input  degenerating another input. the technique could be useful in multiple-  gate-input circuits [18].  when the well is used as an input, the gate must be operated at as low a  voltage as possible in order to obtain maximum dc-input operating range.  capacitive-divider techniques that widen the linear range bear similarity  to our technique of using the well as an input. if appropriate atten-  tion is paid to capacitor sizing, parasitic capacitances, and floating-gate  adaptation in these techniques, then they may yield dynamic range im-  provements similar to ours.  changes in ~, the subthreshold exponential parameter, are due to changes  in dc current and to changes in well-to-gate voltage. these two effects  may be studied separately through the techniques described in the paper.304 neuromorphic systems engineering  acknowledgments  we thank lena peterson for providing the data of figure 13.8. we thank paul hasler  and bradley minch for useful discussions. this work was supported by the beckman  hearing center, and by the center for neuromorphic systems engineering as part of  the national science foundation engineering research center program.  appendix: a  this appendix contains a quantitative discussion of the common-mode effects  in our amplifier. the data in the appendix were taken for a w = 0 amplifier  built in a p-well process, as opposed to the data in the rest of the paper, which  were taken in an n-well process. the ~ for the p-well process is lower, and  consequently the linear range is near 0.6 v, rather than 1v. we also use the  grounded-substrate convention [16]. this convention enables us to present the  data as though they were from an n-well process, as in the rest of the paper.  the grounded-substrate convention implements the following transformation  from n-well space to p-well space: v-+ -v, n-channel-~p-channel, and p-  channel-~n-channel. note that the transformation is applied to all voltages  implicitly defined in terms of the gate, source, drain, or well voltages in addition.  for example, the flatband voltage is defined in terms of vg - vw, and changes  sign as we move from n-well space to p-well space. thus, if the flatband voltage  is quoted as -0.75 v for an n-channel transistor, it's taken as -0.75 v for a native  transistor in n-well space and as +0.75 v for a well transistor in p-well space.  a.1 the effects of changes in /~  in our amplifier, the gates of the w transistors are near ground. as we lower  the voltages of the well inputs, the well-to-gate voltage decreases; consequently  ~ decreases; the transconductance, which is proportional to 1 - ~, increases.  we now analyze this effect more quantitatively.  the parameter ~ is a function of the gate-to-well voltage. we can show that  g = 1 - 2 (13.a.1) '  - (va - vw - vfb)  where "~ is the body-effect parameter and vfb is the flatband voltage.  a well-input amplifier that has no source degeneration or gate degeneration  has a transconductance of magnitude gw, given by gw = (1 - ~). by com-  puting the transconductance at the origin for various common-mode voltages  vc, we measured gw as a function of vc at a bias current corresponding to  vdd -- vb : 0.77 v. from eq. (13.a.1), if we plot 1/(1 - ~)2 versus vw, i.e.,  1/gw 2 versus vc, we get  - vcÃ· 1Ã· g~ ~ ,  4a low-power wide-linear-range transconductance amplifier 305  which is a straight lineâ¢ thus, we can compute 7 and vfb from the slope  and y-intercept of this line, if we know va. for our experiment, we grounded  the gate to allow maximum dc input operating range, so vc was 0. from the  â¢ 1 data shown in figure 13.a.la, we computed 7 = 1.06v~ and vfb = 0â¢68 v. in  comparison, the spice parameters from the mosis sheets quoted 7 = 1.05vÂ½  and vfb = 0.75v. the actual flatband voltages are negative; since the data  were taken in a p-well process, we use the positive values as explained in the  first paragraph of this appendix.  a well-input amplifier with gate degeneration has a transconductance of  magnitude gg, given by  1--t~ (13.a.2) gg -- 1 + (~Â¢/~)  for such an amplifier, we can determine the functional variation of a with  vc from eq. (13.a.1), using the previously determined values of vfb and 7,  and with vc being the amount of diode drop on a g transistor. by using  measured well and native transistor parameters we estimate v6 = 0.69 v given  that vdd -- vb ---- 0.77 v, and also that ~ = 0â¢714. by using these parametric  values in eq. (13.a.2) and eq. (13.a.1), we predicted the dependence of gg  with vc. the middle curve of figure 13.a.lb shows that changes of g~ with vc  were in good agreement with the theory of eq. (13.a.2) and eq. (13.a.1). the  uppermost curve of figure 13.a.lb is that of gw versus vc and is also plotted  for reference; it is simply a different way of plotting figure 13.a.la.  a well-input amplifier with source and gate degeneration has a transcon-  ductance g given by eq. (13.3)â¢ by using the functional variation of a versus  vc, the values of ~, the value of v6 estimated in the previous paragraph, and  ap = 0.753, we were able to predict the variation of g with vc, as shown by  the lowest curve of figure 13.a.1b. the data begin to deviate from theory at  the lower input voltages, probably because of the change in gp with increasing  well-to-gate voltage.  a.2 the effects of the parasitic bipolar transistor  to understand exactly when the parasitic bipolar transistor present in every  mos well transistor becomes significant, we find it instructive to analyze the  interaction between the bipolar and mos modes of operation for a well tran-  sistor: the subthreshold saturation current of an mos transistor situated in a  well, which is assumed to be an n-well without loss of generality, is given by  im = ime( ut ), (13.a.3)  where ~b i8 the surface potential, and im i8 a constant pre-exponential factor.  the constant im does have a weak dependence on ~, described in [27], that we  neglect for simplicity. if ito is the threshold current of the transistor, and if,  at this point, the surface potential is below the source potential by an amount  2Â¢f, then  im =- ito e-2Â¢f/ut â¢306 neuromorphic systems engineering  the constant ito is typically near #cox(w/2l)(ut/~) 2.  if all voltages are referenced to the well (i.e., vw = 0), and we define ~} = -Â¢  and gg = -(va - vfb), then we can show that  we introduce the definitions of ~ and ~ because it is more convenient to work  with -Â¢ and -(va - v~b) when dealing with transistors in the well.  the source current of a well transistor is split into an mos component,  called im, which reaches the drain of the transistor and a bipolar component,  called iu, which is shunted away to ground. the bipolar current is given by  [~  iu = iuet ~ ~, (13.a.4)  where ib is the saturation current of the bipolar. the mos current is given  by eq. (13.a.3).  the question that we now ask is this: when does the mos current exceed  the bipolar current (im ~ ib)? the answer to this question provides insight  into how a well transistor must be operated if it is to have as wide a range of  mos operation as possible. we notice that the mos and bipolar transistors  have the same dependence on the source voltage, vs. thus, in subthreshold, the  answer is independent of the source voltage. the mos pre-exponential factor,  im, is usually 1000 to 10000 times smaller than the bipolar pre-exponential  factor ib. thus, if the mos transistor is to have any hope of competing with  the bipolar transistor, its surface potential must be below that of the well by  the amount that compensates for its pre-exponential handicap. hence, the  gate-to-well voltage must be below the flatband voltage by an amount needed  to generate an adequate depth of depletion region. we now compute exactly  how much this amount must be.  if im 2 is, then, from eqs. (13.a.4) and (13.a.3), we must have  vw-Â¢ ~ utin(i~)  ~ vbm, (13.a.5)  where v~m, defined as in the previous equation, is a voltage that yields a  measure of by how much the bipolar's pre-exponential constant exceeds the  mos's. thus, if we reference all voltages to the well, eq. (13.a.5) yields  7 2 7 > vbm. + _a low-power wide-linear-range transconductance amplifier 30"/  20 Â¸  18Â¸  16 Â¸  14 Â¸  12 Â¸  10 Â¸  8  6  0.5  0.40 well-to-gate voltage (v)  1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5  0.35  '6 0.30  -~  ~ 0.25  i,-  ~, 0.20 (~  _  ,~ -~  ~ o.15  e  0.10  0.05  0.5 gw  gg  g  input voltage (v)  1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5  fil~ure 13.a.1 the effects of changes in ~. (a) the changes in e with well-to-gate voltage  may be used to extract the body-efl:ect parameter ~ and the flatband voltage vfb. the  slope of the graph yields information about ~f, and the intercept then yields information  about vfb. see section a.1 for details. (b) data for the change in transconductance of  well-input amplifiers with no degeneration (gw), with gate degeneration (gg), and with gate  and source degeneration (g). the solid lines are fits to theory.308 neuromorphic systems engineering  after we perform simple manipulations on the previous equation, we finally  obtain the criterion for the mos transistor to dominate over the bipolar tran-  sistor:  <_ vct (13.a.7)  where we define vct to be the rhs of eq. (13.a.6). if we require im ~_ 100ib,  for robust operation, we must increase vsm by utlnl00 before using  eq. (13.a.7). we now have a recipe for operating well transistors in subthresh-  old: as long as we ensure that the gate is sufficiently below the well, we are  in no danger of turning on the bipolar transistor. thus, if the range of mos  operation is to be as wide as possible, the gate voltage must be set to as low a  value as circuit requirements will allow. the well is then free to operate from  low values near the gate voltage to vdd. therefore, when the well is being used  as an input, the gate should be at ground or near ground. in our amplifier, it  is one diode drop above ground.  to understand the effects of the parasitic bipolar transistor in our amplifier,  we need only to understand the half-circuit shown in figure 13.a.2a, because  the bipolar transistor(s) in the arm(s) of our amplifier is (are) activated if the  voltage of either input gets too low. it is thus simpler to analyze the effects  of the bipolar transistor in a single arm of the amplifier, and to extend the  analysis to the case where the bipolar transistors in both arms are activated.  hence, we tie one input to vdd to turn off completely one arm of the amplifier,  and we concentrate on the turned-on arm.  the circuit of figure 13.a.2a is one half of the differential pair of the amplifier  with the s transistor omitted, and the bipolar transistor drawn explicitly. the  s transistor is omitted because it affects the drain voltage of the bias transistor  (the bias-current transistor not shown), but has no effect on the currents is and  im, which are our primary concern. figure 13.a.2b and figure 13.a.2c show  the small-signal model and corresponding signal-flow diagram that describe the  half-circuit of figure 13.a.2a. when the well-input voltage vxy is in the 1v  to 5v range, the gate voltage, v~, which is one diode drop above ground, is  sufficiently below the well voltage that all the bias current is carried by the mos  transistor. as the well voltage begins to drop, it starts to get close to the gate  voltage and the bias current starts to be diverted to the bipolar transistor. the  data of figure 13.a.3a show that a differential-pair-like competition between  the mos and bipolar transistors yields a sigmoid curve in output current versus  input voltage. the data curves of figure 13.a.3 were normalized by their  saturation current values, and were fit by sigmoid equations of the form  1  in = 1 + e -0.326(vi~v-vh)/ut' (13.a.8)  with differing values of vh for the three curves.  the input voltage at which the bipolar and mos currents become equal,  vh, depends on the bias current. higher bias currents drop larger voltagesa low-power wide-linear-range transconductance amplifier 309  (a) (b) ~ i~ ii i;> i. --~i~ _ ~ i~-.iiv~-v,ol  gm[ -  (c)  --  + --  vino ~ ~ ~ ~ b  (vs-v m)  _  figure 13.a.2 the parasitic bipolar half circuit. (a) a simplified half circuit for our  amplifier that is important for understanding the bipolar-mos interaction in it. (b) the small-  signal equivalent circuit for (a). (c) a signal-flow diagram for the small-signal equivalent  circuit.  across the gm transistor, increase the gate voltage of the w transistor, and  consequently cause the well-input voltage to approach the gate voltage at higher  values. the data of figure 13.a.3b show our measurements of this effect, and  also show that  vh = 0.81 (vdd -- vb) + 0.019. (13.a.9)  given the theory that we have developed for the bipolar mos interaction  (eq. (13.a.7)), and the effects of changes in n (eq. (13.a.1)), we can pre-  dict what eqs. (13.a.8) and (13.a.9) should be by analyzing the circuit of  figure 13.a.2a. using measured values for the constants vfb, 7, nn, nb (the n  of the bias-current transistor), and vbm (determined by measurements of the  bipolar and mos pre-exponential constants), we were able to obtain reasonable  agreement between theory and experiment. prom the signal-flow diagram of  figure 13.a.2c, we can show that the constant 0.326 of eq. (13.a.8) is nearly  c~ = 2 (~/2)~  ~/2 + ~n'310 neuromorphic systems engineering  vdd-v b (v)  0.65 0.70 0.75 0.80 0.85 0.90 1.2  1.0  0.8  0.6  0.4  0,2  0.0  -0.2  -0.2  0.775  0.750  0.725  0.700  0.675  0.650  0.625  0.600  0.575  0.550  0.525  0.60 (a)  0.800 v o~  0.725 v~  input voltage (v)  i i i i i i i  0.0 0.2 0.4 0.6 0.8 1.0 1.2  >=  figure 13.a.3 the bipolar-mos characteristics. the sigmoid-like compeitition between  the mos and bipolar transistors as the input voltage is varied. the three parametric voltages  refer to the value of the bias voltage vb of figure 13.1. (b) the threshold point of this  sigmoid varies with vb.  note that we evaluated ~ with the surface potential at a value that was vbm  below the well voltage. we found that it was 0.467. using eq. (13.a.7) and  elementary reasoning, we can show that eq. (13.a.9) is derived from  v. = ~ (v,~,, _ v.) + v:~ 1. ( *g'~ - v~, ~= ~ t,~g)a low-power wide-linear-range transconductance amplifier 311  where i0 ~ and i0 n are the subthreshold scaling parameters for the bias-current  transistor and for the g transistor respectively. we found that ~b = 0.5615,  ~,~ = 0.714, i0 b = 1.12 Ã 10 -17 a, ion = 1.40 Ã 10 -16 a, ib = 2.12 Ã 10 -16 a,  i:~0 = 1.8 Ã 10 -7 a, 2Â¢f = 0.749 v, vbm = 0.2165v, ~/= 1.05vÂ½, vfb = 0.75  v. the most interesting finding is that vbm = 0.26 v, which implies that the  mos transistor's pre-exponential constant im is about 4000 times as weak as  the bipolar transistor's pre-exponential constant ib.  references  [1] x. arreguit. compatible lateral bipolar transistors in cmos technology:  model and applications. dsc these no. 817, ecole polytechnique federale  de lausanne, 1989.  [2] m. c. h. cheng and c. toumazou. linear composite mosfets (com-  fets). electronics letters, 27(20):1802-1804, 1991.  [3] w. chung, k. kim, and h. cha. a linear operational transconductance  amplifier for instrumentation applications. ieee trans. on instrumenta-  tion and measurement, 41(3):441-443, 1992.  [4] t. delbriick. bump circuits for computing similarity and dissimilarity of  analog voltages. cns memo no 26, may 1993.  [5] s. t. dupuie and m. ismail. high frequency cmos transconductors. in  c. toumazou, f.j. lidgey, and d.g. haigh, editors, analogue ic design:  the current mode approach. peter peregrinus ltd. on behalf of ieee, 1990.  [6] p. furth and a. b. andreou. linearized differential transconductors in sub-  threshold cmos. ieee electronics letters, 31(7):545-547, march 1995.  [7] p. r. gray and r. g. meyer. analysis and design of analog integrated  circuits, pages 183 186. john wiley and sons, new york, 1987.  [8] p. hasler. personal communication.  [9] p. hasler, b. a. minch, c. diorio, and c. mead. an autozeroing amplifier  using pfet hot-electron injection. in proc. ieee intl. symp. on circuits  and systems, volume 3, pages 325-328, atlanta, may 1996.  [10] h. khorramabadi and p. r. gray. high frequency cmos continuous time  filters. ieee j. solid state circuits, sc-19(6):939-948, 1984.  [11] f. krummenacher and n. joehl. a 4 mhz cmos continuous time filter  with on-chip automatic tuning. ieee j. solid-state circuits, sc-23:750-  758, 1988.  [12] r. w. landee, d. c. davis, and a. p. albrecht. electronic designers  handbook, pages 3-18, 3-34, 3-38. mcgraw-hill book co, n.y., 19957.  [13] w. liu. an analog cochlear model: signal representation and vlsi real-  ization. phd thesis, john hopkins university, baltimore, maryland, 1992.  [14] r. f. lyon. analog implementations of auditory models. in arpa work-  shop on speech and natural language. morgan kaufmann publishers, san  mateo ca, 1991.312 neuromorphic systems engineering  [15] r. f. lyon, t. delbr/ick, and c. a. mead. circuits for wide input range  analog rectification and correlation. u.s. patent 5,319,268, june 1994.  [16] r. f. lyon and c. a. mead. the cmos grounded-substrate convention.  caltech computation and neural systems memo, 27, 1993.  [17] c. a. mead. analog vlsi and neural systems, pages 33-36, 67-82.  addison-wesley, reading, ma, 1989.  [18] b. a. minch, c. diorio, p. hasler, and c. a. mead. translinear circuits us-  ing subthreshold floating-gate mos transistors. analog integrated circuits  and signal processing, 9(2):1 13, 1996.  [19] b. nauta and e. seevinck. linear cmos transconductance element for  vhf filters. electronics letters, 25:448-450, 1989.  [20] a. nedungadi and t. r. viswanathan. design of linear cmos transcon-  ductance elements. ieee trans., cas-31(10):891-894, 1984.  [21] i. e. opris and g. t. a. kovacs. large-signal subthreshold cmos  transconductance amplifier. electronics letters, 31(9):718-720, april 1995.  [22] j. ramirez-angulo and e. sÂ£nchez-sinecio. programmable bicmos  transconductor for capacitor-transconductor filters. electronics letters,  28(13):1185-1187, 1992.  [23] r. sarpeshkar, t. delbriick, and c. mead. white noise in mos transistors  and resistors. ieee circuits and devices, 9(6):23-29, november 1993.  [24] r. sarpeshkar, lyon r. f., and c. a. mead. an analog vlsi cochlea with  new transconductance amplifiers and nonlinear gain control. in proc. ieee  intl. conf. on circuits and systems, volume 3, pages 292-295, atlanta,  may 1996.  [25] r. sarpeshkar, lyon r. f., and c. a. mead. nonvolatile correction of  q-offsets and instabilities in cochlear filters. in proc. ieee intl. conf. on  circuits and systems, volume 3, pages 329-332, atlanta, may 1996.  [26] s. szczepanski, j. jakusz, and a. czarniak. differential pair transconduc-  tot linearisation via electronically controlled current-mode cells. electron-  ics letters, 28(12):1093-1095, 1992.  [27] y. tsividis. operation and modeling of the mos transistor, pages 136-  140. mcgraw-hill book company, new york, 1987.  [28] y. tsividis, z. czarnul, and s. c. fang. mos transconductors and inte-  grators with high linearity. electronics letters, 22(5):245-246, 1986.  [29] z. wang and w. guggenbuhl. a voltage-controllable linear mos transcon-  ductor using bias offset technique. ieee j. solid state circuits, sc-  22(3):357-365, 1987.  [30] l. watts, d. kerns, r. f. lyon, and c. mead. improved implementation  of the silicon cochlea. ieee journal solid-state circuits, 27(5):692 700,  may 1992.  [31] g. wilson. linearised bipolar transconductor, ectronics letters, 28(4):390  391, 1992.a low-power wide-linear-range transconductance amplifier 313  [32] p. wu and r. schaumann. tunable operational transconductance amplifier  with extremely high linearity over very large input range. electronics  letters, 27(14):1254-1255, 1991.14 floating-gate mos synapse  transistors  chris diorio, paul hasler, bradley a. minch, and carver mead  physics of computation laboratory,  california institute of technology,  pasadena, ca 91125, usa  ch ris@pcmp.caltech.edu  14.1 introduction  our goal is to develop silicon learning systems. one impediment to achieving  this goal has been the lack of a simple circuit element combining nonvolatile  analog memory storage with locally computed memory updates. existing cir-  cuits [63, 132] typically are large and complex; the nonvolatile floating-gate de-  vices, such as eeprom transistors, typically are optimized for binary-valued  storage [17], and do not compute their own memory updates. although floating-  gate transistors can provide nonvolatile analog storage [1, 15], because writing  the memory entails the difficult process of moving electrons through sio2, these  devices have not seen wide use as memory elements in silicon learning systems.  we have fabricated synapse transistors that not only possess nonvolatile ana-  log storage, and compute locally their own memory updates, but also permit  simultaneous memory reading and writing, and compute locally the product  of their stored memory value and the applied input. to ensure nonvolatile  storage, we employ standard floating-gate mos technology, but we adapt the  physical processes that write the memory to perform a local learning function.  although the sio2 electron transport still is difficult, and does require high  voltages, because our devices integrate both memory storage and local compu-  tation within a single device, we expect them to find wide application in silicon  learning systems.  we call our devices synapse transistors because, like neural synapses [11],  they compute the product of their stored analog memory and the applied in-  put. also like neural synapses, they can learn from the input signal, without316 neuromorphic systems engineering  interrupting the ongoing computation. although we do not believe that a sin-  gle device can model the complex behavior of a neural synapse completely, our  single-transistor synapses do implement a learning function. with them, we in-  tend to build autonomous learning systems in which both the system outputs,  and the memory updates, are computed locally and in parallel.  we have described previously [6, 60, 28] the four-terminal nfet synapse  discussed here. we have also described an analog memory cell that employs  the nfet device [5], and an autozeroing amplifier that employs the pfet  device [12]. we here present the four-terminal nfet synapse in greater detail  than we did previously, and for the first time present the four-terminal pfet  synapse. we have also described previously a three-terminal nfet synapse [7].  although the four-terminal synapses require slightly more layout area than does  this three-terminal device, the additional terminal gives us greater control over  the write and erase processes.  14.2 the synapses  the nfet and pfet synapses each possess a polyl floating gate, a poly2  control gate, and an n-well tunneling implant. both synapses use hot-electron  injection [23] to add electrons to their floating gates, and fowler-nordheim  (fn) tunneling [16] to remove the electrons. the nfet synapse differs from  a conventional n-type mosfet in its use of a moderately doped channel im-  plant. this implant facilitates hot-electron injection. the pfet synapse, by  contrast, achieves a sufficient hot-electron gate current using a conventional  p-type mosfet; no special channel implant is required. we fabricated both  synapses in the 2#m n-well orbit bicmos process available from mosis.  in both synapses, the memory is stored as floating-gate charge. either chan-  nel current or channel conductance can be selected as the synapse output.  inputs typically are applied to the poly2 control gate, which couples capaci-  tively to the polyl floating gate. from the control gate's perspective, altering  the floating-gate charge shifts the transistor's threshold voltage vt, enabling  the synapse output to vary despite a fixed-amplitude control-gate input.  we typically operate the synapses in their subthreshold regime [18], and  select either drain current or source current as the synapse output. we choose  subthreshold operation for three reasons. first, the power consumption of  a subthreshold mosfet typically is less than i#w. second, because the  channel current in a subthreshold mosfet is an exponential function of the  gate voltage, only small quantities of oxide charge are required for learning.  third, the synapse output is the product of a stored weight and the applied  input:  ~vf~ i ~(qfl]+cir~ vi~) q[~ ectvi~ ~/v~n  is = ioe v~ = ioe crv~ = ioe qr e vt = wioe vt (14.1)  where is is the synapse's source current, io is the pre-exponential current, ~ is  the coupling coefficient from the floating gate to the channel, qf~ is the floating-  gate charge, ct is the total capacitance seen by the floating gate, ut is thefloating-gate mos synapse 317  thermal voltage kt/q, cin is the input (polyl to poly2) coupling capacitance,  v~n is the control-gate input voltage, qt = ctut/n, ~' =- acin/ct, w =-  exp(qfg/qt), and, for simplicity, the source potential is assumed to be ground  (vs = 0).  the synapse weight w is a learned quantity: its value derives from the  floating-gate charge, which can change with synapse use. the synapse output  is the product of w and the source current of an idealized mosfet that has  a control-gate input v/n and a coupling coefficient ~ from the control gate to  the channel.  because the tunneling and injection gate currents vary with the synapse  terminal voltages and channel current, w varies with the terminal voltages,  which are imposed on the device, and with the channel current, which is the  synapse output. consequently, the synapses exhibit a type of learning by which  their future output depends on both the applied input and the present output.  14.2.1 the ~fet synapse  top and side views of the nfet synapse are shown in fig. 14.1. its principal  features are the following:  â¢ electrons tunnel from the floating gate to the tunneling implant through  the 350_& gate oxide. high voltages applied to the tunneling implant pro-  vide the oxide electric field required for tunneling. to prevent breakdown  of the reverse-biased pn junction from the substrate to the tunneling im-  plant, we surround the n + tunneling implant with a lightly doped n-  well. tunneling removes electrons from the floating gate, increasing the  synapse weight w.  â¢ electron tunneling is enhanced where the polyl floating gate overlaps  the heavily doped well contact, for two reasons. first, the gate cannot  deplete the n + contact, whereas it does deplete the n- well. thus, the  oxide electric field is higher over the n +. second, enhancement at the  gate edge further augments the oxide field.  â¢ electrons inject from the drain-to-channel space-charge region to the  floating gate. to facilitate injection, we apply a p-type bipolar-transistor  base implant to the mos transistor channel. this implant serves two  functions. first, it increases the peak drain-to-channel electric field,  thereby increasing the hot-electron population in the drain-to-channel  depletion region. second, it raises the floating-gate voltage, causing the  drain-to-gate oxide electric field to favor the transport of injected elec-  trons to the floating gate. injection adds electrons to the floating gate,  decreasing the synapse weight w.  â¢ oxide uniformity and purity determine the initial matching between  synapses, as well as the learning-rate degradations due to oxide trap-  ping. we therefore use the thermally grown gate oxide for all sio2 carrier  transport.318 neuromorphic systems engineering  a. top view  source contact poly2  meta~  ~ fi::~ !i  ~ i  i ~  n* source n + drain  diffusion diffusion  b. side view  interpoly polyl n Ã· well n well  ~ing gate coitact ~/  i  ,  \, \  p substrate tunneling junction:  implant thinox over n-  field-oxide interpoly capacitor channel stop oxide  gate electron gate electron  oxide injection oxide tunneling p substrate  c. electron band diagram  r source channel  â¢  "~ 3. ~ ~drain ,_o > 3.2v  "~= .t noating gate ~ .~ 6.0  ~ ; ~  ~ sio~  n barrier  33.( electron  f ~\ injection  sio_,  electron  -- ~.~ tunneling  2v tunneling ~- "~imp,~lant  position (lain) v  figure 14.1 the nfet synapse, showing the electron tunneling and injection locations.  the three diagrams are aligned vertically. diagrams a and c are drawn to scale; for clarity,  we have exaggerated the vertical scale in diagram b. in the 2~m orbit process, the synapse  length is 48#m, and the width is 17#m. all voltages in the conduction-band diagram  are referenced to the source potential, and we have assumed subthreshold channel currents  (is < 100ha). although the gate-oxide band diagram actually projects into the plane of  the page, for clarity we have rotated it by 90 Â° and have drawn it in the channel direction.  when compared with a conventional nfet, the p-type substrate implant quadruples the  mos gate-to-channel capacitance. with a 50ff interpoly capacitor as shown, the coupling  coefficient between the poly2 control gate and the polyl floating gate is only 0.2. to  facilitate testing, we enlarged the interpoly capacitor to ipf, thereby increasing the coupling  to 0.8.  14.2.2 the pfet synapse  top and side views of the pfet synapse are shown in fig. 14.2. its principal  features are the following:floating-gate mos synapse 319  a. top view  source contact poly2 polyl n + well n well  metal cut control gate floating gate contact  / ~  ~  ,  t i ~- i \ ~=/  i ~ \ -- \, pÃ· source p~ d~ain n weli tunneling junction:  diffusion diffusion thinox over n-  b. side view  interpoly field-oxide interpoly  capacitor oxide ~ ch .... istop ~  ~\ !  gate electron  oxide injection oxide tunneling p suhstrate  c. electron band diagram  Ã·v' -- ~ 0 e --drain  electron/~ i ~.  injection ~-  ~ ~ ~  ~ 3.2v  ~ 6.9 ec~ l ~/ -~ ~8-0 e~  ~ .. ~-~o~.  source channel  n  33.0~ ~ impact ionization   .]vl  floating gate ~ ~_~/tu ling  t tunneling i i~plant  1 ?.2v /  [j~ ~  position (~m) w  figure 14.2 the pfet synapse, showing the electron tunneling and injection locations.  the well contact is not shown. like we did in fig. 14.1, we have aligned the three diagrams  vertically, drawn diagrams a and c to scale, exaggerated the vertical scale in diagram b, ref-  erenced the voltages in the band diagram to the source potential, and assumed subthreshold  (is < 100na) operation. whereas the tunneling process is identical to that in the r~fet  synapse, the injection process is different. as we describe in the text, we generate the elec-  trons for oxide injection by means of hole impact ionization at the transistor's drain. in the  2#rr~ orbit process, the synapse length is 56#m, and the width is 16#m. with a 5off  interpoly capacitor as shown, the coupling coefficient between the poly2 control gate and  the polyl floating gate is only 0.25. we enlarged the interpoly capacitor to ipf in the test  device, thereby increasing the coupling to 0.8.  electrons tunnel from the floating gate to the tunneling implant through  the 350/~ gate oxide. the tunneling implant is identical to that used in  the nfet synapse. as in the nfet synapse, tunneling removes elec-320 neuromorphic systems engineering  trons from the floating gate. however, because the pfet and nfet  synapses are complementary, tunneling has the opposite effect on the  pfet synapse: it decreases, rather than increases, the synapse weight  w.  electrons inject from the drain-to-channel space-charge region to the  floating gate. hole impact ionization generates the electrons for oxide in-  jection. channel holes, accelerated in the drain-to-channel electric field,  collide with the semiconductor lattice to produce additional electron-hole  pairs. the liberated electrons, promoted to their conduction band by  the collision, are expelled rapidly from the drain region by this same  drain-to-channel electric field. electrons that acquire more than 3.2ev  of kinetic energy can, if scattered upward into the gate oxide, inject onto  the floating gate. as in the nfet synapse, injection adds electrons to  the floating gate; however, because the transistor is a pfet, injection  increases, rather than decreases, the synapse weight w.  like the nfet synapse, the pfet synapse uses gate oxide for all sio2  carrier transport.  14.3 the gate-current equation  we intend to build silicon learning systems using subthreshold synapse transis-  tors. because the learning behavior of any such system is determined in part  by the tunneling and injection processes that alter the stored weights, we have  investigated these processes over the subthreshold operating regime.  14.3.1 the tunneling process  the tunneling process, for the nfet and pfet synapses, is shown in the  energy-band diagrams [9] of figs. 14.1 and 14.2, respectively. in fn tunnel-  ing, a potential difference between the tunneling implant and the floating gate  reduces the effective oxide thickness, facilitating electron tunneling from the  floating gate, through the sio2 barrier, into the oxide conduction band. these  electrons are then swept over to the tunneling implant by the oxide electric  field. we apply positive high voltages to the tunneling implant to promote  electron tunneling.  14.3.2 the tunneling equation  the data of fig. 14.3 show tunneling gate current versus the reciprocal of the  voltage across the tunneling oxide. we fit these data with an fn fit [16, 22]:  ig : ~vo2ze-v~o~ (14.2):[oao adams aa~ uoql stloaaaaia osoq& "pu~q uo~aanpuoa ap~xo aqa oau~ 'aa~aa~q  uo~aunj-~ao~ ~o~s - ~s aug oq~ aaao 'iouuvqa ao~s~su~a~ oqa moaj aao[u~ suoaa  -aala "xiaa~ aadsaa 'u~i pu~ i'~i "s$~d jo sm~asv~p puvq-*$aaua aqa u~ u~oqs s~  'sosd~uxs Â£add pu~ &aau aqa qaoq aoj '[0g] ssoaoad uo~aao[u~ uoaaaolo-ao q oqÂ£  ssoaoad uo~aaafu i uoaaao[~-~o h oqÂ£ g'g'~  â¢ uouaulouaqd aspo uv .~it.avun.ad st. samst.suva~, osdvuss oq:~  u! ~u!ioaunz 'o~mioa op!xo qa!m xli~t.auouodxo sos~oaau~ ~u~iouun a nd osn~aoa  â¢ +u aqa sdvlso*o o~v$ ~u~o~ pou$~iv-jios oqa aaoq~ aoq$~q s~ pia~ ap~xo aq~  '~a~uoa ilo~ +u oq~ o~oldop Â£iqepaadd~ ~ouu~a o~ oq~ osn~ao h "~uaaana  su~iauuna arl a~ ra~ pu~ 'a$~aioa ap~xo aa~aaa~a ara ~upnpaa 'iia~ _u padop  xiar$~i ara m uo~$aa uo~aalda p v saanpu~ aav~ ~ud~o ~ ara asn~aaq 'suoaa~m  i~au~i u~ 'qasuo i aspa +u-oa-oa~$ aqa oa g'~i '~d jo ~a~p aq~ paz~i~maou o~  â¢ saumsuoa a~ aa~ Â° a pu~ '~a '} oaoqm   (*qa + xÂ°a)  = q  o a  :xiasoia aaoui m~p [e:luault.,iadxa aqa lgi oa {uo[.~:etlba nÂ¢i aq:l o9~ '~qa 'i~,taua:~ od  m.-:~it.n q ~ pp~ a~ qat.qa~ u[. 'a~ i~at.a!dma u~ ~oqs osi~ 033 . ",/aaatu~:i~d ~t ~ st  o~ puv !ap!xo aav$ ~0~ aqa uaat$ '$u!iauuna ~o!s jo [gi] xaaans luaaaa ~ q:g~  :~uaas!suoa s.[ a~86 = ~'a !a~ioa ap.[xo aqa ~t. *o a !auaaana oa~ aqa s! ~i osoqa~  â¢ uosgedujo3 aoj (au!l  paqsep) ~!j tu!aqpjon-~al~ao-i leuo!~uaauo3 aql aaoqs osle a~ :,~lasol :) ajo~ e~ep le~uampadxa  aq~ ~j o~ '~qa 'a~e~loa u[-~l[nq e s~old~a (au h p[los) ~j iedg#~a jn 0 '~xa~ aq~ u[ ssn~s~p  a~ ~eq~ suoseaj joj 'suojd~ ieau~l u~ 'q~ua i a~pa +~-o~-a~e~ uo~unf-~u~lauun~ aq~ o~  e~ep aq~ pazhe~jou a~ 'a~e~ ~u~eolj aq~ pue uo[~dun[ ~uhauun ~ aq~ uaa~aq aduajajj[p  le~lua~od aql aq o~ ~Â° a au~jap a~ zoa/i snsjaa 6[ ~uajjn3 (a~e~) ~uhauun k e'~[ ajn~[j  (a/i) ~izll oa zp!xo / i-  i~0"0- ee0"0- 9e0"0-  i i i i lroo- l~lmo-  ' ' o~-Â°! ...~  ~ ~ 6~- o[  i ~.oi  91-0i  iÃlee'i=~i .... oi ~1-  ._o[  fl_ol r  >  g  ig~ hsdvhixs sobi ~j,vd-dnijno'id322 neuromorphic systems engineering  to the floating gate by the oxide electric field. successful injection, for both  the nfet and pfet synapses, requires that the following three conditions be  satisfied: (1) the electrons must possess the 3.2ev required to surmount the  si - sio2 barrier, (2) the electrons must scatter upward into the gate oxide, and  (3) the oxide electric field must be oriented in the proper direction to transport  the electrons to the floating gate.  nfet injection. in a conventional n-type mosfet, requirements 1 and  2 are readily satisfied. we merely operate the transistor in its subthreshold  regime, with a drain-to-source voltage greater than about 3v. because the  subthreshold channel-conduction band is flat, the drain-to-channel transition is  steep, and the electric field is large. channel electrons are accelerated rapidly  in this field; a fraction of them acquire the 3.2ev required for hot-electron  injection. a fraction of these 3.2ev electrons naturally scatter, by means of  collisions with the semiconductor lattice, upward into the gate oxide.  it is principally requirement 3 that prevents injection in a conventional  nfet. subthreshold operation typically implies gate-to-source voltages less  than 0.sv. with the drain at 3v, and the gate at 0.sv, the drain-to-gate elec-  tric field opposes transport of the injected electrons to the floating gate. the  electrons are instead returned to the drain.  in the synapse transistor, we promote the transport of injected electrons to  the floating gate by increasing the bulk channel doping. the additional dopant  increases the channel surface-acceptor concentration, raising the transistor's  threshold voltage from 0.sv to 6v. with the drain at 3v, and the gate at 6v,  the channel current still is subthreshold, but now the oxide electric field sweeps  injected electrons over to the floating gate, rather than returning them to the  silicon surface.  pfet injection. because the pfet channel current comprises holes, pfet  hot-electron injection is different from nfet injection. we accelerate channel  holes in the drain-to-channel depletion region of a subthreshold pfet. a frac-  tion of these holes collide with the semiconductor lattice at energies sufficient  to liberate additional electron-hole pairs. the ionized electrons, promoted to  their conduction band by the collision, are expelled from the drain by the drain-  to-channel electric field. if these ionized electrons are expelled with more than  3.2ev of kinetic energy, they can inject onto the floating gate.  in the pfet synapse, like in the nfet, injection requirements 1 and 2 are  easily satisfied. we merely operate the transistor in its subthreshold regime,  with a drain-to-source voltage greater than about 6v. the higher drain-voltage  requirement, when compared with the nfet synapse, is a consequence of the  two-step injection process.  in a subthreshold pfet, the gate-to-source voltage typically is less than  1v; if the drain-to-source voltage exceeds 6v, the gate voltage must exceed  the drain voltage by at least 5v. the oxide electric field supports strongly the  transport of injected electrons to the floating gate, and requirement 3 is alwaysfloating-gate mos synapse 323  satisfied. unlike conventional nfet transistors, conventional pfet transistors  naturally inject electrons onto their floating gates (at sufficient drain-to-source  voltages); we do not need to add a special channel implant to facilitate injection.  14.3.4 the injection equation  10 -4  ~ 104  Â¢9 -6 v 10  m  ~ 10 -7  ~ 10 .8 ~9 ~)  n ~0 10-9  10 -~o 3  pfet [ 31.6 \2 ,,Â¢' % ig -3-t~)- ~ -7-=5.50x10 e ~  l \// ? nf~ ~ ~ 6i _[_ 8.06 ~  r ~9.7~ 3e-i~] ~  ~ i s ~  /  i i i  4 5 6 7 8 ~ 10 1~1 12  &an-to-chapel voltage (v)  figure 14.4 injection efficiency versus drain-to-channel voltage, for both the nfet and  pfet synapses. we held the gate-to-channel voltages fixed during the experiments. for  the nfet, vg c = 5.66v; for the pfet, vgc = 1.95v. in the nfet synapse, when the  drain voltage exceeds the floating-gate voltage, the oxide e-field tends to return the injected  electrons to the silicon surface, rather than transporting them to the floating gate. as a  result, for drain-to-channel voltages near vgc = 5.66v, the nfet data deviate from the  fit.  the data of fig. 14.4 show injection efficiency (gate current divided by  source current) versus drain-to-channel potential, for both the nfet and pfet  synapses. the data are plotted as efficiency because, for both devices, the gate  current is linearly proportional to the source current over the entire subthresh-  old range. because the hot-electron injection probability varies with the drain-  to-channel potential, we reference all terminal voltages to the channel. we can  re-reference our results to the source terminal using the relationship between  source and channel potential in a subthreshold mosfet [2, 8]:  â¢ ~ ~v~ + ~o (14.4)  where q2 is the channel potential, vyg is the floating-gate voltage, ~ is the  coupling coefficient from the floating gate to the channel, and ~0 derives from  the mos process parameters.  for both synapses, the injection efficiency is independent, to first-order, of  the floating-gate-to-channel voltage, as long as v.fg > v d (where v~g and vd324 neuromorphic systems engineering  are the floating gate and drain voltages, respectively). in the pfet synapse,  this condition is always satisfied. in the nfet synapse, this condition is not  necessarily satisfied; the data of fig. 14.4 show what happens when we sweep  the nfet drain from voltages much less than v/9, to voltages much greater  than vi~. as va approaches vi~ , the oxide voltage becomes small, and the gate  current drops.  we fit the injection data of fig. 14.4 empirically; we are currently analyzing  the relevant electron-transport physics to derive equivalent analytic results. for  the nfet synapse, we chose not to fit the region where v~ > v~ because, at  such high drain voltages, the gate currents are too large for use in a practical  learning system. for both synapses, then,  ~ ~ ~2  i 9 = oise-' vdc+----~,, (14.5)  where vdc is the drain-to-channel potential and 7, v~, and vn are measurable  device parameters.  14.3.5 the gate-current equation  because the tunneling and injection gate currents flow in opposite directions,  we obtain the final gate-current equation, for both synapses, by subtracting  eqn. 14.5 from eqn. 14.2 :  v~ _~ v~ ~  ~ vox+vbi ig ~(vo~ + vbi)2e - -- ~ise ' ve~+v,' (14.6)  the principal difference between the nfet and pfet synapses is the sign of  the learning. in the nfet, tunneling increases the weight, whereas injection  decreases it; in the pfet, tunneling decreases the weight, whereas injection  increases it.  14.3.6 impact ionization  we choose source current as the synapse output. because, for both synapses,  the activation energy for impact ionization is less than the barrier energy for  injection, a drain-to-channel electric field that generates injection electrons also  liberates additional electron-hole pairs [21]. for both synapses, the drain cur-  rent therefore can exceed the source current. if we choose drain current, rather  than source current, as the synapse output, we can rewrite the gate-current  equation in terms of drain current using a (modified) lucky-electron [24] for-  mulation:  where id is the drain current and Â¢, v~, and v~ are measurable device  parameters. in fig. 14.5, we plot impact-ionization data for both synapses.floating-gate mos synapse 325  4 nf'et /~ 481 ' i[ 1"015  ld -- 4 -~-vdc_~-.87 ] ~  i i s -- ~ i 7270 ,~ ~ ~-=1+2.37x10 e fet ~ ~b =~  \ zd ~ -x/~ ~  ~ ~ ~]1.olo $ ~ ~ t =1+5"%~10 e ~  ~ ~  ~ ~  â¢ ~ ~. ~.oo5 .~  ~ ~  ~ ~  ~ ~  1 ~ _ 1.000 2 4 6 8 10 12  drain-to-channel voltage (v)  figure 14.5 impact ionization versus drain-to-channel potential, for both the nfet and  pfet synapses. impact ionization in the ~fet is markedly more efficient than in the pfet,  for two reasons. first, as a consequence of its bulk p-type substrate implant, the ~fet  synapse experiences a higher drain-to-channel electric field than does the pfet, thereby  increasing the ionization likelihood. second, the impact-ionization process is naturally more  efficient for electrons (the nfet charge carriers) than it is for holes (the pfet charge  carriers).  14.4 synaptic arrays  a synaptic array, with a synapse transistor at each node, can form the basis of  a silicon learning system. we fabricated simplified 2 Ã 2 arrays to investigate  synapse isolation during tunneling and injection, and to measure the synapse  weight-update rates. because a 2 Ã 2 array uses the same row-column addressing  employed by larger arrays, it allows us to characterize the synapse isolation and  weight-update rules completely.  14.4.1 the ~fet array  the nfet array is shown in fig. 14.6. we chose, from among the many  possible ways of using the array, to select source current as the synapse output,  and to turn off the synapses while tunneling. we applied the voltages shown  in table 14.1 to read, tunnel, or inject synapse {1, 1} selectively, while ideally  leaving the other synapses unchanged.  the tunneling and drain terminals of the array synapse transistors connect  within rows, but not within columns. consequently, the tunneling and injec-  tion crosstalk between column synapses is negligible. a synapse's tunneling  gate current increases exponentially with the oxide voltage vox, (vo=, in turn,  decreases linearly with vyg), and the hot-electron gate current increases lin-  early with the channel current is, (is, in turn, increases exponentially with326 neuromorphic systems engineering  column 1  sotlrÂ£'e  *x  row 1 tunneling ~  row 1 drain )--  im  row 2 tunneling ~  row 2 drain )~ gate  {2,1~ !  i  i column 1 column 2  solwce column 2  gate  i  figure 14.6 a 2 x 2 array of nfet synapses. because the row synapses share common  tunneling and drain wires, tunneling or injection at one row synapse can cause undesired  tunneling or injection at other row synapses.  table 14.1 the terminal voltages that we applied to the array of fig. 14.6, to obtain the  data of figs. 14.7 and 14.8.  col 1 col i col 2 col 2 row 1 row 1 row 2 row 2  gate source gate source drain tun drain tun  read +5 0 0 0 +1 0 0 0  tunnel 0 0 +5 0 0 +31 0 0  inject +5 0 0 0 3.15 0 0 0  vfg). consequently, the isolation between row synapses increases exponen-  tially with the voltage differential between their floating gates. by using 5v  control-gate inputs, we achieve about a 4v differential between the floating  gates of selected and deselected synapses; the resulting crosstalk between row  synapses is < 0.01% for all operations.  to obtain the data in fig. 14.7, we initially set all four synapses to is =  100pa. we tunneled the {1, 1} synapse up to 100na, and then injected it back  down to 100pa, while measuring the source currents of the other three synapses.  as expected, the row 2 synapses were unaffected by either the tunneling or the  injection. coupling to the {1, 2} synapse also was small.  to obtain the data in fig. 14.8, we first set all four synapses to is = 100na.  we injected the {1, 1} synapse down to 100pa, and then tunneled it back up  to 100na. as in the experiment of fig. 14.7, crosstalk to the other synapses  was negligible. our large (lpf) gate capacitors provide 80% voltage coupling  between a synapse's control and floating gates, minimizing crosstalk at thefloating-gate mos synapse 327  167  < "~ 10 -8  g  ~9 ,~ ~= 16 9  m  10 -~0 / / ! /! ' { 1,1 } sy~pse  11,2} synaps~  {2,1} and {2,2} synapses  500 10~00 time (s) 1500  figure 14.7 isolation in a 2 Ã 2 array of nfet synapses. source current is the synapse  output. the {i, i} synapse first is tunneled up to 100ha, then is injected back down  to 100pa. the tunneling voltage, referenced to the substrate potential, is vtun -- 31v;  the injection voltage is vds : 3.15v. crosstalk to the {i, 2} synapse, defined as the  fractional change in the {i,2} synapse divided by the fractional change in the {i, i}  synapse, is 0.006% during tunneling, and is 0.002% during injection.  [i i : : : iii::2~iy{:,:} e synaps~s~  : ~ 10.8 Â¸:Â¸::Â¸:::Â¸:Â¸::::Â¸Â¸:Â¸:Â¸ :Â¸ Â¸:Â¸:Â¸ Â¸Â¸Â¸:Â¸:Â¸:Â¸Â¸Â¸Â¸Â¸Â¸Â¸Â¸Â¸Â¸:Â¸Â¸Â¸Â¸Â¸:Â¸  g  ~ 10 "9  10 -~o  0 500 1000 1500 time (s)  figure 14.8 results from the same experiment as in fig. 14.7, but here the {i, 1} synapse  first is injected down to 100pa, then is tunneled back up to 100ha. crosstalk to the {i, 2}  synapse is 0.001% during injection, and is 0.00:2% during tunneling.  expense of increased size and decreased weight-update rates. we intend to  fabricate future synapses with smaller gate capacitors.328 neuromorphic systems engineering  14.4.2 the pfet array  column l column 1 column 2 column 2 source gate source gate  â¢  row 1 tunneling )~ );  row t drain }--  row 2 tunneling )--- ~  row 2 drain )--' ~  i i  i i ....  {2,2~ ~ ....  figure 14.9 a 2 Ã 2 array of pfet synapses. the well connections are not shown. as  in the nfet array, because the row synapses share common tunneling and drain wires,  tunneling or injection at one row synapse can cause undesired tunneling or injection at other  row synapses.  the pfet array is shown in fig. 14.9. we grounded the p-type substrate,  applied +12v to the n-type well, and referenced all terminal voltages to the  well potential.  table 14.2 the terminal voltages that we applied to the array of fig. 14.9, to obtain the  data of figs. 14.10 and 14.11.  col 1 col 1 col 2 col 2 row 1 row 1 row 2 row 2  gate source gate source drain tun drain tun  read -5 0 0 0 -5 0 0 0  tunnel -5 0 0 0 -5 +28 0 0  inject -5 0 -4 0 -9.3 0 0 0  we again chose source current as the synapse output, but we left the pfet  synapses turned on while tunneling, rather than turning them off like we did  for the nfet array experiment. we applied the voltages shown in table 14.2  to read, tunnel, or inject synapse {1, 1} selectively, while ideally leaving the  other synapses unchanged.  to obtain the data in fig. 14.10, we initially set all synapses to is -- 100pa.  we injected the {1, 1} synapse up to 100ha, and then tunneled it back down to  100pa. to obtain the data in fig. 14.11, we performed the opposite experiment.floating-gate mos synapse 329  10 -7  <  '~o 10-8  ~-  ~ 10 -9  10 -~o tj ~  i i i i 500 1000 1500 2000  time (s) , i i i  /  y { 1,2} synapse  12,1} and {2,2} synapses  figure 14.10 isolation in a 2 Ã 2 array of pfet synapses. source current is the synapse  output. the {1, i} synapse first is injected up to 100ha, then is tunneled back down to  100pa. the injection voltage is vds ---- -9.3v; the tunneling voltage, referenced to the  well potential, is vtun = 28v. crosstalk to the {1, 2} synapse, defined as the fractional  change in the {i, 2} synapse divided by the fractional change in the {1, i} synapse, is  0.016% during injection, and is 0.007% during tunneling.  <  ~ 10.~  ~ ~ ~ 10 .9  10 -~o i i i ~  ....... {2,1} and {2~} synapses )  { i'2} ;ynapse i  500 1000 1500 2000  time (s)  figure 14.11 results from the same experiment as in fig. 14.10, but here the {1, 1}  synapse first is tunneled down to 100pa, then is injected back up to 100~ta. crosstalk to  the {i, 2} synapse is 0.005% during injection, and is 0.004% during tunneling.  for the pfet array, like for the nfet array, the crosstalk between column  synapses was negligible, and the crosstalk between row synapses was small.330 neuromorphic systems engineering  when we injected the {1, 1} synapse, we applied -4v, rather than 0v, to  the (1,2} synapse's control gate. we did so because hot-electron injection  can occur in a pfet synapse by a mechanism different from that described  in section 14.3: if the floating-gate voltage exceeds the well voltage, and the  drain-to-channel potential is large, electrons can inject onto the floating gate  by means of a non-destructive avalanche-breakdown phenomenon [23] at the  mos surface.  14.5 the synapse weight-update rule  10 -9  ~10 -~  ~ -13  .~  ~ -15  e~ ' = -  â¢ .  10 "17 ........ ~ ........ ~ ......  -i0 -9 -8 ' " -7 lo lo lo lo source current (a)  figure 14.12 the magnitude of the temporal derivative of the source current versus the  source current, for an nfet synapse with a continuous tunneling-oxide current. we tunneled  the {i, i} synapse up as in fig. 14.7, with the source at ground and the ground-referenced  tunneling voltage stepped from 29v to 35v in 1v increments. the mean slope is +0.83.  we repeated the experiments of figs. 14.7 and 14.10, for several tunneling  and injection voltages; in figs. 14.12 through 14.15 we plot, for the nfet and  pfet synapses, the magnitude of the temporal derivative of the source current  versus the source current. we held the control-gate input v~n fixed during these  experiments; consequently, the data show the synapse weight updates 5w/st,  as can be seen by differentiating eqn. 14.1. starting from the gate-current  equation, eqn. 14.6, we now derive weight-update rules that fit these data.  14.5.1 tunneling  we begin by taking the temporal derivative of the synapse weight w, where  w = exp(qfg/qt):  5w_ w 5qi 9 _ w i~ (14.7)  5t qt qt--floating-gate mos synapse 331  10_91 ........ ! ........ : .... c  t  ~ i0-' ...... < ~.~  .~  ~= 10_ ~  10 17 ......... _ 1o-" .... ...... ~'~~  ~ i0 -'3 ...... ~  -10 -9 -8 -7 10 10 10 10  source current (a)  figure 14.13 the magnitude of the temporal derivative of the source current versus the  source current, for an nfet synapse with a continuous hot-electron oxide current. we  injected the {111} synapse down as in fig. 14.7, with the source at ground and the ground-  referenced drain voltage stepped from 2.9v to 3.5v in 0.1v increments. the mean slope  is -1.76; we have added the minus sign because the synapse weight is injecting down.  10 .9 ~ 10 -~ ~=  ~10 ~  ~ 10-~5 =~  10 "17 ,, ~ , ~  -i0 ...... -9 ....... -8 ....... '-7 i0 i0 i0 lo  source current (a)  figure 14.14 the magnitude of the temporal derivative of the source current versus  the source current, for a pfet synapse with a continuous tunneling-oxide current. we  tunneled the {1, 1} synapse down as in fig. 14.10, with the source and well at -i-12v and  the tunneling voltage, referenced to the well potential, stepped from 26v to 32v in iv  increments. the mean slope is -0.99; we have added the minus sign because the synapse  weight is tunneling down.o lo -9  10 "~ : ....... ; .... â¢ .......  10 "13  10 -15  10.17 '  10 332 neuromorphic systems engineering  -10 -9 -8 -7 l0 l0 l0  source current (a)  figure 14.15 the magnitude of the temporal derivative of the source current versus the  source current, for a pfet synapse with a continuous hot-electron oxide current. we  injected the {1, 1} synapse up as in fig. 14.10, with the source and well at +i2v and  the drain voltage, referenced to the source potential, stepped from -8.0v to -11.0v in  -0.5v increments. the mean slope is +1.89.  in appendix a.1, we substitute for the tunneling gate current using  eqn. 14.3, and solve for the tunneling weight-update rule:  5w 1  -- ~ w (1-~) (14.8)  5t "rtun  where a and ~-tun are defined in eqns. 14.a.3 and 14.a.4, respectively. equa-  tion 14.8 fits the tunneling weight-update data for both synapses. in the nfet  synapse, 0.12 < a < 0.22; in the pfet, 0.01 < a < 0.05.  14.5.2 injection  we begin with 5w/fit from eqn. 14.7. in appendix a.2, we substitute for the  injection gate current using eqn. 14.5, and solve for the injection weight-update  rule:  5w _ ~1 w(2_~) (14.9)  5t ti~y  where Â¢ and ~-~,~j are defined in eqns. 14.a.8 and 14.a.9, respectively. equa-  tion 14.9 fits the injection weight-update data for both synapses. in the nfet  synapse, 0.14 < Â¢ < 0.28; in the pfet, 0.08 < ~ < 0.14.floating-gate mos synapse 333  14.5.3 the weight-update rule  we obtain the synapse weight-update rule by adding eqns. 14.8 and 14.9, with  a leading (Â±) added because the sign of the updates is different in the nfet  and pfet synapses:  (~w_~ "~Â± [__1 w(l_a) - 1 w(2-~)] (14.10)  [ ttun tinj  for nfet synapses, we use the (+) in eqn. 14.10; for pfet synapses, we  use the (-).  14.5.4 learning-rate degradation  sio2 trapping is a well-known issue in floating-gate transistor reliability [3]. in  digital eeproms, it ultimately limits the transistor life. in the synapses, trap-  ping decreases the weight-update rate. however, because a synapse's weight w  is exponential in its floating-gate charge qfg (see eqn. 14.1), the synapses in a  subthreshold-mos learning system will transport only small quantities of total  oxide charge over the system lifetime. we tunneled and injected lnc of gate  charge in both synapses, and measured a ~ 20% drop in both the tunneling and  injection weight-update rates. because lnc of charge represents an enormous  change in synapse weight, we believe that oxide trapping can be ignored safely.  14.6 conclusion  we have described complementary single-transistor silicon synapses with non-  volatile analog memory, simultaneous memory reading and writing, and bidirec-  tional memory updates that are a function of both the applied terminal voltages  and the present synapse output. we have fabricated two-dimensional synaptic  arrays, and have shown that we can address individual array nodes with good  selectivity. we have derived a synapse weight-update rule, and believe that we  can build autonomous learning systems, that combine single-transistor analog  computation with weight updates computed both locally and in parallel, with  these devices. finally, we anticipate that our single-transistor synapses will  allow the development of dense, low-power, silicon learning systems.  appendix: a  a.1 the tunneling weight-update rule  we begin with the temporal derivative of the synapse weight w (see eqn. 14.7):  5w w - qtiq_ (14.a.1) 6t  we substitute eqn. 14.3 for the gate current ig:+ o, +  :~ pue ~i jo  smaaa u! ~ iv!auaaod aa~jans art1 aoj anos ah~ '~'v'iri pu~ i~'h 'suba su!sfl  (~'v'h) ~ aÂ°i = ~i  Â£q [~i] so~ioa oaanos pu~ oa~$-$u!a~olj oqa oa poa~iaa s~. auaaana  aaanos aqa 'Â±adgoi~ oav$-$u!a~olj ploqsaaqlq ns v u i "~i pu~ ~va ~o stuaoa m  '~pa 'iw.~uaa Â°d iouuvqa-ol-u!eap s,aoas~.su~aa osdvuks ~ su!a!a~aa Â£q utflaq a3a  31n~1 3.i.vcldn-.i.h913~ noi.i.:)3rni 3h.i. ~:'v  unaa, gp  -- ~ --  (~-t)m i m9  :olna oa~pdn  -aq$.to~ su!ioutln a oqa ao$ oa 'uv'h "ub~t olut. um.~ oanat.lsqns oa,, 'xli~u~.~ i  (~v>o "+2"" ~: -(~'~a - *~a + ~)7~ -- ~'~"  ougop pu~ 'm  jo auopuodopu! 'lu~asuoa ~ oq ol ~(eta - ~a + ~*'~91) oa~mt.xoadd~ am 'xlmois  sosu~qa asvaloa oa~$-$ut.lvolj at d 'sauaaana oaanos ploqsoaqaqns aoj 'osnvaa~  (e'v'>i) ~(~a + ~)~ _= ~ ~;2oa  o~oq~  (sv>i) (~_~)~ ~+~o,~ ~:('~ - *~a + ~)t ~  .x- m9  :olna  oaupdn-aq$!o~ su!iouuna oq~ aoj oalos pu~ '~ox/eto* o = ~a olna~asqns o~  ~ , ~0 ~9 c~a+'"a) ,~a+~,.aa(~fa - ~a + n;~ ~  ~aÂ°a -- Â°a- m9  :0aios pti~ 'x + i ~ i-( x -- i) xq ~uouodxo oq:l  puvdxo '~c a << ~qa + un~a ~l[~. omnss~ '(.Â£[oa!~odsoa 'so~:~ioa o~-~ti.l:l~olj pu~  opou-~u!iouum oti~ oa~ ~'a pu~ ~'~9i ojoqa~) ~ga - ~a -- ~Â°a o:m~!~sqns o3a  .o., ~0 ~ ,~.,+.o., ~(~a + - ~ a)~ m9  dni~ih~inidn~i si~i~&sxs dihd]:io!a!o]:ifern 17ggfloating-gate mos synapse 335  we now solve for v~:  vac = va - ~ = vds - ~o - ut ln ( i~o ) (14.a.6)  the injection gate current ig is given by eqn. 14.5. we add a minus sign  to the gate current, because hot-electron injection decreases the floating-gate  charge, and substitute for vac using eqn. 14.a.6:  ( v~  t -- i~ = -~1~ ~"~Ã·~-~;---~t'n(#Â°)  v 2 ~ u~ -~ :__,is~--(vd~+vv-'o) [1-- vd~+wv-'o ln(~)]  we expand the exponent by (1 - x) -~ ~ 1 + 2x, substitute for i~ using  eqn. 14.1, and solve:  [ ( (l~e)~t v'n -~  i, ~ -,ioe[ ~- ' ~+~'-*Â°) ~] w (1-~) (14.1.7)  where  ~  (vds + v~ - %)3  we substitute eqn. 14.a.7 into 6wilt, eqn. 14.a.1,  ( 5w ~71o i~- v~+v,~-~o ~- ~ w (~-~)  we define (14.a.8)  [( qt e ,~4-~,-~o - vt (14.a.9) _ __ tiny ~ 7]io  finally, we substitute tiny into eqn. 14.a.9 to get the injection weight-update  rule:  6w 1  _ __ w(2-~) 6t tiny  acknowledgments  lyn dupr4 edited the manuscript. this work was supported by the office of naval  research, by the advanced research projects agency, by the beckman hearing insti-  tute, by the center for neuromorphic systems engineering as a part of the national  science foundation engineering research center program, and by the california  trade and commerce agency, office of strategic technology.336 neuromorphic systems engineering  references  [1] t. allen, a. greenblatt, c. mead, and j. anderson. writeable analog  reference voltage storage device. u.s. patent no. 5,166,562, 1991.  [2] a. g. andreou and k. a. boahen. analog vlsi signal and information  processing. in m. ismail and t. fiez, editors, neural information process-  ing ii, pages 358-413. mcgraw-hill, new york, 1994.  [3] s. aritome, r. shirota, g. hemink, t. endoh, and f. masuoka. reliability  issues of flash memory cells. in proc. of the ieee, volume 82-5, pages 776-  787, 1993.  [4] p. churchland and t. sejnowski. the computational brain. mit press,  1993.  [5] c. diorio, p. hasler, b. a. minch, and c. mead. a high-resolution non-  volatile analog memory cell. in proc. ieee intl. syrup. on circuits and  systems, volume 3, pages 2233-2236, 1995.  [6] c. diorio, p. hasler, b. a. minch, and c. mead. a single transistor sil-  icon mos device for long term learning. u.s. patent office serial no.  08/399966, mar. 7 1995.  [7] c. diorio, p. hasler, b. a. minch, and c. mead. a single-transistor silicon  synapse. ieee trans. electron devices, 43(11):1972 1980, 1996.  [8] c. c. enz, f. krummenacher, and e. a. vittoz. an analytical mos  transistor model valid in all regions of operation and dedicated to low-  voltage and low-current applications. analog integrated circuits and signal  processing, 8:83-114, 1995.  [9] a. s. grove. physics and technology of semiconductor devices. john  wiley & sons, new york, 1967.  [10] p. hasler, c. diorio, b. a. minch, and c. mead. single transistor learning  synapses. in advances in neural information processing systems 7, pages  817-824. mit press, cambridge, ma, 1995.  [11] p. hasler, c. diorio, b. a. minch, and c. mead. single transistor learning  synapses with long term storage. in ieee intl. syrup. on circuits and  systems, volume 3, pages 1660-1663, 1995.  [12] p. hasler, b. a. minch, c. diorio, and c. mead. an autozeroing amplifier  using pfet hot-electron injection. in proc. ieee intl. syrup. on circuits  and systems, volume 3, pages 325-328, atlanta, may 1996.  [13] b. hochet, v. peiris, s. abdo, and m. j. declercq. implementation of  a learning kohonen neuron based on a new multilevel storage technique.  ieee j. solid-state circuits, 26(3):262-267, 1991.  [14] p. hollis and j. paulos. a neural network learning algorithm tailored for  vlsi implementation. ieee tran. neural networks, 5(5):784-791, 1994.  [15] j. lazzaro, j. wawrzynek, , and a. kramer. systems technologies for  silicon auditory models. ieee micro, 14(3):7 15, june 1994.floating-gate mos synapse 337  [16] m. lenzlinger and e. h. snow. fowler-nordheim tunneling into thermally  grown sio2. j. of appl. phys., 40(6):278-283, 1996.  [17] f. masuoka, r. shirota, and k. sakui. reviews and prospects of non-  volatile semiconductor memories. ieice trans., e 74(4):868-874, 1991.  [18] c. mead. scaling of mos technology to submicrometer feature sizes. j.  of vlsi signal processing, 8:9-25, 1994.  [19] c. a. mead. analog vlsi and neural systems. addison-wesley, reading,  ma, 1989.  [20] j. j. sanchez and t. a. demassa. review of carrier injection in the  silicon/silicon-dioxide system. in iee proceedings-g, volume 138-3, pages  377 389, 1991.  [21] w. shockley. problems related to pn junctions in silicon. solid-state  electronics, 2(1):35-67, 1961.  [22] s. m. sze. physics of semiconductor devices. john wiley & sons, new  york, 1981.  [23] e. takeda, c. yang, and a. miura-hamada. hot-carrier effects in mos  devices. academic press, san diego, ca, 1995.  [24] s. tam, p. ko, and c. hu. lucky-electron model of channel hot-electron  injection in mosfet's. ieee trans. electron devices, ed-31(9):1116-  1125, 1984.15 neuromorphic synapses  for artificial dendrites  wayne c. westerman, david p. m. northmore, and john. g. elias  departments of electrical engineering and psychology  university of delaware newark, de 19716  15.1 introduction  in the past few years several researchers have developed general-purpose spiking  silicon neurons, or neuromorphs, which attempt to capture various functional  behaviors of biological neurons [19, 23, 24]. these cmos analog vlsi im-  plementations are intended to be the computational elements linking sensory  input to motor output in artificial nervous systems. systems consisting of  thousands or millions of neuromorphs may be required to realize certain basic  behaviors. however, until appropriate methods are found to specify and adapt  network parameters (e.g., connectivity, synapse weights, dynamics, morphol-  ogy), these systems will remain beyond our grasp. though various hebbian  or correlative adaptation rules have been applied to perceptron based net-  works (e.g., [77, 11, 141]) and single-compartment model neurons [12, 26], these  rules are virtually untested with multi-compartment spiking neuromorphs that  model the spatially extensive dendrites found in biological systems. this paper  discusses the vlsi design and performance of just one of the adaptive net-  work parameters: variable synaptic conductances that mimic the fast chemical  synapses found on dendrites throughout high-order nervous systems.340 neuromorphic systems engineering  this paper is organized as follows. first, we present an overview of biological  synaptic function and discuss those aspects that need to be carefully mimicked  by artificial synapses. second, we discuss existing synapse designs for artificial  vlsi neural network chips and consider implications of the time-multiplexed  inter-chip communication schemes that have recently been adopted for neuro-  morphic networks. third, we evaluate approximations that can simplify neu-  romorphic, variable weight synapse designs within artificial dendrites. finally,  we present results from two complementary implementations, the conductance  array and the self-timed synapse, and compare the engineering efficiency, mod-  eling accuracy and on-chip adaptation potential of each.  we will use the following conventions to distinguish the terms synaptic  weight and synaptic efficacy. synaptic weight is the value of the index or con-  trol signal that modulates artificial synapse conductance or duration. when a  synapse is activated by a pre-synaptic spike its conductance changes from zero  to a value set by its weight. the synapse remains activated for a time period  called the duration. the synaptic efficacy is the total charge transferred by  an activated synapse, and for conductance synapses is a function of both the  membrane potential just before activation and the integral of the conductance  over the duration.  15.2 modeling synapses as switchable conductances  though current sources are easy to build in cmos technology, they are in-  appropriate models for the fast chemical synapses found in central nervous  systems [36]. the following discussion of biological synaptic function will show  that synaptic activations are more properly modeled as modulations in conduc-  tance, and thus, charge flow will depend upon the product of the conductance  and the difference between the channel reversal potential and the intracellular  potential. when discussing artificial synapses we will use synapse supply po-  tential as the analog to the biological channel reversal potential. for reviews  of general synaptic function, see [21, 30, 33, 34].  when an axonal spike reaches a pre-synaptic terminal, a complex chemical  cascade causes packets of neurotransmitter to be released across the correspond-  ing synaptic cleft, where they briefly bind to receptor molecules embedded in  the post-synaptic membrane. the transmitter-receptor complex produces a  channel that permits small ions to enter and exit the post-synaptic cell, thus  modulating the intracellular potential. in the case of fast synapses the channel  closes after about a millisecond and the transmitter molecule detaches from the  receptor. the collective channel openings can be considered a temporary, local  increase in membrane conductance with driving potential set by the difference  between the reversal potential of the ion species passed by the channels and the  intracellular potential. note that unlike their voltage-sensitive cousins, these  ligand-sensitive channels closely follow ohm's law [30].  because the current through a channel is proportional to the difference be-  tween the local membrane potential and the reversal potential, the change inneuromorphic synapses 341  membrane voltage due to synapse activation will be smaller if the membrane  voltage is already near the reversal potential. for synapses located on dendrites,  the charge contributed by each activation will depend on the recent activity of  neighboring synapses, an effect known as sublinear summation [71, 72, 80]. if  synaptic activations instead caused current injections which were independent  of membrane voltage, dendrites would still spatio-temporally filter the charge  injections to form extended post-synaptic potentials (psps) at the soma, but  the filtered psps from distinct synaptic events would add together linearly [87].  while sublinear summation cannot amplify signals and thus is not as strong  a nonlinearity as those produced by active channels [60], it still provides a pow-  erful spatio-temporal computation mechanism for passive dendrites. koch and  poggio [37, 38] have shown sublinear summation can be used for crude multi-  plication, which is necessary for motion perception. korn and mallet [40] have  suggested it dampens synaptic noise. we have already demonstrated sublin-  ear summation on artificial dendrites with fixed-weight synaptic conductances,  where it has proven useful for tasks such as spike train coincidence detection and  frequency selection [69]. ensuring that the variable weight synapses presented  in this paper behave as conductances has thus been a top design priority.  the most basic function of dendrites is to transfer the charge injected at  spatially distributed synapses toward the soma, or cell body, where that charge  can influence the neuron's output firing. in passive dendrites this transfer is  not a wave-like propagation toward the soma, but rather a slightly asymmet-  ric spread away from the synapse into all connecting branches. the psp as  typically measured by neurophysiologists with electrodes at the soma therefore  appears severely attenuated and temporally extended compared to the origi-  nal event at the synapse. neuroscientists wishing to avoid the computational  burden of simulating dendrites often use alpha-functions [73] to model the post-  synaptic current. but in analog vlsi, it is straightforward to build artificial  dendrites which filter synaptic events using diffusive mechanisms similar to bio-  logical dendrites. without artificial dendrites, each synapse circuit would need  to generate a time-varying current that matched an alpha-function's smooth  waveform. this would require significantly more complex synapse circuitry  than that presented in this paper, as well as prevent local interactions between  synapses [69] and the variety in psp shape that normally results from differing  synapse positions [23, 24].  15.3 synaptic variation  understanding stochastic, short-term, and long-term variations in transmission  at individual synapses is important for designing neuromorphic systems which  adapt according to biologically plausible rules. the neuromorphic synapses de-  scribed in this paper do not yet include on-chip adaptation circuitry. therefore,  we will restrict the following discussion to the physical consequences of synap-  tic modification, disregarding the conditions which may lead to potentiation or  depression. our current research involves "chip in the loop" experimentation342 neuromorphic systems engineering  with adaptation rules (e.g., hebbian) implemented in software. the results  of these experiments should provide direction towards designing autonomously  adapting silicon neurons.  long-term potentiation (ltp) in the hippocampus has been studied exten-  sively (e.g., [7, 16, 22, 57]). it appears to be an example of associative or  hebbian learning, in which synaptic weights are increased when synaptic ac-  tivation coincides with existing depolarization of the post-synaptic membrane.  changes in synaptic weight may result from pre-synaptic changes in the amount  of transmitter released, an increase in the number of post-synaptic channels,  conformational changes in channel proteins which increase their conductance  or de-activation times, or a change in transmitter reuptake [3, 35, 50]. at this  time it is unclear which of these mechanisms is primarily responsible for ltp,  and some evidence suggests that both pre- and post-synaptic changes occur  under differing conditions [41, 42]. most of these mechanisms can be mimiced  in artificial systems by varying either the synaptic conductance or the duration.  most synapses also display at least one of many types of short-term plas-  ticity which may be associative or may depend only on the recency of past  activations [21]. such effects have been shown to mediate habituation and sen-  sitization of reflexes in simple invertebrates such as aplysia [34]. they also  appear in some ltp experiments [9], but their role in the function of higher  level nervous systems is not clear. should short-term plasticity prove useful for  artificial systems, it can be implemented with circuitry that combines a leaky  integral of synaptic activity with a long-term conductance control signal.  in addition to deterministic changes in synaptic efficacies, many central ner-  vous system (cns) synapses exhibit stochastic, quantized fluctuations in effi-  cacy from activation to activation. this is because cns pre-synaptic terminals  typically have a few or just one active site where a transmitter packet can be  released, and release from each site is probabilistic [39]. therefore the collec-  tive channel conductance and hence the magnitude of successive psps varies  in discrete steps according to the number of packets actually released. also,  presynaptic depolarization does not always result in release of any transmit-  ter [29], and packets can be released spontaneously, causing miniature excita-  tory synaptic currents [65] which may be as large as the current from a normal  activation [70]. many researchers are trying to determine whether the locus  of ltp is primarily pre- or post-synaptic by examining whether ltp shifts  the psp magnitude quantization levels themselves or the distribution of psps  among fixed levels (e.g., [8, 46, 56]).  besides what it can reveal about the location of synaptic modifications, the  apparently unreliable, quantal nature of biological synaptic transmission has  several implications for robust artificial system design. though axons transmit  spikes fairly reliably [49], individual synapses do not generate psps with con-  sistent magnitudes [2]. therefore, if a neuromorphic network adapts and com-  putes with mechanisms sufficiently similar to those in the cns, it should still  function with unreliable or noisy synapses. similarly, such a network should  not fail if the interchip cominunication system randomly loses some of theneuromorphic synapses 343  synaptic activation events corresponding to each source spike. note, however,  that the biological analogy does not justify losing spikes at their sources, which  is equivalent to the much rarer failure of action potential propagation at the  soma. whether artificial synapses are reliable or not, any adaptation algorithm  touted to reflect biology should be able to tolerate synaptic noise. adaptation  in the cns could conceivably depend on this noise [83] yet require precision in  the mean synaptic weight [66]. if so it may be necessary to purposely generate  similar unreliability in silicon by superimposing quantal noise on conductance  or duration control signals. to model changes in synaptic weight which are  a result of reliability rather than potency modifications, the characteristics of  the noise rather than the weights themselves would need to be individually  modifiable.  research on ltp mechanisms may also reveal to what degree neighboring  biological synapses adapt independently. random deviations in the number of  post-synaptic channels and pre-synaptic active sites ensures that the efficacies  of synapses connected to a length of dendrite will be different, but this does  not imply that they adapt independently. if the second messengers (e.g., intra-  cellular free calcium) which control changes in efficacy diffuse over fairly large  areas [54, 55, 65], the weights of all synapses in a dendritic segment with the  same type of ligand-sensitive channel could be modified in unison. if, on the  other hand, these changes are governed by reactions within the pre-synaptic  termini or confined to dendritic spines [91], they could be fully independent.  until this issue is resolved we shall take the view that connections to the  same dendritic compartment from different neuromorphs should have individ-  ually adjustable weights. therefore, in our artificial dendrites each compart-  ment is meant to represent an isopotential length of dendritic branch containing  many synapses, not just the membrane underlying a single pre-synaptic termi-  nal. time multiplexing of synaptic activations allows the synaptic circuitry of  a single compartment to emulate the tens or hundreds of synapses that might  be found on an equivalent isopotential segment of biological dendrite. if bi-  ological synapses do adapt independently but all connections to an artificial  compartment are forced to use the same locally stored weight, the computa-  tional and adaptive abilities of the resulting neuromorphic network could be  relatively limited.  15.4 synapse designs for multi-layer perceptron  networks  many vlsi implementations of multi-layer perceptron networks have been built  since backpropagation [21] and related supervised learning algorithms appeared  in the mid 1980's. synapses in perceptron networks multiply an input signal  (typically an output of another perceptron unit) by a constant: the synaptic  weight. synapse outputs are added together and passed through a non-linear  transfer function. this high-level model of biological synaptic function is not  equivalent to dendritic membrane conductances activated by spikes [79].344 neuromorphic systems engineering  perceptron synapse designs diverge according to whether on-chip computa-  tion and communication are primarily digital [31] or analog [27]. among purely  analog implementations, synapses typically consist of gilbert or transconduc-  tance multipliers [173, 2o], with the weights stored locally on floating gates [45]  or dynamically refreshed capacitors (e.g., [77, 97]). the outputs of each synapse  are generally currents tied to a common node (e.g. [64]) in order to form a linear  sum of the synaptic contributions within the constraints of mosfet technol-  ogy. in these designs weights must be present continuously at each synapse,  prompting interest in programmable, non- volatile on-chip storage devices such  as floating gates (e.g., [13, 28, 55]).  pulse stream networks are an interesting alternative in which synap-  tic multiplication can be implemented with analog, digital, or hybrid tech-  niques [62, 67, 92]. these include pulse width modulation with digital weight  shift registers and pulse amplitude scaling with transconductance multipli-  ers [68].  15.5 spiking silicon neurons  since the appearance of mead's seminal book in 1989 [18], several research  groups have developed silicon neurons and networks more closely based on  known principles of biological circuitry. artificial sensory transducers such as  silicon retinas (e.g., [1, 182, 59]) and cochleas (e.g., [43, 90]) constitute the  brunt of this work. a few researchers have also built general-purpose neurons  modeled upon various aspects of biological interneurons. for instance, dou-  glas and mahowald [19, 52] have built single compartment neuromorphs with  voltage-sensitive channels that produce action potentials, and linares-barranco  et al. [47] have implemented the fitzhugh-nagumo soma model in cmos. our  work has focused on constructing neuromorphs having a large number of com-  partments (figure 15.1b) that form artificial dendrites (figure 15.1a) coupled  to a leaky integrate-and-fire soma model [23]. so far we have concentrated  our efforts on passive dendrites which lack the strong nonlinearities of voltage-  sensitive channels [60].  the common thread among these efforts is the type of communication be-  tween sensory input and interneurons. many of these silicon neuromorphs out-  put digital spikes or address events which can be passed to other neuromorphs  over time-multiplexed asynchronous [63] or synchronous communication chan-  nels. neuromorph output is coded in the frequency and timing of their spikes  rather than a multi-valued digital or analog code [18]. while the address event  protocol [51] allows simultaneous activation of synapses, placing source address  decoders at each synapse consumes lots of space and prevents a synapse from  receiving events from multiple sources. virtual wires (figure 15.2, [23]) adds a  connection list to translate from source address space to synapse address space.  simple row and column synapse address decoders can then service all synapses  on a chip. the main advantages of virtual wires is that, with the aid of a  connection list, connections from any source to any destination can be quicklyneuromorphic synapses 345  ca)  dendritic tree threshold setting synapses  ,,,,,,,,,,,,,,,,,,, 1  ~ soma iiiiii [ i  (b)  o2  gi g+  > rrn  -cm 6 vresl  vexcitatory  .-~  i r m  cm  vinhibilory  gnd  figure 15.1 (a) silicon neuron with dendritic tree. synapses exist on the dendrites at the  cross points and at the soma. activating soma synapses sets the spike firing threshold for the  integrate-and-fire soma. the integration time constant is determined by a programmable  resistor, 1~, and a fixed capacitor, (7. the integration capacitor is discharged whenever  a spike is generated by the soma. (b) a two-compartment section of dendrite. each  compartment contains a storage capacitor (gin) representing the membrane capacitance  of an isopotential length of dendrite and a set of conductance paths from the capacitor  to various potentials. the switched capacitor membrane (rm) and axial (}~a) resistors,  connect the compartment to a resting potential and adjacent compartments. the excitatory  (ge) and inhibitory (gi) synaptic conductances, which turn on momentarily when a synapse  is activated, pull the compartment capacitor voltage towards their respective synapse supply  potentials.  configured with arbitrary source fanout, arbitrary delays, and supplemental in-  formation such as weights. since the diffusive and integrative time constants  of the underlying analog dendrites and somas are longer than the time to se-  rially service thousands of connections, analog computations still take place in  parallel.  in our system, synapses charge or discharge a compartment capacitor only  when activated, instead of continuously multiplying an input signal by the346 neuromorphic systems engineering  s~keou~u,  compa~ma]  v~h~g~s  figure 15.2 virtual wires interchip communication system emulates the fanout and trans-  mission delays of biological axons. when a neuromorph fires, its address references a block  of connections in the digital connection list memory. each connection consists of a synapse  address, weight, and delay. if the delay is non-zero the state machine will place a connection  in the delayed connection memory for later activation. otherwise it will successively activate  each synapse specified in the connection list memory. the fanout and temporal precision  are limited only by the bandwidth of the synapse bus and the combined neuromorph firing  rates.  synaptic weight. because the synapse address must appear on the synapse  bus every activation, a digital synaptic weight can easily be delivered along  with the synapse address. if the synapse needs to remain on for an extended  period, the weight can be converted to an analog voltage and deposited on a  local synapse capacitor at the time of activation. assuming the synapse always  turns off before this weight voltage decays significantly, the complexities of  dynamic refresh circuitry or permanent storage are unnecessary. in addition  to reducing silicon real estate requirements, storing weights off-chip with other  connection information allows the weights of connections made from different  neuromorphs to the same compartment to be independent.  15.6 methods for variable-weight synapse design in  artificial dendrites  building compact, wide-range variable synaptic conductances in cmos is dif-  ficult because mosfets behave as voltage-controlled current sources when  their drain-source voltage exceeds the gate voltage less the threshold voltage.  the horizontal resistor (hres) of mead [18] works well for image segmenta-  tion in resistive networks of artificial retinas but only has a linear range of  100inv. several researchers trying to build integrated active filters have used  multiple mosfets to emulate voltage-controlled resistors (e.g., [1, 5, 61, 89]),neuromorphic synapses 347  but most have a limited operating range of about 2v. shibata and ohmi [81]  report a fairly compact variable resistor with moderate linearity over a 5v  range and suitability for resistances in the kft range. switched capacitor tech-  niques are very effective as globally variable resistances for low frequency sig-  nals [24, 86, 88], with operating ranges of lvdd -- vtl ~ 4v.  a wide range of resistances and durations, and thus a variety of fast synapse  designs can be used with artificial dendrites when a conductance-duration  equivalence approximation is invoked. under certain conditions (i.e. long and  thin compartments, high axial and membrane resistances, and short durations),  dendritic filtering validates use of short durations and large conductances. con-  sider an isolated compartment capacitor with only a switchable synaptic con-  ductance attached as shown in figure 15.3. the compartment voltage v,~(t)  after the conductance has been active for duration = t seconds is given by  ~vcx tator~ ci  ge  vm  cm  gnd  figure 15.3 isolated compartment for illustrating the conductance-duration product  equivalence. the total charge transferred to on when ge is connected t seconds is  the same as long as the integral of the conductance over the duration is constant. this is  only an approximation when axial resistances to neighboring compartments are added.  = (vo - - e - g/cm) + vm(0)  where v,~(0) is the initial voltage, ge is the conductance, ve is the conduc-  tance supply potential, and c,~ is the compartment capacitance. the argument  of the exponential in this equation clearly indicates that the final compartment  voltage and total charge transferred will be the same whenever the product of  the conductance and duration are the same. in fact, for any two time-varying  conductances, the final voltage will be the same as long as the integrals of  the conductance over the duration are equal. this suggests that a very short,  50as synapse activation pulse can control the duration when used with cor-  respondingly strong conductances, and that synaptic weight variation can be  accomplished either by varying the conductance or the duration or both. we  have fabricated and tested neuromorph chips having both types of synapses:  one that varies the conductance, and the other that varies the duration.348 neuromorphic systems engineering  when leakage through the axial resistances is taken into account, this equiv-  alence property is only an approximation. the total charge transferred also  becomes dependent on the potentials of neighboring compartments during ac-  tivation. if the duration is short enough compared to dendrite time constants,  interaction with nearby compartments will be negligible until long after the  synapse has turned off, and the charge transferred by activation of a lms con-  ductance for a 50ns duration is a reasonable approximation to that of a weaker  50ns conductance for lms.  (a) favorable params (b) short compartments (c) unfavorable params  , 8  ~6  $  ,53  ~2  i i  2 4 6  5  ....  4  3  2  1  o ~ ~  initial neighboring compartment voltages (v) ::  40  35  30  25  20  15  10  5  0 i  figure 15.4 comparison of short duration approximation (dashed line) with long duration  synapses (solid line) under a) favorable dendritic parameters, b) favorable parameters except  shorter compartments, c) unfavorable dendritic parameters. for a) and b) the cytoplasmic  resistivity ri = 270~cm, specific membrane resistance rm = 165k~cm 2, specific mem-  brane capacitance cm = .751~f/cm 2, branch diameter d = .75#m, synapse resistances  1/g~e -- 15k~ for 50ns duration and 300m~ for lms duration. for a) compartment  length l = 1/5 space constant --- .2a -- .02cm, while for b) l -- .1a = .olcm. for c)  r~ = 70ftcm, r,~ = 20kf~cm 2, c,~ = .75#f/cm ~, d = 2.5#m, l = .2a = .02cm,  1/ge ---- 3.4kf~ for 50ns duration and 68mft for lms duration. the synapse was placed  on the middle compartment of an eleven compartment branch (like figure 15.1) whose ac-  tual resistor and capacitor values were calculated from the parameters above with standard  formulas [77]. simulations were performed with spice3f4. see text for further discussion.  the simulation results shown in figure 15.4 compare the total charge trans-  ferred by short (50ns, dashed lines) and long (1ms, solid lines) duration synap-neuromorphic synapses 349  tic activations under differing dendrite dynamics and initial conditions. an  excitatory synaptic conductance was placed on the middle compartment of an  eleven compartment branch. in all cases this compartment to be activated  was initialized to the resting potential of 2.5v, and in all but fig. 15.4b the  synaptic conductance was normalized to c,~/t. this resulted in a synapse  time constant, equal to the duration, that produced depolarization of an iso-  lated compartment to 4v using a 5v excitatory supply potential. the other  ten compartments (five on each side) were initialized to a single voltage which  was stepped over simulation runs from the inhibitory supply potential (0v) to  the excitatory supply potential (5v). the dashed lines (50ns duration) are  horizontal because the duration is short enough that interactions with neigh-  boring compartments are negligible, and thus the neighboring compartmental  voltages are irrelevant. the solid lines (lms duration) each have a negative  slope because the effective load on the synapse through the axial resistors in-  creases with hyperpolarization of neighboring compartments. the difference  between the horizontal and slanted lines at a particular initial neighbor voltage  represents the error of the short duration approximation.  the dendrite parameters for figures 15.4a-b, taken from [53], represent fa-  vorable conditions for the short duration approximation. such high cytoplas-  mic resistivities (ri) and specific membrane resistances (r,~) have been found  in several recent studies [74, 78, 84, 85]. results with less favorable but more  commonly accepted dendrite parameters [6, 14, 77] are shown in figure 15.4c.  compartment lengths for figures 15.4a and c were normalized to one-fifth of  the dendrite space constant [77] based on a specific membrane capacitance of  0.75#f/cm. the compartment length was cut in half for figure 15.4b, increas-  ing interactions with neighboring compartments and halving the compartment  capacitance. however, figure 15.4b uses the same synaptic conductances as  figure 15.4a, so the charging time constants are less than the durations.  in all cases short-duration activations match long activations best when ad-  jacent compartments are depolarized. initializing all adjacent compartments  to the inhibitory supply potential (0 volts) maximizes the pull on the acti-  vated compartment through the axial resistors, maximizing interaction under  long durations. for inhibitory synapses, these relationships are reversed. note  that in biological neurons the inhibitory supply potential is much closer to the  resting potential than the excitatory supply potential, which would lessen the  error for excitatory synapses with hyperpolarized neighbors. comparing fig-  ure 15.4a to figure 15.4b indicates that breaking a branch into smaller com-  partments increases the percent error for the short duration approximation.  this is counter-intuitive because finer compartmentalization normally makes  discrete spatial models closer to continuous biological dendrites. the large per-  cent differences in figure 15.4c show that the approximation also worsens as  branch diameter grows and axial and membrane resistances decrease. while  the charge transfer errors may be significant even under favorable conditions,  they would be much greater with current-source synapses, even for an isolated  compartment.350 neuromorphic systems engineering  though the total charge transferred can be about the same with very short  durations, simulations indicate that the shape of the psp measured at the orig-  inating compartment differs significantly for the first couple milliseconds. for  a 50as duration synapse the local compartment voltage instantaneously jumps  upon activation and then decays quickly through the axial and membrane resis-  tances. upon activation of a 1ms duration synapse, the compartment voltage  rises gradually to a smoother, smaller peak before decay becomes visible. after  a few milliseconds artificial dendrites filter out the high frequencies in the psp  caused by a short duration, and psps from all but the most proximal synapses  appear smooth at the artificial soma. nevertheless, because of these differences  in initial psp shape, short duration synapses may not be a biologically ac-  curate model for interactions between adjacent compartments that have been  stimulated at nearly the same time. clearly the short duration approximation  cannot be used for slower neuromodulatory synapses, which require more ex-  tended conductance control. nevertheless, we will see that it accommodates  very simple circuitry and emulation of any number of independently weighted  connections with one synapse circuit per compartment.  15.7 variable-weight synapses - two different  approaches  we have tested two fundamentally different synaptic conductance designs. the  first design, referred to hereafter as the conductance array (figure 15.5) em-  ploys a minimal on-duration equal to the activation pulse width, and achieves  conductance variation by selecting an appropriate-valued conductance form an  on-chip array. in the second, or self-timed conductance design (figure 15.6),  the controllable decay of a local gate capacitance determines the duration while  switched capacitors form a constant conductance. both synapses are meant  to operate between inhibitory and excitatory synapse supply potentials of zero  and five volts. though many hybrids or variations of these designs are certainly  possible, these two were chosen because the advantages of each are quite com-  plementary, and they allow us to consider two diverging approaches to on-chip  adaptation. we will not know which is more suitable for on-chip adaptation  until we settle on a particular weight and connection modification scheme.  15.8 conductance array  unfortunately, linear voltage-controlled resistors cannot be made from a sin-  gle minimum size component in the standard cmos process, but individual  synapse circuitry must be small for large numbers of synapses to fit on a chip.  the most striking aspect of the conductance array design is that all synapses on  a neuromorph (90 in this case) share two conductance arrays. since the conduc-  tance circuitry need not be duplicated at each synapse, its size and complexity  is much less of a concern. sharing is only possible when the synapse duration  is equal to the activation pulse width, so that the conductance array can be  time-multiplexed along with the synapse address. most perceptron networkneuromorphic synapses 351  ~ulter  row_n itor]  figure 15.5 sampling and pre-charging circuitry for conductance array synapses. when a  sample signal is asserted (not shown), the synapse address is decoded which leads to assertion  of col_x and rowl signals thus connecting the n-channel source followers of the appropriate  compartment to the output buffer. the resulting voltage will be the compartment voltage  less the n-channel threshold. if pre-charging is desired the precharge signal is asserted,  charging all pull-up lines to approximately vm -- vthr (n) 4- vthr(p), the threshold voltages  for n- and p-channel transistors. the pre-charge range is approximately 1 - 4v.  figure 15.6 vdd  dac ~  i ~o,. t ~ ~ p,  ~ _ pclock i  ~ pclock2  excitatory self-timed synapse circuitry. see text for explanation.  synapses cannot be shared in this manner because they do multiplications in  parallel.  we will only describe the operation of excitatory synapses since inhibitory  synapses work similarly, except that all p-channel transistors are replaced with  n-channel and supply voltages are exchanged. the excitatory conductance  array (figure 15.5) consists of 15 p-channel mosfets whose sources are con-  nected to the excitatory synapse supply potential (vexcitatory) and whose drains  are connected to a common pull-up line. upon presentation of the 4-bit synaptic  weight and assertion of the synapse activate signal, a weight decoder switches  the gate voltage of the selected weight transistor from 5v to 0v, biasing the~/~ ~/o~ ~  >/g e/~ ~  9/g g/~ gi  9if g/or gi  z/~ ~/9 ii  o~/g >/~ o~  o~/~ ~1~ 6  6/~ 9/~ ~  ii/~ 9/~ z  ~i/~ 6/2 9  9~/~ z/~ ~  0~/~ 6/~ ~  ~/~ ~/~ ~  0~/~ ~/~ ~  9~/~ 9~/~ ~  7/a4 7/a4 ~q~aa4  ~o~}q}qu i ~o~v~ax~ a}~dvug~  "saz!s jÂ°ls!suej.l_ rejjv a~uel:)npuÂ°d i'$i alqe/  â¢ s:~ioa!ii!m paapunq v anoq~ jo uov~vz!a~iodo p v sasn~a ~sosva~ oq~ ain~  'uo~a~aa~ olsu~s ~ u~ ag oa ai moaj ~uom~a~dmoa ~ ~u~$a~qa ~o olq~d~a s~  ar~o~ aso~uoals orz 'i'~i alq~& u~ u~ors oa~ aoas~sueaa aq$~o~ ra~o aq sonata  qlsuoi/qap~ or& 'sosuvqa osmloa auamaa~dmoa poavds xluoao xlaavm~xoaddv  u~ sainsoa osind uo~a~a~aa~ ~u0g ~ a~qa os paz~s 2wnpd~pu ~ aaa~ saoas~su~aa  aq$!o~ aqa 'uo~annm~s 3 o dlo q oqa qa~ 'o~i oa o~ moa/osu~a adaad sna  u~ pozwoa soau~ls~soa or1 '.aoas~su~aa ar$~o~ polaolas oqa jo aoaa~va~d oau~a  -anpuoasuvaa pu~ o~l~a qasuoi/qlp~ oqa oa i~uo~aaodoad ~ oauvaanpuoa i~nla~  or& "iv~uo~od xiddns osd~uxs oqa oa aoas~suval aq$!o~ poaaoios oqa qsnoaqa  aoad~d~a auomaa~dmoa aqa moaj qaud oauvaanpuoa-~o i ~ stusq sir & "ouii dn  -lind oqa oa oaumd~d~a au~aqmam ddi ~ s,luau~aa~d~oa oqa saaouuoa luom  -aa~dmoa aavvdoadd~ oqa jo (~s) saoas~su~aa aaaios umnioa pu~ ~oa oqa pu~  'papoaap si ssaapp~ asd~uxs xaol~ldxa ara 'om!a am~s ara w '(ai ~) pi o  -qsoaqa oaoq~ si o$~aloa au!i dn-iind oqa sv ~uo i sv uot$oa a~ou!i sa! u! aoas!su~aa  dnii:i~nidn~ sin~&sas oihdltopiotti~i~tn gsgneuromorphic synapses 353  -2 weight 0  i  weight 15  -4 -3 ,.~;:i ~h~ls ..........  : . :  i  i  ::ii:i:!:::~:i  -2 -1  initial ddving potential (v) weight oi  figure 15.7 the compartment voltage change upon activation plotted versus the driving  potential (synapse supply potential - compartment voltage, vc - vn(0)) before activation  for each excitatory and inhibitory weight. the compartment voltages immediately preceding  and after synapse activation with the selected weight were measured through the internal  compartment voltage sampling circuitry (cf. fig. 15.4). through analog multiplexing, any  compartmental voltage can be sampled and output on a single pin, with settling times less  than 100n8 per sample and a linear range of 1 - 5v. the dotted portions of the curves are  estimates based on our understanding of the circuitry for regions where direct measurement  was not possible.  the compartment voltage change upon activation is plotted versus the driv-  ing potential before activation for each excitatory and inhibitory weight in  figure 15.7. if the transistors behaved as ideal conductances the transfer char-  acteristics would be straight lines with various slopes passing through the origin.  in regions where the lines become horizontal, the weight mosfet is operating  in the saturation rather than the linear region. this occurs mainly for smaller  weights when the compartment membrane potential is far from the synapse  supply potential (i.e., ivdsl > iv~s - vtl).  the measured transfer characteristics are nearly straight lines, but they do  not pass through the origin. this is related to the major problem with the con-  ductance array approach: unavoidable parasitic capacitances along the shared  pull-up and pull-down lines. the capacitance of each shared line, due mainly  to the source-bulk and drain- bulk capacitances of the weight and synapse  transistors connected to it, amounts to approximately lpf, as much as each354 neuromorphic systems engineering  compartment's membrane capacitance. if the pull-up line voltage differs from  the compartment voltage when the two are connected, the pull-up line will  dump charge into the compartment, causing large voltage offset errors.  these offset errors are reduced by using the internal sampling circuitry to  pre-charge the pull-up lines to a compartment's voltage before its synapse is  activated (figure 15.5). because of the limited range and mismatch between  the n- and p-channel source followers, the pull-up line is not always precharged  to the correct voltage. the worst errors in the precharging actually occur when  the membrane potential is less than one volt, where it is out of the range of  the sampling circuitry and therefore not directly measurable. the remaining  offset errors could be eliminated by increasing the range of and compensating  for the offsets in the sampling circuitry with a rail-to-rail op-amp. they could  also be further reduced by dividing the pull-up line into switchable segments,  which could break up the parasitic capacitance. even with good precharging,  the pull-up line parasitic capacitance ultimately limits the scalability of the  shared conductance method. precharging also requires at least one additional  clock cycle per synapse activation, increasing the minimum time to service a  connection.  15.9 self-timed synapses  the distinguishing feature of a self-timed synapse is that its duration can be  much longer than the activation pulse width. capacitor voltage decay in the  local synapse circuit regulates the duration, leaving the synapse address bus  free to activate other synapses. in the current design the conductance is held  constant while the voltage deposited on a small gate capacitance determines the  synaptic weight by modulating the duration. we were able to use switched ca-  pacitors to implement the conductances because the durations are long enough  to smooth out the discrete charge transfers. alternative self-timed synapse  designs could have a fixed duration and modulate the conductance. the self-  timed synapse weights are currently supplied by an external digital-to-analog  converter, though they could also be stored within each compartment on a  floating gate or flux capacitor [67].  the self-timed synapse circuit is shown in figure 15.6. if a compartment's  address is presented to the chip, both row and column select transistors (p3-p6)  will turn on for about 50ns. this will deposit the synaptic weight manifested  as a duration control voltage on the parasitic p2 gate capacitance. at the  same time the p1 gate capacitance is discharged to ground through p3 and  p4, turning on p1. this forms a path from the compartment capacitor to  the excitatory synapse supply potential through the switched capacitors (pt-  ps). assuming p2 is biased in sub-threshold, it will slowly charge the p1 gate  back to ydd after the row and column select transistors turn off. the duration  will approximately be the length of time until the switched capacitor current  becomes limited by the p1 drain-source current, at which point the synapse  will behave like a very weak current source rather than a conductance. noteneuromorphic synapses 355  that p1 does not turn off abruptly, and therefore a clear-cut duration cannot  be defined. also note that the p2 gate voltage will have a natural decay of its  own, increasing or decreasing the charging rate of the p1 gate. the duration  control voltage need not be refreshed as long as it stays low enough that p1  turns off before p2.  the switched capacitor clocks are shared by all synapses on a chip and there-  fore cannot be used to modulate synaptic conductances individually. however,  by varying the switched capacitor clock frequency between lohz and lomhz,  the resistance of all synapses can simultaneously be varied between 10m~t and  10t~. the resistances are typically set so that activation with the maximum  desired duration, or strongest weight, can fully charge a compartment to the  synapse supply potential. weaker weights can then be achieved with shorter  durations. the maximum desired duration is typically one millisecond to em-  ulate fast chemical synapses but can be much longer to model slower second-  messengers synapses. durations between ten microseconds and one second can  be achieved with the current chip.  voltage transfer characteristics for the self-timed synapse are shown in fig-  ure 15.8. the deviations from linearity visible in the transfer characteristic  are, for the most part, due to temperature drift between successive measure-  ments. maintaining duration consistency over moderate temperature changes  is the most serious problem with self-timed synapses. this first version of the  self-timed synapse circuitry had several minor layout flaws as well. because the  switched capacitors (pt-ps) are always connected to the compartment capac-  itor, high clock frequencies cause spurious membrane leakage toward ground  even when p1 is turned off. switching the placements of p1 and p7-8 would  eliminate this problem but introduce source-bulk effects to the operation of pl.  also, crosstalk causes the inhibitory synapse to turn on gradually after a few  excitatory activations, and the pl gate voltage naturally decays toward ground  if p2-p4 are all turned off, causing the synapse to turn on spontaneously after  long periods of inactivation. these latter problems can be eliminated by creat-  ing a small leakage pathway from the pl gate to vdd, and from the corresponding  inhibitory n1 gate to gnd.  15.10 engineering comparisons  at this point we will compare the two designs directly, first from a purely  engineering standpoint, second based on their suitability as models of biological  synapses, and third as paths toward on-chip weight and connection adaptation.  both designs were fabricated through mosis using a standard cmos double-  poly n-well 2.0#m process on 2 Ã 2ram 2 tinychips. because of its shared  conductance circuitry, the conductance array design is more space efficient,  allowing each compartment to be short (246#m), only 66#m of which is synapse  activation circuitry. the excitatory and inhibitory conductance arrays servicing  45 compartments are only 125 Ã 246#m 2 combined, less than ten percent of the  total compartment area. the local self-timed compartments are much longer356 neuromorphic systems engineering  5 .......................................... : ........................ : .................. ,:  : â¢ . :  4 ......... ........... ............. ............ i .......... ............. ............. ~edac = 3.95v .........  a- ) ...... i ............ ............. ............. ........... ; ............. ..........  ~ ............ i i ....... .......  ~ ! ~i .... " . !  ~ ; .... :  'i ........... ......... .... : "  ~ o~ ..................... ,,,, ~,~, ,,,,.i~:: ~:  . _  ...................................................................................  -4 .................... : ~ : : ....  : : :  i i i i i ~ i i  -~ -4 -~ -2 -~ 0 ~ 2 ~ 4 5  initial driving potential (v)  figure 15.8 the peak compartment voltage change upon activation plotted versus the  driving potential before activation for inhibitory duration control voltages ranging from  0.95v to 1.10v and excitatory control voltages from 3.83v to 3.95v at 10~tv intervals.  to facilitate comparison with the conductance array which uses much shorter durations, ax-  ial resistors were turned off and membrane resistors were set so that the membrane time  constant was much longer than the maximum duration (2~s). the dotted portions of the  curves are estimates based on our understanding of the circuitry for regions where direct  measurement was not possible.  (360#m), about half or 165#m of which is actually self-timed synapse circuitry.  because the self-timed synapse circuitry is two and a half times taller, only three  instead of four neuromorphs fit on a tinychip. both compartment designs are  24#m wide.  though voltage-controlled linear resistors (e.g. [81]) could be used for shared  synaptic conductance circuitry, the conductance array described here is ideal for  systems with digital weight storage because it performs its own digital-to-analog  conversion with fully customizable quantization levels. considering excitatory  and inhibitory synapses together, the present conductance array decodes thirty  evenly spaced non-zero weights from five bits in each synapse address. since  combining conductances in series or parallel does not result in well- spaced  weights, each weight requires its own transistor. thus the conductance array  width is proportional to the number of weights, not the number of weight  bits. because the conductance array is never biased in sub-threshold, weightneuromorphic synapses 357  variations due to changes in temperature are negligible. weight variations  between the four chips we tested were also quite low.  transistors of the self-timed synapse, on the other hand, must be biased  in sub-threshold to produce the appropriate durations. therefore, the dura-  tions of self-timed synapses are very sensitive to temperature, noise on the  duration- control voltage, and, presumably, processing variations. the self-  timed synapses do have an advantage in that they do not share pull-up lines,  and thus are free from the complications of precharging.  15.11 modeling comparisons  though the conductances are not perfectly linear in either design, the currents  in both exhibit the desired dependence on the voltage of the underlying com-  partment. with the stronger weights, self-timed synapses stay on long enough  to exhibit sub-linear dependence on the voltages of neighboring compartments.  the minimal duration of the conductance array allows the peak of the post-  synaptic potential to be sampled immediately after activation, which may make  the off-chip software implementation of hebbian learning much simpler.  if a single weight voltage is stored permanently within a compartment's  synaptic circuitry, all connections made to that compartment must have the  same weight. this can be a severe limitation on network connectivity if each  neuromorph has only as a few compartments. one work-around would be to  associate multiple weight storage devices with each compartment, which could  still require considerable space even if the synaptic conductance circuitry was  not duplicated. as long as excitatory and inhibitory synapses have separate  circuitry, at least the weights of excitatory and inhibitory connections to the  same compartment will be separate.  in this regard, associating weights with connections in digital memory is  preferred at this time. different connections to the same compartment can use  separate weights as long as the time between successive activations of the same  synapse is greater than the duration. with our conductance array design,  any number of connections with separate weights can be made to the same  compartment. because our self-timed synapses use temporary local weight  storage and longer durations, they are not quite as flexible. undesirable weight  interference will occur if the synapse is activated more than once within one  duration. for instance, a strong weight could be overwritten by a weak one  early in its duration and the result would be a short duration. though less  likely for variable duration synapses, the result would be more tolerable if a  weak weight was overwritten by a stronger one. the former type of error can  be prevented by placing a diode between the incoming duration control voltage  and the local weight storage capacitance.  15.12 adaptation comparisons  though off-chip digital weight storage is well-suited for early experiments in  hybrid software/hardware based adaptation, large systems will eventually need358 neuromorphic systems engineering  on-chip analog adaptation circuitry. this usually implies weights should be  stored on-chip in analog form. ideally, weight storage and adaptation capabil-  ity would be combined in a single small device. for example, programmable  amorphous silicon resistors [32, 75] could serve as synaptic conductances and  perform simple anti-hebbian learning by increasing their conductance when  the compartment voltage before activation is far from the synapse supply po-  tential. however, we may find that effective adaptation requires more complex  interactions which cannot be reproduced by a simple device. the next best  approach might be to use conventional mosfet design techniques to inte-  grate capacitive weight storage, local adaptation circuitry and some variation  of a self-timed synaptic conductance into each compartment. this approach  could quickly become quite area intensive, especially if it attempted to integrate  information from neighboring compartments.  while the inherently digital nature of the conductance array may seem  unsuitable for on-chip adaptation circuitry, the experience gained with time-  multiplexed sampling and precharging suggest a space-efficient means for on-  chip adaptation circuitry to integrate information from arbitrary spatial scales.  under direction of the virtual wires state machine and connection list, the volt-  ages of arbitrary compartments can be sampled and deposited on adaptation  parameter storage capacitors. the resulting spatially averaged dendritic activ-  ity, representing second messenger concentrations, could complement the local  information needed by adaptation circuitry. work on this is now in progress.  15.13 conclusions  on-chip analog synaptic weight storage may be unnecessary for neuromorphic  systems with time-multiplexed synaptic activation busses. until issues such as  the degree that adjacent synapses adapt independently and the importance of  quantal synaptic noise are better understood, off-chip digital weight storage  provides more flexibility. digital storage of weights and connections combined  with sampling of dendrite voltages provides all of the information necessary to  experiment with correlative adaptation rules in software. experience gained  from this hybrid system approach can eventually be used to design appropriate  on-chip adaptation hardware.  in order to match the performance of biological neural systems, it is not nec-  essary to duplicate them in every detail, but only to capture those properties  underlying their computational and adaptive abilities. unfortunately, in these  early days of neuromorphic engineering, we sometimes must guess which mech-  anisms are critical to neural computation. therefore, we gradually increase the  complexity of our artificial dendrite models, noting what useful behaviors can  be observed with each additional feature. the neuromorphic, variable weight  synapses presented in this paper are the most recent enhancement to our arti-  ficial dendrites.  both the conductance array and self-timed synapses behave like conduc-  tances to enable sublinear summation, a basic computational mechanism inneuromorphic synapses 359  passive dendrites which may be essential for realizing biologically plausible  adaptation. because the charge transferred into dendrites by fast synapses  will be approximately the same whenever the product of the conductance and  duration are equal, synaptic weights can be varied by modulating the conduc-  tance or duration or both. the longer, more realistic durations produced by  self-timed synapses can cause more complex sublinear summation and more  accurately model activation of adjacent compartments at sub- millisecond in-  tervals. however, these longer durations prevent full weight independence for  connections to the same compartment which are activated at sub-millisecond  intervals. thus, in highly connected networks, the minimal durations used by  the conductance array synapses may be a more powerful model. if, through  time-multiplexing, all of the connections to an isopotential dendritic segment  can be modeled with a single synapse circuit per compartment, it will be possi-  ble to emulate the thousands of synaptic connections found on corticai neurons  with the synapse circuits of only tens or hundreds of compartments.  acknowledgments  support for this work was provided by national science foundation grant (# bcs-  9315879) and by a nsf graduate fellowship for wcw.  references  [1] c. acar and m. s. ghausi. fully integrated active-rc filters using mos  and non-balanced structure. int. j. of circuit theory and applications,  15:105-121, 1987.  [2] c. allen and c. f. stevens. an evaluation of causes for unreliability of  synapt ic transmission. in proc. natl. acad. sci. usa, volume 91, pages  10380-10383, 1994.  [3] p. o. anderson. properties of hippocampal synapses of importance for  integration and memory. in g. m. edelman, w. e. gall, and w. m.  cowan, editors, synaptic function, pages 403-430. john wiley & sons,  1987.  [4] y. arima, m. murasaki, t. yamada, a. maeda, and h. shinohara. a  refreshable analog vlsi neural network chip with 400 neurons and 40k  synapses. ieee j. of solid state circuits, 27:1854-1861, 1992.  [5] m. banu and y. tsividis. floating voltage-controlled resistors in cmos  technology. electron. lett., 18:678-679, 1982.  [6] j. n. barrett and w. e. crill. specific membrane properties of cat mo-  toneurones. j. of physiol., 239:301-324, 74.  [7] m. bawdry and j. l. davis. long-term potentiation, volume 2. the mit  press, cambridge, ma, 1994.  [8] j. m. bekkers. quantal analysis of synaptic transmission in the central  nervous system. current opinion in neurobioiogy, 4:360-365, 1994.360 neuromorphic systems engineering  [9] t. v. p. bliss and g. l. collingridge. a synaptic model of memory: long-  term potentiation in the hippocampus. nature, 361:31-39, 1993.  [10] k. a. boahen and a. g. andreou. a contrast sensitive silicon retina with  reciprocal synapses. advances in neural information processing systems,  4:764-772, 1992.  [11] t. h. brown, e. w. kairiss, and c. l. keenan. hebbian synapses: bio-  physical mechanisms and algorithms. annu. rev. of neurosci., 13:475-511,  1990.  [12] d. v. buonomano and m. m. merzenich. temporal information trans-  formed into a spatial code by a neural network with realistic properties.  science, 267:1028-1030, 1995.  [13] h. a. castro, s. m. tam, , and m. a. holler. implementation and prefor-  mance of an analog nonvolatile neural network. analog integrated circuits  and signal processing, 4:97-113, 1993.  [14] j. d. clements and s. j. redman. cable properties of cat spinal motoneu-  rons measured by combining voltage clamp, current clamp and intracellular  staining. j. of physiol., 409:63-87, 1989.  [15] m. h. cohen and a. g. andreou. current-node subthreshold mos imple-  mentation of the herault-jutten autoadaptive network. ieee j. of solid  state circuits, 27:714-727, 1992.  [16] g. l. collingridge and t.v.p. bliss. memories of nmda receptors and ltp.  trends in neurosci., 18:54-56, 1995.  [17] t. delbriick. silicon retina with correlation-based velocity-tuned pixels.  ieee transactions on neural networks, 4(3):529-541, may 1993.  [18] j. van der spiegel, p. mueller, d. blackman, p. chance, c. donham,  r. etienne-cummings, , and p. kinget. an analog neural computer with  modular architecture for real-time dynamic computations. ieee j. of  solid-state circuits, 27:82-92, 1992.  [19] r. douglas and m. mahowald. a constructor set for silicon neurons. in  s. f. zornetzer, j. davis, and t. mckenna, editors, an introduction to  neural and electronic networks. academic press, 1994.  [20] s. eberhardt, t. duong, and a. thakoor. design of parallel hardware  neural network systems from custom analog vlsi 'building blocks' chips.  in int. jr. conf. neural network, volume 2, pages 183-190, 1988.  [21] g. m. edelman, w. e. gall, and w. m. cowan. synaptic function. john  wiley & sons, 1987.  [22] f. a. edwards. ltp- a structural model to explain the inconsistencies.  trends in neurosci., 18:250 255, 1995.  [23] j. g. elias. artificial dendritic trees. neural computation, 5:648-663,  1993.neuromorphic synapses 361  [24] j. g. elias and d. p. m. northmore. switched-capacitor neuromorphs  with wide-range variable dynamics. ieee trans. on neural networks,  6:1542-1548, 1995.  [25] j. g. elias, d. p. m. northmore, and w. westerman. an analog memory  device for spiking silicon neurons. neural computation, 9:419-440, 1997.  [26] w. gerstner, r. ritz, and j. leo van hemmen. why spikes? hebbian  learning and retrieval of time-resolved excitation patterns. biol. @bern.,  69:503-515, 1993.  [27] h. p. graf, e. sackinger, and l. d. jackel. recent developments of elec-  tronic neural nets in north america. j. of vlsi signal processing, 5:19-31,  1993.  [28] p. hasler, c. diorio, b. a. minch, and c. mead. single transistor learning  synapses with long term storage. in ieee intl. syrup. on circuits and  systems, volume 3, pages 1660-1663, 1995.  [29] n. a. hessler, a. m. shirke, and r. malinow. the probability of transmit-  ter release at a mammalian central synapse. nature, 366:569-573, 1993.  [30] b. hille. ionic channels of ezcitable membranes. sinauer associates inc.,  2 edition, 1992.  [31] y. hirai. recent vlsi neural networks in japan. j. of vlsi signal pro-  cessing, 6:7 18, 1993.  [32] a. j. holmes, r. a. g. gibson, j. hajto, a. f. murray, a. e. owen,  m. j. rose, and a. j. snell. use of a-si:h memory devices for non-volatile  weight storage in artificial neural networks. j. of non-crystalline solids,  164-166:817-820, 1993.  [33] t. m. jessell and e. r. kandel. synaptic transmission: a bidirectional and  self-modifiable form of cell-cell comunication. cell, 72:1-30, 1993. neuron  vol. 10 (suppl.) pp. 1-30.  [34] e. r. kandel. part iii: elementary interactions between neurons: synaptic  transmission. in j. h. schwartz and t. m. jessell, editors, principles of  neural science, pages 123-260. appleton and lange, norwalk, connecti-  cut, 3 edition, 1991.  [35] e. r. kandel, m. klein, b. hochner, m. shuster, s. a. siegelbaum, r. d.  hawkins, d. l. glanzman, v. f. castellucci, and t. w. abrams. synaptic  modulation and learning: new insights into synaptic transmission from the  study of behavior. in g. m. edelman, w. e. gall, and w. m. cowan,  editors, synaptic function, pages 471-518. john wiley & sons, 1987.  [36] c. koch and t. poggio. the biophysical properties of spines as a basis for  their electrical function: a comment on kawato and tsakahara (1983). j.  theor. biol., 113:225-230, 1985.  [37] c. koch and t. poggio. biophysics of computation: neurons, synapses,  and membranes. in g. m. edelman, w. e. gall, and w. m. cowan, editors,  synaptic function, pages 637-698. ohn wiley & sons, 1987.362 neuromorphic systems engineering  [38] c. koch and t. poggio. multiplying with synapses and neurons. in  t. mckenna, j. davis, and s. f. zornetzer, editors, single neuron com-  putation, pages 315-346. academic press, inc., 1992.  [39] h. korn and d. s. faber. regulation and significance of probablistic  release mechanisms at central synapses. in g. m. edelman, w. e. gall,  and w. m. cowan, editors, synaptic function, pages 57-108. john wiley  & sons, 1987.  [40] h. korn and a. mallet. transformation of binomial input by the postsy-  naptic membrane at a central synapse. science, 225:1157-1159, 1984.  [41] d. m. kullmann and r. a. nicoll. long-term potentiation is associated  with increases in quantal content and quantal amplitude. nature, 357:240-  244, 1992.  [42] a. larkman, t. hannay, k. stratford, and j. jack. presynaptic release  probability influences the locus of long-term potentiation. nature, 360:70-  73, 1992.  [43] j. p. lazzaro and c. mead. circuit models of sensory transduction in  the cochlea. in mead and ismail, editors, analo 9 vlsi implementation  of neural systems, pages 85 101. kluwer academic publishers, norwell,  ma, 1989.  [44] b. w. lee, b. j. sheu, , and h. yang. analog floating-gate synapses for  general-purpose vlsi neural computation. ieee trans. on circuits and  systems, 38:654-658, 1991.  [45] b. w. lee and b. j. sheu. general-purpose neural chips with electrically  programmable synapses and gain-adjustable neurons. ieee j. of solid-  state circuits, 27:1299-1302, 1992.  [46] d. liao, n. a. hessler, and r. malinow. activation of post synaptically  silent synapses during pairing-induced ltp in ca1 region of hippocampal  slice. nature, 375:400-404, 1995.  [47] b. linares-barranco, e. sÂ£nchez-sinencio, a. rodriguez-vazquez, and  j. l. huertas. a cmos implementation of fitzhugh-nagumo neuron model.  ieee solid-state circuits, 26(7):956-965, 1991.  [48] b. linares-barranco, e. sÂ£nchez-sinencio, a. rodriguez-vazquez, and  j. l. huertas. a cmos analog adaptive bam with on-chip learning and  weight refreshing. ieee trans. on neural networks, 4:445-457, 1993.  [49] p. j. mackenzie, m. umemiya, , and t. h. murphy. a2+ imaging of cns  axons in culture indicates reliable coupling between single action potentials  and distal functional release sites. neuron, 16:783-795, 1996.  [50] k. l. magleby. short-term changes in synaptic efficacy. in g. m. edelman,  w. e. gall, and w. m. cowan, editors, synaptic function, pages 21-56.  john wiley & sons, 1987.  [51] m. mahowald. vlsi analogs of neuronal visual processing: a synthesis of  form and function. computation and neural systems, california institute  of technology, 1992.neuromorphic synapses 363  [52] m. mahowald and r. douglas. a silicon neuron. nature, 354:515-518,  1991.  [53] g. major, a. u. larkman, p. jones, b. sakmann, and j. j. b. jack. de-  tailed passive cable models of whole-cell recorded ca3 pyramidal neurons  in rat hippocampal slices. j. of neurosci., 14(8):4613-4638, 1994.  [54] r. c. malenka, j. a. kauer, d. j. perkel, m. d. mauk, p: t. kelly, r. a.  nicoll, and m. n. waxham. an essential role for postsynaptic calmodulin  and protein kinase activity in long- term potentiation. nature, 340:554-  557, 1989.  [55] r. c. malenka, j. a. kauer, r. s. zucker, and r. a. nicoll. postsynaptic  calcium is sufficient for potentiation of hippocampal synaptic transmission.  science, 242:81-84, 1988.  [56] a. malgaroli, a. e. ting, b. wendland, a. bergamaschi, a. villa, r.w.  tsien, and r.h. scheller. presynaptic component of long-term potentiation  visualized at individual hippocampal synapses. science, 268:1624-1628,  1995.  [57] j. l. martinez and b. e. derrick. long-term potentiation and learning.  annu. rev. of psychology, 47:173 203, 1996.  [58] c. a. mead. analog vlsi and neural systems. addison-wesley, reading,  ma, 1989.  [59] c. a. mead and m. a. mahowald. silicon model of early visual processing.  neural networks, 1:91-97, 1988.  [60] b. w. mel. information processing in dendritic trees. neural computation,  6:1031-1085, 1994.  [61] g. moon, m. e. zaghloul, , and r. w. newcomb. an enhancement-mode  mos voltage-controlled linear resistor with large dynamic range. ieee  trans. on circuits and systems, 37:1284-1288, 1990.  [62] g. moon, m. e. zaghloul, , and r. w. newcomb. vlsi implementation  of synaptic weighting and summing in pulse coded neural-type cells. eee  trans. neural networks, 3:394-403, 1992.  [63] a. mortara and e. a. vittoz. a communication architecture tailored for  analog vlsi neural networks: intrinsic performance and limitations. ieee  transactions on neural networks, tnn-5(3):459-466, may 1994.  [64] p. mueller, j. v. d. spiegel, d. blackman, t. chiu, t. clare, j. dao,  c. donham, t. hsieh, and m. loinaz. a general purpose analog neural  computer. in int. jt. conf. neural network, volume 2, pages 177-182,  1989.  [65] t. h. murphy, j. m. baraban, w. g. weir, and l. a. blatter. visualization  of quantal synaptic transmission by dendritic calcium imaging. science,  263:529-532, 1994.  [66] a. f. murray and p. j. edwards. enhanced mlp performance and fault tol-  erance resulting from synaptic weight noise during training. ieee trans.  on neural networks, 5:792-802, 1994.364 neuromorphic systems engineering  [67] a. f. murray and l. tarassenko. analogue neural vlsi: a pulse stream  approach. chapman and hall, london, england, 1994.  [68] a.f. murray, d. del corso, and l. tarassenko. pulse-stream vlsi neural  networks mixing analog and digital techniques. ieee transactions on  neural networks, 2:193-204, 1991.  [69] d. p. m. northmore and j. g. elias. spike train processing by a sili-  con neuromorph: the role of sublinear summation in dendrites. neural  computation, 8:1245-1265, 1996.  [70] n. otmakhov, a. m. shrike, and r. malinow. measuring the impact  of probabilistic transmission on neuronal output. neuron, 10:1101-1111,  1993.  [71] t. poggio and v. torre. a new approach to synaptic interactions. in  h. heim and g. palm, editors, lecture notes in biomathematics. theo-  rectical approaches to computer systems, volume 2, pages 89-115. heidel-  berg, newyork, 1977.  [72] w. rall. theoretical significance of dendritic trees for neuronal input-  output relations. in r. f. reiss, editor, neural theory and modeling.  stanford university press, palo alto, 1964.  [73] w. rall. distinguishing theoretical synaptic potentials computed for dif-  ferent soma-dendritic distributions of synaptic inputs. j. of neurophys.,  30:1138-1168, 1967.  [74] m. rapp, i. segev, and y. yarom. physiology, morphology and detailed  passived models of cerebellar purkinje cell. j. of physiol, 474:101-118,  1994.  [75] m. j. rose, j. hajto, p. g. lecomber, s. m. gage, w. k. choi, a. j.  snell, and a. e. owen. amorphous silicon analogue memory devices. j.  of non-crystalline solids, 115:168-170, 1989.  [76] d. e. rumelhart, g. e. hinton, and r. j. williams. learning internal  representations by error propagation. in d. e. rumelhart, j. l.mcclelland,  and the pdp research group, editors, parallel distributed processing:  explorations in the microstructures of cognition, volume i: foundations.  mit press/bradford books, cambridge, ma, 1986.  [77] i. segev, j. w. fleshman, and r. e. burke. compartmental models of  complex neurons. in c. koch and i. segev, editors, methods in neuronal  modeling: from synapses to networks, pages 63 96. the mit press, 1989.  [78] d. p. shelton. membrane resistivity estimated for the purkinje neuron by  means of a passive computer model. neuroscience, 14:111-131, 1985.  [79] g. m. shepherd. canonical neurons and their computational organization.  in j. davis and s. f. zornetzer, editors, single neuron computation, pages  27-60. academic press, inc., 1992.  [80] g. m. shepherd and c. koch. introduction to synaptic circuits. in g.m.  shepherd, editor, the synaptic organization of the brain, pages 3-31.  oxford university press, new york, 1990.neuromorphic synapses 365  [81] t. shibata and t. ohmi. a functional mos transistor featuring gate-level  weighted sum and threshold operations. ieee trans. on electron devices,  39:1444 1455, 1992.  [82] t. shima, t. kimura, y. kamatani, t. itakura, y. fujita, and t. iida.  neuro chips with on-chip back-propagation and/or hebbian learning. ieee  j. of solid-state circuits, 27(12):1868-1875, 1992.  [83] c. f. stevens and y. wang. changes in reliability of synaptic function as  a mec hanism for plasticity. nature, 371:704 707, 1994.  [84] k. j. stratford, a. j. r. mason, a. u. larkman, g. major, and j. j. b.  jack. the modelling of pyramidal neurones in the visual cortex. in  r. durbing, c. miall, and g. mitchison, editors, the computing neurone,  pages 296-321. addison-wesley, 1989.  [85] d. thurbon, a. field, and s. j. redman. electrotonic profiles of interneu-  roues in stratum pyramidale of the ca1 region of rat hippocampus. j. of  neurophys., 71:1948-1958, 1994.  [86] j. tomberg. synchronous pulse density modulation in neural network  implementation. in m. e. zaghloul, j. l. meador, and r. w. newcomb,  editors, silicon implementation of pulse coded neural networks, pages  165-198. kluwer academic publishers, 1994.  [87] k. y. tsai, n. t. carnevale, and t. h. brown. hebbian learning is jointly  controlled by electronic and input structure. network, 5:1-19, 1994.  [88] y. p. tsividis and d. anastassiou. switched-capacitor neural networks.  electron. lett., 23:958-959, 1988.  [89] z. wang. novel electronically-controlled floating resistors using mos tran-  sistors operating in saturation. electron. lett., 27:188-189, 1991.  [90] l. watts, d. kerns, r. f. lyon, and c. mead. improved implementation  of the silicon cochlea. ieee journal solid-state circuits, 27(5):692-700,  may 1992.  [91] a. zador, c. koch, and t. h. brown. biophysical model of a hebbian  synapse. in proc. natl. acad. sci. u.s.a., pages 6718-6722, 1990.  [92] m. e. zaghloul. silicon implementation of pulse coded neural networks.  kluwer academic publishers, 1994.16 winner-take-all  with lateral networks  excitation  giacomo indiveri  institute of neuroinformatics,  gloriastrasse 32 ch-8006 zurich,  switzerland  giacomo@ini.phys.et hz.ch  16.1 introduction  the analog vlsi current mode winner-take-all (wta) circuit, originally pre-  sented in [16] is a good example of a very well designed architecture. it is able to  process globally all the signals of an input array, it uses a very limited amount  of transistors per input node and it operates in parallel, with strictly local in-  terconnections. this architecture has been extensively and successfully used in  a wide variety of applications [3, 6, 7]. more recently, interesting modifications  to the original circuit have been proposed in [4] and [12]. in both cases, the  authors added to each element of the wta network a local feedback circuit to  obtain a hysteretic behavior in the selection/de-selection of the winning node:  every time a new winner is selected the local feedback circuit adds a constant  bias current to its input. the circuit will then de-select the winner when either  its input current becomes lower than other inputs by a factor greater than the  bias current or when the whole network is reset. from a functional point of  view, this operation enhances the resolution of the network and eliminates in-  stability problems, providing a robust mechanism that withstands the selection  of other potential winners unless they are stronger than the selected one by a  set amount. the authors of [4] proposed a scheme for distributing locally the  hysteretic component so that the winning input would be able to shift between  adjacent locations maintaining its winning status, without having to reset the  network. following a similar approach, in this paper we present two novel vari-  ants of the original wta network that also use a feedback circuit to provide  hysteresis, but that have a different scheme for implementing lateral excitation.  367368 neuromorphic systems engineering  ~lout  vr m5~  m,  io node m2 to node  n-i n+l  in mi~m 3  l~ '  .~ ............. ~ to node n-i v ~ iout  m4~ vr  ~_ m2 to node nÃ·l  ~'" m3  md]~~. "1- " --  2 t  ~ i  figure 16.1 elements of the modified wta circuits. (a) version described in this paper;  (b) previously proposed version. the two circuits differ in the way the transistors m2 are  connected. the power supply voltage vdd is set to 5v. input currents typically range from  picoamperes to microamperes. the laterally connected transistors operate in weak-inversion.  the two variants differ in the way the output signal is read: the first one has  a discrete output, with only the winning element active and all others inac-  tive; the second one represents a generalized version of the wta architecture,  with all its output elements active simultaneously, which behaves like a non-  linear filter. these new circuits can be used as handy building blocks for vlsi  models of attention mechanisms [3, ii] and, more generally, for a larger set of  neuromorphic analog vlsi architectures [5].  16.2 circuit descriptions  16.2.1 discrete output wta  figure 16.1 shows the single elements of the modified winner-take-all circuit  described in [4] next to the discrete output variant here proposed. as shown, the  circuits are remarkably similar, yet their operating conditions and functional  behaviors are quite different.  in the circuit of fig. 16.1(b) laterally connected transistors m2 implement  a "diffusor network" operating in weak-inversion [1, 14], used to distribute  the hysteretic component of the winner's output (i.e. the feedback current  ib flowing through transistor m3) to neighboring units. lateral excitation is  independent of the intensity of the winner's input current; on the other hand, in  the circuit in fig. 16.1(a), laterally connected transistors are used to implement  a different type of diffusor network that distributes the sum of both input  current and hysteretic component to neighboring units. this operation, while  laterally spreading the hysteretic current, simultaneously performs smoothing  on the input data. the circuit here proposed will thus tend to favor areaswinner-take-all networks 369  that have a higher average input activity rather than selecting the single input  with maximum intensity. this is instrumental in eliminating errors arising  from salt-and-pepper noise and is helpful in eliminating errors that arise from  offsets and device mismatches typical of analog vlsi technology. moreover, the  circuit in fig. 16.1(a) has a discrete output which is convenient for use with  centroid circuits [2, 13] for encoding the winner's spatial position in the array,  whereas the wta network proposed in [4] has multiple outputs that follow the  current distribution imposed by the diffusor network: one is maximum for the  winning element and the others decrease exponentially with distance. another  significant difference between the two circuits is in the way the bias voltages  of the laterally connected transistors m2 are set: in order to correctly operate  the diffusor network of the circuit in fig. 16.1(b) the gate voltages vr need to  be set at values higher than the power supply voltage vdd; this problem does  not occur for the circuit in fig. 16.1(a) for which the voltages v~ are typically  in the range 0.5v to 1v.  the circuit of fig. 16.1(a) works as follows: if n is the winning node, vn  is set so that transistor m3 supplies approximately all the bias current ib and  v~Â¢n are all set so that the transistors with those gate voltages are out of  their saturation region and their drain currents are approximately null. the  bias current is hence copied only in the winning element through the current  mirror m4-m5 and diffused, along with part of the input current, to neighboring  elements through the diffusor network. the diffusor network is implemented by  transistors m1, m2 and all the equivalent ones belonging to the other elements  of the array.  figure 16.2 shows the voltage distribution of the input nodes of a 13 element  array, for a case in which there is an input current in the center and no input at  all other nodes. the diffused current flows out of the winning element because  vn > vn + 1 > v~ + 2, and so forth. as shown in the figure, the winner-take-all  network forces a discontinuity in the voltage distribution at the winning node.  for all the elements that are more than one node away from the winning one,  the voltage distribution has a traditional resistive-network form, which can be  approximated by the equation  v = v0e -~lxl  where x represents the distance from the input node and c~ represents the space  constant of the diffusor network, defined as the rate at which signals die out  with distance from the source [18].  the diffusor network here proposed is a current mode one, hence also the  current distribution will follow a similar profile. for such a network the space  constant is defined as:  ---~-~ (v~-vg) o~ -~- e 2ut  where ~ is the subthreshold slope coefficient, ut is the thermal voltage and  v~ and vg are the gate voltage of the laterally connected transistors and the  common node voltage (gate voltage of transistors m1 in fig. 16.1) respectively.370 neuromorphic systems engineering  the amount of lateral excitation can thus be controlled by changing the  bias voltage v~ (see fig. 16.3(a)) or by having input currents of different in-  tensities (see fig. 16.3(b)). specifically, since the common node voltage vg  increases logarithmically with the winning input current (for transistors op-  erating in weak-inversion), the space constant c~ increases linearly with input  current intensity. as a consequence, if the winning input is relatively strong  (high confidence), the lateral excitation area is confined to a small neighbor-  hood around the winning node. if, on the other hand, the winning input is  relatively weak (low confidence), the lateral excitation area is wider.  having not instrumented transistor m1 of fig. 16.1(a) (and all other equiv-  alent transistors in the array), in the hardware implementation, it was not  possible to verify directly the expected behavior described above. nonetheless,  circuit simulations, by means of which it was possible to measure the current  flowing through the transistors m1 without affecting the behavior of the circuit,  provide consistent results (see fig. 16.3). experimental data was obtained on  the output nodes of the circuits for a 25 element wta network (of the type  shown in fig. 16.1(a)), implemented on a 2.3mm by 2.3mm chip using ana-  log cmos 2#m technology. fig. 16.4 shows measurement results for a case in  which 3 input units are active and all others are null. the input current on  unit 8 is gradually swept from 70 na to zero and back, while inputs on unit 10  and on unit 16 are kept constant. initially the wta network selects unit 8 as  the winner. as soon as the current is decreases to values lower than 61 na the  network selects unit 10 as the winner, despite its input current being lower than  the current on unit 16 (follow dashed line on fig. 16.4 from left to right). this  is a consequence of the effect of lateral excitation that, as mentioned previously,  tends to favor spatial areas with greater average activity. for the particular val-  ues of bias current ib and gate voltage v~ used in this experiment, the network  will switch to selecting unit 16 as the winner, only when is decreases to a value  lower than approximately 41 na. once the current on node 8 has decreased  to zero and has started increasing again, the network will switch back to se-  lecting unit 8, neglecting unit 10, when is reaches values greater than 61 ha.  this experiment, while indirectly demonstrating the hysteretic behavior of the  network, shows how it tends to select elements close to the previously selected  winner when the average input activity around the winner is sufficiently high,  and how it tends to function as a traditional wta network when the winner  is an isolated input.  an application of the discrete-output wta architecture, that exploits this  property, can be found in [7]: the authors used this architecture as the last  computational stage of a focus of ezpansion detection chip. such system was  designed for selecting heading direction in case of translatory ego-motion and  tracking it in time. the authors chose to use this variant of the wta architec-  ture in order to account for the a-priori assumption that the heading direction  position (represented by the winning output node of the wta circuit) shifts  smoothly in space.winner-take-all networks 371  2000  1800  ~ 1400  ~1200 -~  >1000 Â¢  ~ ooo  c~ soo j'  200  _- _- -4 ..... 4 ~--~_.~ ~ _-  -4 -2 2 ~  unit position j'  figure 16.2 voltage distribution at the input nodes, obtained through circuit simulations  of an array of 13 wta elements with -fb = 5ha, v~ = 0.80v, z0 = 10na and all other  input currents null. the inset is a zoom-in of the data for units i to 6 fitted with the  exponential function f(x) = 51.39 â¢ e -0"73(x-0'13) -i- 2.53.  o  Ã Ã ~ ~  unit position o ,~=~o~a i  im=2ona  umt posit~on  figure 16.3 sum of input current and hysteretic current flowing through transistors mi of  the array. (a) simulation results for different values of vr with [b = 5ha, -f0 = 10na and  all other input currents null; (b) simulation results for different values of ~0 with ]b = 5na  and vr -- 0.80v. both sets of data are fitted with exponential functions that have realistic  space constant coeflicients, for positions more than one unit away from the winner.372 neuromorphic systems engineering  16.2.2 generalized analog output wta  the basic element of the generalized wta architecture with lateral excitation  and analog output is shown in fig. 16.5. the circuit is identical to the one  shown in fig. 16.1(a) except for the addition of the extra diode-connected  transistor m6, which is used to read the output current. for a case similar to  the one of fig. 16.3, in which there is only one active input in the center of  the wta array, the behavior of the network is similar to the one of the circuit  previously described (see fig. 16.6). for more realistic cases though, in which  there is a structured pattern of input values, the architecture behaves like a non-  linear filter that enhances the input with maximum amplitude and smoothes the  rest of the data. specifically the enhancement effect and the smoothing effect  are superimposed: the diffusor network implemented with transistors m1 and  m2 performs the smoothing operation on the input data while the winner-take-  all network adds the hysteretic feedback current to the input with maximum  intensity. the net result is that of having output currents that correspond to  the sum of the smoothed input data with the smoothed hysteretic current.  figure 16.7 shows measurement results, obtained from a 25 elements gener-  alized wta network. in the example shown all nodes of the array have random  input values set by external potentiometers that control the gates of the input  transistors. initially (fig. 16.7(a)) unit 11 has maximum input. as shown the  output corresponds to a smoothed version of the input with the winning node  enhanced. subsequently (fig. 16.7(b)) the input on unit 15 is increased to a  value greater than the one on unit 11. the wta selects the winning input,  modifying the rest of the data accordingly. for the data shown, the bias current  ib lies within the range of the input currents. by modifying the value of ib it  is possible to intensify (or weaken) the strength of the enhancement effect over  the smoothing effect, thus emphasizing (or de-emphasizing) the winner-take-all  nature of the circuit.  16.3 application example  as an application example, we have designed a system in which the input  to the wta network is provided by a 1-d silicon retina as proposed by [9].  the circuit diagram of fig. 16.8 shows one basic element of the overall 1-d  array. the bottom part of the circuit implements the adaptive photoreceptor  circuit with spatial coupling between pixels described in [9]. the current mirror  implemented by the two p-type transistors in the central part of the circuit  amplifies the photodiode light induced current. the top part of the circuit  implements both types of wta architectures proposed: by connecting vse~ to  vdd we will implement the discrete-output variant of the wta architecture  described in section 16.2.1 and obtain positive output currents sourced from  the p-type output transistor, whereas by connecting vse~ to ground we will  implement the generalized wta architecture described in section 16.2.2 and  obtain negative output currents sunk from the n-type output transistor.winner-take-all networks 373  ~,oi ~ i _c !  45!  35  2 4 6 10 12 ~4 16 18 20 22 24  unit position  figure 16.4 chip data measurements for a 25 element wta network with vr -- 0.75v  and v5 = 0.9v, where v5 is the gate voltage of a 4#m by 4/~m transistor used to generate  the bias current ib. the dashed line shows the selection of the winner as the input current  on unit 8 is swept from 70 na to zero.  {o node to nodc  n* / ~+1  .. i  ~- iou~ ~  =  ...................  figure 16.5 circuit diagram of an element of the generalized wta architecture with  lateral excitation and analog output. the circuit dif[ers with the one of fig.l(a) by one  transistor (m6).374 neuromorphic systems engineering  1.75  17  1,6~  1.Â£  1,5~  ~ t~ 8  1.4~  1.z  ~ 3~ :  1.31  4 6 8 10 12 14 16 18 20 22  unit position  figure 16.6 chip data measurements for a 25 element generalized wta network, with  [5 : 21ha, v11 = 0.gv and v~#11 = 0.~sv. the output current has been converted into  voltage using an olin-chip sense amplifier.  3.4  2Â¸8  2Â¸1' ' " , ~ / ~ ~'*', ,,  , , , , 3.4  3.  28t/  ', i'  2 4 6 8 10 12 14 16 t8 20 22 2 4 6 8 10 12 14 16 18 20 22  unit p~it~on unit posi~on  figure 16.7 data measurements of input versus output currents for a 25 element gener-  alized wta network with vr = 2.25v and [5 = 21ha. the solid line represents the output  of the network, while the dashed line represents its input. in (a) the input on unit 15 is set  to v15 = 0.89v and the input to unit 11 (set to vll = 0.91v) is the maximum input  value; in (b) unit 15 (set to v15 = 0.95v) is the maximum value, with the rest of the input  values unchanged. note how the output of unit 15 is enhanced (and the rest of the data  smoothed); a normal smoothing network would have decreased this value, possibly making  it loose its winning status.winner-take-all networks 375  the spatio-temporal filtering properties of the silicon retina designed allow  the circuit to extract edges at different spatial frequencies. by changing the  bias voltages vgp and v~p it is possible to set the spatial-frequency tuning of  the filter (see fig. 16.9). the wta network will then select the edge with  spatial-frequency that elicits the strongest response and track it as it moves.  depending on the value of vscl, the output of the chip will correspond either  to a single active pixel in the output array (representing the winner) or to a  smoothed contour map of the input image with the strongest edge enhanced.  the chip, fabricated using a 2#m n-well cmos process provided by mosis,  contains a 1-d array of 25 pixels.  16.4 conclusions  we have presented two circuits that implement current-mode winner-take-all  networks with lateral excitation. their behavior was analyzed and compared  to the one of similar circuits previously proposed. as an application example of  these wta networks, we have presented an analog cmos vlsi system, with  on-chip adaptive photoreceptors, for selecting the most salient edges present in  an image. as demonstrated, the circuits are extremely compact and suitable  for integration with analog vlsi neuromorphic systems.  acknowledgments  i would like to thank christof koch for his support, ernst niebur and tonia morris for  their constructive comments and timothy horiuchi for the insightful discussions on  winner-take-all circuits. this work was supported in part by the center for neuromor-  phic systems engineering as a part of the national science foundation engineering  research center program; and by the california trade and commerce agency, of-  rice of strategic technology. fabrication of the integrated circuits was provided by  mosis.  references  [1] k. a. boahen and a. g. andreou. a contrast sensitive silicon retina with  reciprocal synapses. advances in neural information processing systems,  4:764-772, 1992.  [2] s.p. deweerth. analog vlsi circuits for stimulus localization and centroid  computation. international journal of computer vision, 8(3):191-202,  1992.  [3] s. p. deweerth and t. g. morris. analog vlsi circuits for primitive  sensory attention. in proceedings of the ieee international symposium  on circuits and systems, volume 6, pages 507-510, 1994.  [4] s. p. deweerth and t. g. morris. cmos current mode winner-take-all  circuit with distributed hysteresis. electronics letters, 31(13):1051-1053,  1995.376 neuromorphic systems engineering  f- ........... ~  ~ --i  i "~ "~' 1  i i  i ~. ]  ,'  i i  ~ ....... ~  i ~ r- ..........  ~ ~ ~-'~ ,,  i ~  ~,,.:.~  i  i  i  --" i,:,~-~ --, , ~;  ~ ~2  ~ ..  ~ ...........  figure 16.8 basic cell for a i-d adaptive retina chip connected to the wta architecture.  the bottom part of the figure contains the adaptive photoreceptor circuit. the top part  of the figure contains a circuit that implements both types of wta networks described in  section 16.2 (depending on the value of vsez). the current mirror in the middle part of the  figure is used to amplify the output current of the adaptive photoreceptor.  ~ -14(~  ~14 065  -5 4 -3 -2 -i 0 1 2 3 4 5 unit posit~n  figure 16.9 simulation results of a i-d array of adaptive photoreceptor circuits with  spatial coupling. the curves plotted represent the spatial impulse response of the system,  obtained by setting all photodiode currents of the array to 100pa, except for the current  of unit zero, which was set to 300pa. the photoreceptor bias voltage was set to vb~as ----  0.4v, the p-type coupling transistor gate voltage was set to v~p = 0.gv and the n-type  coupling transistor gate voltage was varied. as shown, it is possible to obtain center-surround  convolution kernels with different frequency selectivities by changing the values of these gate  voltages.winner-take-all networks 377  [5] r. douglas, m. mahowald, and c. mead. neuromorphic analogue vlsi.  annu. rev. neurosci., 18:255-281, 1995.  [6] t. horiuchi, w. bair, b. bishofberger, j. lazzaro, and c. koch. comput-  ing motion using analog vlsi chips: an experimental comparison among  different approaches. international journal of computer vision, 8:203-  216, 1992.  [7] g. indiveri, j. kramer, and c. koch. system implementations of analog  vlsi velocity sensors. ieee micro, 16(5/:40-49, october 1996.  [8] j. lazzaro, s. ryckebusch, m. a. mahowald, and c. a. mead. winner-  take-all networks of o(n) complexity. in d.s. touretzky, editor, advances  in neural information processing systems, volume 2, pages 703-711, san  mateo - ca, 1989. morgan kaufmann.  [9] s. liu and k. boahen. adaptive retina with center-surround receptive field.  in d.s. touretzky, m.c. mozer, and m.e. hasselmo, editors, advances in  neural information processing systems, volume 8. mit press, 1996.  [10] c. a. mead. analog vlsi and neural systems. addison-wesley, reading,  ma, 1989.  [11] e. niebur and c. koch. control of selective visual attention: modeling the  "where" pathway. in d.s. touretzky, m.c. mozer, and m.e. hasselmo,  editors, advances in neural information processing systems, volume 8. mit  press, 1996.  i12] j. a. starzyk and x. fang. cmos current mode winner-take-all cir-  cult with both excitatory and inhibitory feedback. electronic letters,  29(10):908-910, may 1993.  [13] m. tartagni and p. perona. computing centroids in current-mode tech-  nique. electronics letters, 29(21):1811-1813, october 1993.  [14] e. vittoz and x. arreguit. linear networks based on transistors. elec-  tronics letters, 29(3):297 299, february 1993.v neuromorphic learning17 neuromorphic learning  vlsi systems: a survey  gert cauwenberghs  17.1 introduction  carver mead introduced "neuromorphic engineering" [1] as an interdisciplinary  approach to the design of biologically inspired neural information processing  systems, whereby neurophysiological models of perception and information pro-  cessing in biological systems are mapped onto analog vlsi systems that not  only emulate their functions but also resemble their structure [18]. the mo-  tivation for emulating neural function and structure in analog vlsi is the  realization that challenging tasks of perception, classification, association and  control successfully performed by living organisms can only be accomplished  in artificial systems by using an implementation medium that matches their  structure and organization.  essential to neuromorphic systems are mechanisms of adaptation and learn-  ing, modeled after the "plasticity" of synapses and neural structure in biological  systems [25, 4]. learning can be broadly defined as a special case of adapta-  tion whereby past experience is used effectively in readjusting the system re-  sponse to previously unseen, although similar, stimuli. based on the nature  and availability of a training feedback signal, learning algorithms for artificial  neural networks fall under three broad categories: unsupervised, supervised  and reward/punishment (reinforcement). physiological experiments have re-  vealed plasticity mechanisms in biology that correspond to hebbian unsuper-  vised learning [19], and classical (pavlovian) conditioning [17, 22] characteristic  of reinforcement learning.382 neuromorphic systems engineering  mechanisms of adaptation and learning also provide a means to compen-  sate for analog imperfections in the physical implementation of neuromorphic  systems, and fluctuations and uncertainties in the environment in which it op-  erates. to this end, it is crucial that the learning be continuously performed  on the system in operation. this enables the system to be functionally self-  contained, and to adapt continuously to the environment in which they operate.  for neuromorphic systems which involve a large number of parameters such as  synapses in a densely connected neural network, it is imperative that the learn-  ing functions be an integral part of the hardware, implemented locally and  interfacing directly with the synaptic functions.  practical limits of integrated implementation of learning functions are im-  posed by the degree of locality implied by the learning rule, and the available  memory bandwidth and fanout provided by the technology. this is an im-  portant point to consider in the design, and determines whether an electronic,  optical, or hybrid implementation is most suited for the targeted application.  a very important consideration as well is the need for locally storing the analog  or digital parameter values, to retain the information being extracted during  learning. not surprisingly, technological issues of adaptation and memory are  directly related, and both need to be addressed concurrently.  a vast research literature is dedicated to various styles of neural hardware  implementations with provisions for learning, some of it with integrated learn-  ing functions. a selection of the literature (which is bound to be incomplete  even at the time of printing!) is included in the list of references below. some  examples of early implementations of neural systems with integrated adap-  tation and learning functions can be found in edited volumes such as [8, 9]  and [10], in conference proceedings such as nips, ijcnn (icnn/wcnn) and  iscas, and in special and regular issues of journals such as ieee transactions  on neural networks (may 1992 and 1993 [12, 13]), ieee micro (micro-neuro  special issues) and kluwer's international journal of analog integrated cir-  cuits and signal processing [14, 15]. the exposition below will serve as a brief  description of a limited cross-section of research in the field over the last decade  (mainly focusing on analog vlsi systems), as well as a general coverage of the  important issues.  17.2 adaptation and learning  definitions for the terms adaptation and learning come in several varieties, dif-  fering with the particular discipline in which it is formulated, such as cognitive  science, neuroscience, neural computation, artificial intelligence, information  theory, and control theory.  from a system level perspective, a general framework for adaptation and  learning is depicted in figure 17.1 [18]. a system with adjustable parameters p~  (vector p) interacts with the environment through sensory inputs and activation  outputs. an adaptive element, either internal or external to the system, adjusts  the parameters of the system to "optimize" some performance index that isanalog vlsi stochastic perturbative learning 383  "p  li s stem  ~(p)  - element  figure 17.1 adaptation and learning in an information processing system by adjusting the  analog system parameters pi to optimize a performance index ~'(p). the system interacts  with the environment through its sensory inputs and activation outputs.  either defined or implied in relation to the system and its interaction with  the environment. in most models of learning and adaptation, the measure  of performance is quantified either as an error index $(p) which needs to be  minimized:  p = argmin Â£(p)  or, equivalently, a quality index which needs to be maximized. the optimization  is subject to suitable constraints that have to do with physical limits on the  system as well as other requirements on the system and the way it interacts  with the environment.  what distinguishes learning from more general forms of adaptation is the  way in which the system uses past experience in trying to respond effectively  to previously unseen, although similar, input stimuli. the distinct objective in  learning is to generalize beyond the specifics of the presented input samples,  and minimize the expected value of e(p) from the underlying statistics of the  training samples:  p = argmin e(e(p)) .  based on the nature and availability of a training feedback signal in the for-  mulation of e(~(p)), learning algorithms for artificial neural networks (anns)  fall under three broad categories: supervised [30], unsupervised [27], and re-  ward/punishment (reinforcement)[2].  supervised learning [19]-[24] assumes that a "teacher" is continuously  target available to produce target values yk (t) for the outputs yk(t), whereby  the (instantaneous) error index is quantified as the distance between ac-  tual and target outputs  $(p;t) = e target ly~ (t) - ye(t)l ~ , (17.1)  ks!sariaujs s!q,l. 'smo:~sjs o*!:m!aoss~-oaoaori oa!:ld~p~ su!z!u~$ao-j[os Â£ii~u  -aalu! al~aouo$ oa soanaao:~!qaa~ s>iao~aou snoi.iva ql!m sx~at Â£u~m m  pau!qmoa oq uva saria~oadd~ sm.ua~o i pos!aaadns pu~ pas!aaodnsufl :sp!aqÂ£ h  â¢ [aoaderia lxou oq:~] u! uoa!$ oar aidmvxo uiolsÂ£s ~ pu~ igqa $oi~u~ m  mo~sÂ£s su!ua~o i auamoaaoju~oa uo gwla~ "aauosaoauoa jo paods pas~oaa  -u~ zoj uodvmaoju ~ auo~p~a$ i~aoaaoa su!sn [i~] ,,$u!mm~a$oad a~m~uÂ£p  adsvno q poau~ap% pu~ 'uod~aoa ~ Â£a~iod i~m~ldo aoj oa~ds oams oqa uo  uo~aaunj ani,* v sulsn [6g] ~u~uj~i-b '[g] jo asia i~d~ds ~ ~ [~g] (y)g&  ao ,,~u~uava[ aauaaa~p amd,, apnlau~ Â£aga '[0~ 'gg] aaua$~ilaau~ i~pg~aav  u~ ~u!ua~a i jo slopom oa paanoa Â£iosoio 'saa~aodo 1~ ran~ u~ auomoadua  aql pu~ maasxs aq~ jo sdnsuodvloa i~snva arl uo suo~admnssv aomd ou  a~vm qan~ luamu$~ss~ ~paaa jo sms~u~qaam i~uaalu~ ash odxa ~u~ua~a i  auamaaaquda aqa 1o smqlvo$iv 'pa~oa ao Â£ai~uad aqa xq paa~a~pu~  ssaaans ao aanhg maasds aqa oi ~u~pvai as~d or1 u~ suoda~ aiq~suodsoa  oa a~paaa aadoad jo auamu$~sv aql s~ su~uavo i jo adxa sna u~ Â£alna~p  aq& "sandano maasxs aqa aq pagdads aasam v lnoqa!~ 'sauamqs~und  puv spav~aa poxno p 'omd-oloaas~p oa paa~m~ i s~ oau~maqaad maasxs uo  ~a~qpaoj i~uaoaxa oiqnna~ oqa samnss~ [6g]@i] ~u!uaeoi ~uomoaamu!o~  "[saaadmta ~udtoiio j aria jo ouo] u! paq!aas  -op s! aaz~au~nb aolaoa Â£a~u~q ~u~ua~a[ is~a v "[0~] Â£aoaqa oau~uosoa  oa~ad~p~ pu~ '[8g] aaz~au~nb aoaaaa ~ u~ su~aaasnia su~am-~ '[98 'zg] sa~a  -omam oadv!aoss~-oanv '~ao~aau i~anau su~z!u~$ao-jla~ ~ u ! [gg] su~ua~a i  u~qqah apniau~ sanb~uraaa su~ua~ai pasdaadnsun i~a~dx& "[i~] sass~ia  lndano aqa jo qa~uo i uodd~aaso p osvaaa~ oq~ oz~m~u~m o1 (Â£duoi~a~nba)  ao '[6g] sassvia andano olaaas~p an1 puv sandu~ ~oi~u~ an] uoamaaq uod~m  -ao~u~ i~n~nm aqa az~m~x~m oa s~ aoqad 'smaoa adoaooqa-uo~a~maqu ~ m  possaadxo aq u~a ~assna uoa~laq sav~puno q aqa ~udsn[p~ aq uo~aaa~aa  or& "$mssaaoad uod~maqu ~ auanbasqns aq uodmuosaadoa mvp olqm~ns  aaom as~aarao u~ ao uomsaad~oa m~p jo maq a~os aa~nbaa 2[auaaaru ~  qan~ s~s'~a aq papua~u~ aa~ adja sna jo saag~ssvlo "m~p oqa jo sa~ls~vls  su~xlaopun aqa uo pasvq sandu~ Â£j~ssvia oa sadmaaa~ pue 'aoqa~aa ieuaaa  -xo u~ moaj ~a~qpaa 1 xu~ amnss~ aou soop [0g]-[gg] ~u!uaeoi pos!aaodnsun  "[ao:~d~ria axou oril]  m. pa:~uosaad s! sa!tu~udp auoaanaaa ql~ isqa u! ~u:ua~ai pos!aaodns jo  ald~xa maas~s v '[>8 '~gj salq~a~a aa~as aqa u~ sa~m~ujp auaaanaaa ra~  saan~anaas xaldmoa aaom oa puv '[og] saanaanaas pav~aqpaaj i~aau~ aaom  oa papuaaxa aq uva ran~ pu~ 'nnv pa~aqpaaj aoÂ£ndinm v uo (i'zi) jo  auaasap aua~pm$ oa po~idd~ uod~duaaa~p 1o alna u~qa aq~ Â£iaadaa~a s~  qan~ '[ig] uod~$~doad~avq s~ smqa~ao$iv nu~ua~o i ii ~ jo anndod asom  or& "au~i-uo paz~mddo puv paaeni~aa oq uva 's[~u~s su~u~aa aa~am aqa  jo smaaa u~ pagbu~nb Â£[aaaa~p 'xapu~ aau~maqaad or1 pu~ paugop [ia~  s~ nsm nu~ua~a i aqa aau~s 'auamaldm ~ oa ~u~ua~a i 1o asia aso~s~a aqa asuas  ~ u~ s~ su~ua~a i pos~aaodn s '0 < a ~aou qa~ a~aaam aau~as~p v sumn  dnii:i~hnidn~ sin~li,$xs olhdltoinohfktn p{~analog vlsi stochastic perturbative learning 385  reaches beyond neural nets in the restricted sense of what is convention-  ally known as anns, and includes fuzzy neural systems [41, 42, 43] as well  as "hierarchical mixture of experts" models trained with the expectation-  maximization algorithm [44]. in both cases, internal structure is learned  using unsupervised clustering techniques based on the input statistics,  and the output structure is trained through (gradient-based and other)  supervised learning.  17.3 technology  biological neural systems are built out of "wetware" components in an imple-  mentation medium which is necessarily different from technologies available to  the implementation of artificial computing systems, such as semiconductors and  optical propagation media. the neuromorphic engineering approach extends  the functionality and structure of biological systems to artificial systems built  with components and architectures that closely resemble their biological coun-  terparts at all levels, transparent to differences in technology. still, the physical  limits on size, density and connectivity depend strongly on the technology used.  most neural hardware implementations use vlsi technology, which is func-  tionally highly versatile but mostly restricted to two dimensions. the planar  nature of vlsi technology is not necessarily a restriction for neural implemen-  tations since neural structures such as in the cerebral cortex are mostly two-  dimensional as well-- after all the brain is itself a folded 2-d structure. optical  free-space interconnects, on the other hand, allow synaptic densities presently  unavailable in state-of-the-art vlsi technology. hybrid opto-electronic sys-  tems combine the technological advantages of both worlds, with functionally  rich local vlsi processing and global optical interconnects.  for learning and adaptation, a central issue in all implementation technolo-  gies is the local storage of synaptic parameters. this issue, together with the  means of incrementally adapting the stored parameters, is addressed below  in particular detail. for brevity, the exposition focuses mainly on electronic  implementations in analog vlsi technology.  17.3.1 vlsi subthreshold mos technology  mos transistors operating in the subthreshold region [25] are attractive for use  in medium-speed, medium-accuracy analog vlsi processing, because of the  low current levels and the exponential current-voltage characteristics that span  a wide dynamic range of currents [48] (roughly from 100 fa to 100 na for a  square device in 2 #m cmos technology at room temperature). subthreshold  mos transistors provide a clear "neuromorph" [1], since their exponential i-v  characteristics closely resemble the carrier transport though cell membranes in  biological neural systems, as governed by the same boltzman statistics [46].  the exponential characteristics provide a variety of subthreshold mos circuit  topologies that serve as useful computational primitives (such as nonlinear con-  ductances, sigmoid nonlinearities, etc.) for compact analog vlsi implementa-386 neuromorphic systems engineering  /adapt  aqadapt wstÂ°red g dii b  --  -  figure 17.2 adaptation and memory in analog vlsi: storage cell with charge buffer.  tion of neural systems [18]. of particular interest are translinear subthreshold  mos circuits, derived from similar bipolar circuits [48]. they are based on the  exponential nature of current-voltage relationships, and offer attractive com-  pact implementations of product and division operations in vlsi.  17.3.2 adaptation and memory  learning in analog vlsi systems is inherently coupled with the problem of  storage of analog information, since after learning it is most often desirable to  retain the learned weights for an extended period of time. the same is true  for biological neural systems, and mechanisms of plasticity for short-term and  long-term synaptic storage are not yet clearly understood.  in vlsi, analog weights are conveniently stored as charge or voltage on a  capacitor. a capacitive memory is generically depicted in figure 17.2. the  stored weight charge is preserved when brought in contact with the gate of an  mos transistor, which serves as a buffer between weight storage and the imple-  mentation of the synaptic function. an adaptive element in contact with the  capacitor updates the stored weight in the form of discrete charge increments  1 ystored(t q- /kt) : vstored(t) if- ~a(~adapt(t) (17.2)  or, equivalently, a continuous current supplying a derivative  ~tvstored(t) = 1 ~iadapt(t) (17.3)  where aq~dapt (t) = f~+at/adapt (t')dt t.  on itself, a floating gate capacitor is a near-perfect memory. however, leak-  age and spontaneous decay of the weights result when the capacitor is in volatile  contact with the adaptive element, such as through drain or source terminals  of mos transistors. this distinguishes volatile from non-volatile storage vlsi  technology. an excellent review of analog memories for neural computation is  given in [12].analog vlsi stochastic perturbative learning 387  non-volatile memories [50]-[61] contain adaptive elements that interface  with the floating gate capacitor by capacitive coupling across an insu-  lating oxide. in standard cmos vlsi technologies, charge transport  through the oxide is typically controlled by tunneling [85, 51, 50, 52], hot  electron injection [60] or uv-activated conduction [180, 53, 57, 7]. flash  memories offer fast adaptation rates (msecs) and long retention times  (years) without the need for high programming voltages or uv light, but  are not standardly available in cmos processes.  volatile memories [50],[62]-[67] offer fast adaptation rates and instanta-  neous reprogramming of the parameter values, using a voltage-controlled  ohmic connection to the capacitor in the form of mos switches and  switched current sources. a leakage current results from the reverse  diode formed between source and drain diffusions and bulk connection  of a switch transistor. the leakage typically resticts the retention time  of the memory to the msec range, adequate for short-term storage. an  active refresh mechanism is required for long-term storage [50],[63]-[10].  an adaptive element which combines active refresh storage and incremen-  tal adaptation, and which allows a random-access read and write digital  interface, is described in [the next chapter].  other implementations frequently use local or external digital storage of the  parameters, combined with either local or multiplexed d/a conversion. this  solution is less attractive for large-scale neural processors with local learning  functions that require incremental adaptation of the parameters, since then  the increment would need to be performed in digital as well. both volatile  and non-volatile analog memories allow incremental updates in direct analog  format, according to (17.2) or (17.3).  the non-volatile solution is more attractive than volatile alternatives when  long-term storage is a more pressing concern than speed of adaptation and  flexibility of programming. the volatile scheme is particularly useful in multi-  plexed hardware implementations for multi-purpose applications or to realize  virtual larger-scale systems, requiring frequent reloading of large blocks of par-  tial weight matrices. this could be done with an external digital cache memory  and an array of a/d/a :onverters for bi-directional digital read and write ac-  cess to the synaptic array [6]. random-access memory addressing in digital  format is on itself a valuable feature for system-level interfacing.  17.3.3 emerging technologies  innovation and continued progress in information technology benefits the design  of learning neural systems of larger size and better performance, as it benefits  other information processing systems. some relatively new developments in  vlsi include micro-electromechanical systems (mems) [68], wafer-scale in-  tegration [142, 144], chip-scale packaging [69], and silicon-on-insulator (sol)  integrated circuit fabrication [70, 71]. the latter is of special interest to analog388 neuromorphic systems engineering  storage, because significant reduction of leakage currents due to bulk reverse  diodes in mos switches allows longer retention times of capacitive memories.  continued technology developments in optical and optoelectronic informa-  tion processing in combination with mature vlsi technology hold the potential  for significant performance improvements in artificial neural information pro-  cessing systems [151]-[159], promising massive inter-chip connectivity as needed  for larger size neural networks. high-density optical storage and adaptation  for integrated learning could be achieved in 3-d optical media such as photo-  refractive crystals.  17.4 architecture  learning algorithms that are efficiently implemented on general-purpose digital  computers do not necessarily map efficiently onto analog vlsi hardware. the  good news is that the converse is also true, as it is well known that special-  purpose processors tuned to a given task easily outperform most general-  purpose computing engines, on that particular task. from the perspective  of computational efficiency, it is therefore important to closely coordinate the  design of algorithms and corresponding vlsi architecture to ensure an optimal  match.  important guidelines in efficiency of computation dictate the usual principles  commonly taught in modern vlsi design: locality, scalability, and parallelism.  the principle of locality confines intensive computations to the cell level, and  restricts global operations to nearest-neighbor interactions. in addition, certain  scalar global operations which can be easily performed with a single common  wire in analog vlsi technology are allowed, such as global summing of currents  or charges, and global communication of voltage-coded variables. scalability  implies that the implemented algorithms cannot scale stronger than second or-  der in a linear parameter such as the number of neurons, since nothing more  complex than a 2-d array can be implemented on an extended scale in planar  vlsi technology. parallelism in this context implies that the number of opera-  tions performed concurrently at any given time scales linearly with the number  of cells.  even if the learning algorithm supports a parallel and scalable architecture  suitable for analog vlsi implementation, inaccuracies in the implementation  of the learning functions may significantly affect the performance of the trained  system. neuromorphic principles call for a distributed architecture not only for  the network of neurons but also to implement the learning functions, robust to  localized errors in the implementation.  17.4.1 incremental outer-product learning in distributed systems  for networks with distributed neurons such as linear and multilayer percep-  trons [21]  xi = f(~-~ pijxj) (17.4)  janalog vlsi stochastic perturbative learning 389  gradient descent of an lms error functional g defined on the output neurons  x~ ut gives rise to incremental outer-product learning rules of the form  apij = r i xjei (17.5)  where the backpropagation of the error variables ei is derived by application of  the chain rule for differentiation as [30]  05  e~ ut =: --]~ ox~ut  oj -_ j;  i (17.6)  where f~ denotes the derivative of the function f(.) evaluated at its argument  in (17.4). outer-product rules of the form (17.5) are local: synaptic updates  are constructed from intersecting variables at the location of the synapses.  the general class of learning algorithms of the incremental outer-product type  include  supervised learning: the delta rule [19] and backpropagation [21] for  supervised learning in linear or multilayer feedforward perceptrons with a  functional (17.1). also included, with stochastic rather than deterministic  neurons, are boltzman learning in networks of stochastic neurons [22, 72],  and pulse firing neural nets [90].  unsupervised learning: hebbian learning [25], where ei = f~x~ corre-  sponding to a functional g ~ - ~i x~ 2. the k-means clustering algorithm  for learning vector quantization (lvq) [28] is a special case of the latter,  where the nonlinearity in the output layer fk selects a single winner across  all outputs k. kohonen topology-preserving maps [27] further include the  neighbors of the winner k + 1 into the learning updates. learning in art  networks [30] also fits in this category although it is slightly more in-  volved. learning in hopfield networks [26] is hebbian in slightly modified  form.  hybrids and variants: fuzzy maps, hetero-associative neural networks,  radial basis networks, etc. which conform to the general structure of  eqns. (17.4)-(17.6) and their variants and combinations.  reinforcement learning: the reinforcement learning updates for both  the action network and the adaptive critic in [2] are of the general in-  cremental outer-product form (17.5), although modulated with a global  (common) reinforcement signal, and low-pass filtered for credit assign-  ment back in time. see [the next chapter] for more details on the equiva-  lent gradient-descent outer-product formulation. an outer-product vlsi  implementation is described in [9].390 neuromorphic systems engineering  .... ....  ei,/ ~ x i  e i  figure 17.3 incremental outer-product learning.  model; (b) simplified vlsi architecture. (a) feedforward and backpropagation  since all of the above learning algorithm share essentially the same incre-  mental outer-product learning rule, they can be cast into the same general  vlsi architecture depicted in figure 17.3. clearly, this architecture exhibits  the desirable properties of locality, parallelism and scalability. forward and  backward signal paths xj and e~ traverse in horizontal and vertical directions  through the array of synapse cells p~j. the neuron nonlinearity f(.) and its  derivative are implemented at the output periphery the array. several layers  of this structure can be cascaded in alternating horizontal and vertical direc-  tions to form multi-layer perceptrons. the array architecture of figure 17.3 (b)  forms the basis for many of the implemented vlsi learning systems [72]-[104].  one example, described in [7], arguably contains the densest vlsi array for  general outer-product learning developed to date, using only two transistors for  synapse and learning operations per cell. an array of single-transistor learning  synapses for certain classes of learning is presented in [60].  digital vlsi implementations [140]-[150] differ from the analog architecture  mainly in that contributions to the summations in (17.4) and (17.6) cannot  be accumulated onto a single line. global summations are most commonly  implemented using a systolic array architecture.  17.4.2 localized outer-product learning in ceflular neural systems  notice that the fully interconnected architecture of figure 17.3 (b) becomes  inefficient when the network that it implements has sparse connectivity. a lim-  iting case of sparsely interconnected networks are cellular neural networks [106],  in which neurons only interact with their immediate neighbors, conveniently ar-  ranged on a 2-d grid. since the synaptic connections in networks of this type  are only peripheral, the implementation architecture is determined directly by  the topology of the neurons in relation with their neighbors. the synapse andanalog vlsi stochastic perturbative learning 391  learning functions are integrated at the neuron level, rather than distributed  over an array as in figure 17.3 (b). other than that, the same principles  hold, and rules of the outer-product type as illustrated in figure 17.3 (a) are  implemented locally at the neuron inter-cell level [105]-[108].  17.4.3 model-free learning approaches  although model-based approaches for learning such as the outerproduct  learning models described above are fairly robust to mismatches in the im-  plementation of the learning functions owing to their distributed architec-  ture [119, 120, 123, 124], the same can not be said a priori of more general  classes of learning which do not fit the outerproduct type. this is particu-  larly so for recurrent neural networks with hidden internal dynamics for which  learning complexity rises sharply with the number of parameters [23, 24], or  for more complex systems of which a model is difficult to derive or unknown  to the learning element. model-free approaches to learning [12] do not assume  a particular model for the system nor the environment in which it operates,  and derive parameter updates ap~ by physically probing the dependency of  the performance index Â£ on the parameters p~ through perturbations ~ri on the  parameters.  the term "model-free" pertains to the learning, and not necessarily to the  structure of the system itself being adapted, which can be anything and which  clearly is parametric. the main advantage of model-free learning is that it  leaves tremendous freedom in configuring the system, which is allowed to change  structurally on-line as learning progresses, without the need to compile models.  this is particularly useful for training reconfigurable architectures [136, 112].  the insensitivity of learning performance to inaccuracies in the implemented  system, and the ability to learn systems with intractible models, are direct  benefits of model-free learning. an additional benefit of stochastic perturba-  tive learning approaches seems to be that the synaptic noise thus introduced  improves generalization performance of the learned system [121].  variants on perturbative model-free learning use some limited model infor-  mation to train feedforward multilayer anns more effectively [14, 132, 134].  the question of how much model information can be reliably used is impor-  tant, although truly model-free approaches are most generally applicable and  expandable, and their performance does not significantly suffer from the lack  of complete gradient information on the error Â£ as some asymptotic theory  establishes [4].  the model-free nature of learning applies to general learning tasks beyond  the traditionally supervised and unsupervised, and can be extended to rein-  forcement learning. an extensive study of model-free supervised and rein-  forcement learning architectures with examples of analog vlsi systems is the  subject of [the next chapter].392 neuromorphic systems engineering  17.5 systems  several examples of adaptive and/or learning vlsi systems with applications  in vision, speech, signal processing, pattern recognition, communications, con-  trol and physics are included in the references [171]-[203]. this list is by no  means complete, and the spectrum of applications will likely expand as the new  application areas are discovered and research advances create new ways of using  adaptation and learning in the design of intelligent neuromorphic information  processing systems.  covering such diverse range of disciplines across neurobiology, artificial in-  telligence, cognitive science, information theory, etc., research on learning sys-  tems is bound to develop further as different concepts and experimental evi-  dence combine to bridge the gap between bottom-up and top-down modeling  approaches, towards the engineering of truly intelligent autonomous learning  systems, and towards a better understanding of learning mechanisms in biolog-  ical neural systems at different levels of abstraction.  references  [1] c. a. mead. neuromorphic electronic systems. in proceedings of the  ieee, volume 78-10, pages 1629-1639, 1990.  [2] c. a. mead. analog vlsi and neural systems. addison-wesley, reading,  ma, 1989.  neurobiological inspiration  [3] g. m. shepherd. the synaptic organization of the brain. oxford univ.  press, new york, 3 edition, 1992.  [4] p. churchland and t. sejnowski. the computational brain. mit press,  1990.  [5] s. r. kelso and t. h. brown. differential conditioning of associative  synaptic enhancement in hippocampal brain slices. science, 232:85-87,  1986.  [6] r. d. hawkins, t. w. abrams, t. j. carew, and e. r. kandell. a cel-  lular mechanism of classical conditioning in aplysia: activity-dependent  amplification of presynaptic facilitation. science, 219:400-405, 1983.  [7] p. r. montague, p. dayan, c. person, and t. j. sejnowski. bee foraging  in uncertain environments using predictive hebbian learning. nature,  377(6551):725 728, 1996.  [81 edited book volumes, journal issues and reviews  c. a. mead and m. ismail, editors. analog vlsi implementation of  neural systems. kluwer, norwell, ma, 1989.analog vlsi stochastic perturbative learning 393  [9] n. morgan, editor. artificial neural networks: electronic implementa-  tions. ieee computer society press, ca, los alamitos, 1990.  [10] e. sÂ£nchez-sinencio and c. lau, editors. artificial neural networks:  electronic implementations. ieee computer society press, 1992.  [11] m.a. jabri, r.j. coggins, and b.g. flower. adaptive analog vlsi neural  systems. chapman hall, london, uk, 1996.  [12] e. sgnchez-sinencio and r. newcomb. special issue on neural network  hardware. in ieee transactions on neural networks, volume 3-3. ieee  press, 1992.  [13] e. s~nchez-sinencio and r. newcomb. special issue on neural network  hardware. in ieee transactions on neural networks, volume 4-3. ieee  press, 1993.  [14] t. s. lande, editor. special issue on neuromorphic engineering. int. j.  analog int. circ. signal proc., march 1997.  [15] m. bayoumi g. cauwenberghs and e. sÂ£nchez-sinencio, editors. special  issue on learnin9 in silicon. int. j. analog int. circ. signal proc., to  appear.  [16] g. cauwenberghs et. al. learning on silicon. in special session, proc. int.  symp. circuits and systems, hong kong, june 1997.  [17] h. p. graf and l. d. jackel. analog electronic neural network circuits.  ieee circuits and devices mag. , 5:44 49, 1989.  [18] g. cauwenberghs. adaptation, learning and storage in analog vlsi. in  proceedings of the ninth annual ieee international asic conference,  rochester, ny, september 1996.  learning models  supervised learning  [19] b. widrow and m. e. hoff. adaptive switching circuits. ire wescon  convention record, 4:96-104, 1960.  [20] p. werbos. beyond regression: new tools for prediction and analysis in  the behavioral sciences. in the roots of backpropagation. wiley, new  york, 1993.  [21] d.e. rumelhart, g. e. hinton, and r. j. williams. learning internal rep-  resentations by error propagation. in d. e. rumelhart, j. l.mcclelland,  and the pdp research group, editors, parallel distributed processing:  explorations in the microstructures of cognition, volume i: foundations.  mit press/bradford books, cambridge, ma, 1986.  [22] g. e. hinton and t. j. sejnowski. learning and relearning in boltzman  machines. in d. e. rumelhart and j. l. mcclelland, editors, parallel394 neuromorphic systems engineering  distributed processing, explorations in the microstructure of cognition,  volume 1. mit press, cambridge, ma, 1986.  [23] r. j. williams and d. zipser. a learning algorithm for continually running  fully recurrent neural networks. neural computation, 1(2):270 280, 1989.  [24] b. a. pearlmutter. learning state space trajectories in recurrent neural  networks. neural computation, 1(2):263-269, 1989.  unsupervised learning  [25] d. o. hebb. the organization of behavior. wiley, new york, ny, 1949.  [26] j. hopfield. neural networks and physical systems with emergent collec-  tive computational abilities. in proc. natl. acad. sci., volume 97, pages  2554 2558, 1982.  [27] t. kohonen. self-organisation and associative memory. springer-verlag,  berlin, 1984.  [28] a. gersho and 1~. m. gray. vector quantization and signal compression.  kluwer, norwell, ma, 1992.  [29] r. linsker. self-organization in a perceptual network. ieee computer,  21:105-117, 1988.  [30] g. a. carpenter. neural network models for pattern-recognition and  associative memory. neural networks, 2(4):243-257, 1989.  [31] c. m. bishop. neural networks for pattern recognition. oxford univer-  sity press, 1995.  reinforcement learning and related models  [32] k. s. narendra and m. a. l. thatachar. learning automata--a survey.  in ieee t. syst. man and cybern., volume smc-4, pages 323-334, 1974.  [33] s. grossberg. a neural model of attention, reinforcement, and discrimi-  nation learning. international review of neurobiology, 18:263 327, 1975.  [34] a. g. barto, r. s. sutton, and c. w. anderson. neuronlike adaptive  elements that can solve difficult learning control problems. ieee trans-  actions on systems, man, and cybernetics, 13(5):834-846, 1983.  [35] s. grossberg and d. s. levine. neural dynamics of attentionally modu-  lated pavlovian conditioning: blocking, inter-stimulus interval, and sec-  ondary reinforcement. applied optics, 26:5015-5030, 1987.  [36] r. s. sutton. learning to predict by the methods of temporal differences.  machine learning, 3:9 44, 1988.  [37] p. j. werbos. a menu of designs for reinforcement learning over time. in  w. t. miller, r. s. sutton, and p. j. werbos, editors, neural networks  for control, pages 67 95. mit press, cambridge, ma, 1990.analog vlsi stochastic perturbative learning 395  [38] w. t. miller, r. sutton, and p. werbos, editors. neural networks for  control. mit press, cambridge, ma:, 1990.  [39] c. watkins and p. dayan. q-learning. machine learning, 8:279-292,  1992.  [40] w.-m. shen. autonomous learning from the environment. freeman,  computer science press, new york, ny, 1994.  hybrid learning approaches  [41] g. a. carpenter et al. fuzzy artmap - a neural network architecture for  incremental supervised learning of analog multidimentional maps. ieee  transactions on neural networks, 3(5):698-713, 1992.  [42] d. white and d. sofge, editors. handbook of intelligent control: neural,  adaptive and fuzzy approaches. van nostrand, new york, 1992.  [43] p. 3. werbos. neurocontrol and elastic fuzzy logic: capabilities, con-  cepts, and applications. ieee transactions on industrial electronics,  40(2):170-180, 1993.  [44] m. jordan and r. jacobs. hierarchical mixtures of experts and the em  algorithm. neural computation, 6:181-214, 1994.  [45] r. m. sanner and j. j. e. slotine. gaussian networks for direct adaptive  control. ieee transactions on neural networks, 3(6):837-864, 1992.  technology  subthreshold mos operation  [46] a. l. hodgkin and a. f. huxley. current carried by sodium and potas-  sium ions through the membrane of the giant axon of loligo. journal of  physiology, 1952.  [47] e. vittoz and j. fellrath. cmos analog integrated circuits based on weak  inversion operation. ieee journal on solid-state circuits, 12(3):224-231,  1977.  [48] a. g. andreou, k. a. boahen, p. o. pouliquen, a. pavasovid, r. e.  jenkins, and k. strohbehn. current-mode subthreshold mos circuits for  analog vlsi neural systems. ieee transactions on neural networks,  2(2):205-213, 1991.  analog storage  [49] y. horio and s. nakamura. analog memories for vlsi neurocomputing.  in e. sgnchez-sinencio and c. lau, editors, artificial neural networks:  paradigms, applications, and hardware implementations, pages 344-363.  ieee press, 1992.396 neuromorphic systems engineering  [50] e. vittoz, h. oguey, m. a. maher, o. nys, e. dijkstra, and  m. chevroulet. analog storage of adjustable synaptic weights. in vlsi  design of neural networks, pages 47-63. kluwer academic, norwell ma,  1991.  [51] m. a. holler. vlsi implementations of learning and memory systems,.  in advances in neural information processing systems, volume 3, pages  993-1000. morgan kaufman, san marco, ca, 1991.  non-volatile analog storage  [52] a. kramer, c. k. sin, r. chu, and p. k. ko. compact eeprom-based  weight functions. in advances in neural information processing systems,  volume 3, pages 1001-1007. morgan kaufman, san mateo, ca, 1991.  [53] d. a. kerns, j. e. tanner, m. a. sivilotti, and j. luo. cmos uv-  writable non-volatile analog storage. in proc. advanced research in vlsi  int. conf., santa cruz ca, 1991.  [54] a. soennecken, u. hilleringmann, and k. goser. floating gate structures  as nonvolatile analog memory cells in 1.0#m-locos-cmos technology  with pzt dielectrica. microel eng, 15:633-636, 1991.  [55] b. w. lee, b. j. sheu, , and h. yang. analog floating-gate synapses for  general-purpose vlsi neural computation. ieee trans. on circuits and  systems, 38:654-658, 1991.  [56] d. a. durfee and f. s. shoucair. low programming voltage floating gate  analog memory cells in standard vlsi cmos technology. electronics  letters, 28(10):925 927, may 1992.  [57] r. g. benson. analog vlsi suprevised learning system. phd thesis,  california institute of technology, 1993.  [58] o. fujita and y. amemiya. a floating-gate analog memory device for  neural networks. ieee device, 40(11):2029 2055, november 1993.  [59] a. thomsen and m. a. brooke. low control voltage programming of  floating-gate mosfets and applications. ieee circ i, 41(6):443-452, june  1994.  [60] p. hasler, c. diorio, b. a. minch, and c. mead. single transistor learning  synapses. in advances in neural information processing systems 7, pages  817-824. mit press, cambridge, ma, 1995.  [61] h. won, y. hayakawa, k. nakajima, and y. sawada. 'switched diffusion  analog memory for neural networks with hebbian learning-function and its  linear-operation. ieice t. fund. el. comm. comp. sci.d elect commun  comp sci, e79a(6):746-751, june 1996.analog vlsi stochastic perturbative learning 397  volatile analog storage and refresh  [62] d. b. schwartz, r. e. howard, and w. e. hubbard. a programmable  analog neural network chip. ieee j. solid-state circuits, 24:313-319,  189.  [63] b. hochet, v. peiris, s. abdo, and m. 3. declercq. implementation of  a learning kohonen neuron based on a new multilevel storage technique.  ieee j. solid-state circuits, 26(3):262-267, 1991.  [64] r. castello, d. d. caviglia, m. franciotta, and f. montecchi. selfrefresh-  ing analog memory cell for variable synaptic weights. electronics letters,  27(20):1871-1873, 1991.  [65] g. cauwenberghs and a. yariv. fault-tolerant dynamic multi-level stor-  age in analog vlsi. ieee transactions on circuits and systems ii,,  41(12):827-829, 1994.  [66] g. cauwenberghs. a micropower cmos algorithmic a/d/a converter.  ieee transactions on circuits and systems i: fundamental theory and  applications, 42(11):913 919, 1995.  [67] j. g. elias, d. p. m. northmore, and w. westerman. an analog memory  device for spiking silicon neurons. neural computation, 9:419-440, 1997.  emerging vlsi technologies  [68] b. gupta, r. goodman, f. 3iang, y. c. tai, s. tung, and c. m. ho. ana-  log vlsi system for active drag reduction. ieee micro mag., 16(5):53-  59, october 1996.  [69] t. distefano and 3. fjelstad. chip-scale packaging meets future design  needs. solid state tech., 39(4):82, april 1996.  [70] b. elkareh, b. chen, and t. stanley. silicon-on-insulator - an emerging  high-leverage technology. ieee t. comp. pack. man. techn. part a,  18(1):187 194, march 1995.  [71] c. m. hu. soi (silicon-on-insulator) for high-speed ultra large-scale inte-  gration. japan jap 1, 33(1b):365 369, 3anuary 1994.  architecture  outer-product supervised learning  [72] 3. alspector, b. gupta, and r. b. allen. performance of a stochastic  learning microchip. in advances in neural information processing sys-  tems, volume 1, pages 748-760. morgan kaufman, san mateo, ca, 1989.  [73] f. m. a. salam and y. w. wang. a real-time experiment using a 50-  neuron cmos analog silicon chip with on-chip digital learning. ieee t.  neural networks, 2(4):461-464, 1991.398  [74]  [75] neuromorphic systems engineering  c. r. schneider and h. c. card. cmos mean field learning. electronics  letters, 27(19):1702-1704, 1991.  g. cauwenberghs, c. f. neugebauer, and a. yariv. analysis and ver-  ification of an analog vlsi outer-product incremental learning system.  ieee transactions on neural networks, 3(3):488-497, 1992.  [76] s. p. eberhardt, r. tawel, t. x. brown, t. daud, and a. p. thakoor.  analog vlsi neural networks - implementation issues and examples in  optimization and supervisvised learning. ieee t. ind. el., 39(6):552-  564, december 1992.  [77] y. arima, m. murasaki, t. yamada, a. maeda, and h. shinohara. a  refreshable analog vlsi neural network chip with 400 neurons and 40k  synapses. ieee j. of solid state circuits, 27:1854-1861, 1992.  [78] r. g. benson and d. a. kerns. uv-activated conductances allow for  multiple time scale learning. ieee transactions on neural networks,  4(3):434-440, 1993.  [79] k. soelberg, r. l. sigvartsen, t. s. lande, and y. berg. an analog  continuous-time neural-network. int. j. analog integ. circ. signal proc,  5(3):235-246, may 1994.  [80] t. morie and y. amemiya. an all-analog expandable neural-network  lsi with on-chip backpropagation learning. ieee j. solid-state circuits,  29(9):1086-1093, september 1994.  [81] f. j. kub and e. w. justh. analog cmos implementation of high-  frequency least-mean square error learning circuit. ieee j. solid-state  circuits, 30(12):1391-1398, december 1995.  [82] y. berg, r. l. sigvartsen, t. s. lande, and -&. abusland. an analog  feedforward neural-network with on-chip learning. int. j. analog integ.  circ. signal proc, 9(1):65-75, january 1996.  [83] j. w. cho, y. k. choi, and s. y. lee. modular neuro-chip with on-chip  learning and adjustable learning parameters. neural proc. letters, 4(1),  1996.  [84] m. valle, d. d. caviglia, and g. m. bisio. an experimental analog vlsi  neural-network with on-chip backpropagation learning. int. j. analog  integ. circ. signal proc., 9(3):231-245, april 1996.  outer-product unsupervised learning  [85] 3. p. sage and r. s. withers. analog nonvolatile memory for neural net-  work implementations. in artificial neural networks: electronic imple-  mentations, pages 22-32. ieee computer society press, ca, los alami-  tos, 1990.  [86] k. a. boahen, p. o. pouliquen, a. g. andreou, and r. e. jenkins. a  heteroassociative memory using current-mode mos analog vlsi circuits.  ieee t. circ. syst, 36(5):747-755, 1989.analog vlsi stochastic perturbative learning 399  [87] j. r. mann and s. gilbert. an analog self-organizing neural network  chip. in advances in neural information processing systems, volume 1,  pages 739-747. morgan kaufman, san mateo, ca, 1989.  [88] a. hartstein and r. h. koch. a self-learning neural network. in advances  in neural information processing systems, volume 1, pages 769-776. mor-  gan kaufman, san mateo, ca, 1989.  [89] m. r. walker, s. haghighi, a. afghan, and l. a. akers. training a  limited-interconnect, synthetic neural ic. in advances in neural infor-  mation processing systems, volume 1, pages 777-784. morgan kaufman~  san mateo, ca, 1989.  [90] a. murray. pulse arithmetic in vlsi neural networks. ieee micro mag.,  pages 64-74, december 1989.  [91] y. arima, k. mashiko, k. okada, t. yamada, a. maeda, and et al. a  336-neuron, 28k-synapse, self-learning neural network chip with branch-  neuron-unit architecture. ieee j. solid-state circuits, 26(11):1637-1644,  1991.  [92] b. j. maundy and e. i. elmasry. a self-organizing switched-capacitor  neural network. ieee t. circ. syst., 38(12):1556-1563, december 1991.  [93] d. a. watola and j. l. meador. competitive learning in asynchronous-  pulse-density integrated-circuits. int. j. analog integ. circ. signal proc.,  2(4):323 344, november 1992.  [94] j. donald and l. akers. an adaptive neural processor node. ieee  transactions on neural networks, 4(3):413-426, 1993.  [95] y. he and u. cilingiroglu. a charge-based on-chip adaptation kohonen  neural network. ieee transactions on neural networks, 4(3):462-469,  1993.  [96] d. macq, m. verleysen, p. jespers, and j. d. legat. analog implementa-  tion of a kohonen map with on-chip learning. ieee t. neural networks,  4(3):456 461, may 1993.  [97] b. linares-barranco, e. sgnchez-sinencio, a. rodriguez-vazquez, and  j. l. huertas. a cmos analog adaptive bam with on-chip learning and  weight refreshing. ieee trans. on neural networks, 4:445 457, 1993.  [98] p. helm and e. a. vittoz. precise analog synapse for kohonen feature  maps. ieee j. solid-state circuits, 29(8):982-985, august 1994.  [99] g. cauwenberghs and v. pedroni. a charge-based cmos parallel analog  vector quantizer. in advances in neural information processing systems,  volume 7, pages 779-786, cambridge, ma, 1995. mit press.  [100] t. shibata, h. kosaka, h. ishii, and t. ohmi. a neuron-mos neural-  network using self-learning-compatible synapse circuits. ieee j. solid-  state circuits, 30(8):913-922, august 1995.  [101] r. y. liu, c. y. wu, and i. c. jou. a cmos current-mode design  of modified learning-vector-quantization neural networks. int. j. analog  integ. circ. signal proc., 8(2):157-181, september 1995.4oo  [102]  [103]  [104] neuromorphic systems engineering  c. y. wu and j. f. lan. mos current-mode neural associative memory  design with on-chip learning. ieee t. neural networks, 7(1):15~181,  january 1996.  k. hosono, k. tsuji, k. shibao, e. io, and h. yonezu et al. fundamen-  tal device and circuits for synaptic connections in self-organizing neural  networks. ieice t. electronics, e79c(4):560-567, april 1996.  t. serrano-gotarredona and b. linares-barranco. a real-time clustering  microchip neural engine. ieee t. vlsi systems, 4(2):195-209, june  1996.  adaptive cellular neural networks  [105] p. tzionas, p. tsalides, and a. thanailakis. design and vlsi imple-  mentation of a pattern classifier using pseudo-2d cellular automata. iiee  proc g, 139(6):661-668, december 1992.  [106] t. roska and l. o. chua. the cnn universal machine - an analogic  array computer. ieee t. circ. syst. ii, 40(3):163-173, march 1993.  [107] y. miyanaga and k. tochinai. parallel vlsi architecture for multilayer  self-organizing cellular network. ieice t. electronics, e76c(7):1174-  1181, july 1993.  [108] s. espejo, r. carmona, r. dominguez-castro, and a. rodriguez-  vazquez. a cnn universal chip in cmos technology. int j. circuit  theory appl., 24(1):93-109, 1996.  adaptive fuzzy classifiers  [109] j. w. fattaruso, s. s. mahant-shetti, and j. b. barton. a fuzzy logic  inference processor. ieee journal of solid-state circuits, 29(4):397-401,  1994.  [110] z. tang, y. kobayashi, o. ishizuka, and k. tanno. a learning fuzzy  network and its applications to inverted pendulum system. ieice t.  fund. el. comm. comp. sci., e78a(6):701-707, june 1995.  [111] f. vidal-verdu and a. rodriguez-vazquez. using building blocks to de-  sign analog neuro-fuzzy controllers. ieee micro, 15(4):49-57, august  1995.  [112] w. pedrycz, c. h. poskar, and p. j. czezowski. a reconfigurable fuzzy  neural-network with in-situ learning. ieee micro, 15(4):19-30, august  1995.  [113] t. yamakawa. silicon implementation of a fuzzy neuron. ieee fuz sy,  4(4):488-501, november 1996.analog vlsi stochastic perturbative learning 401  reinforcement learning  [114] c. schneider and h. card. analog cmos synaptic learning circuits  adapted from invertebrate biology. ieee t. circ. syst., 38(12):1430-  1438, december 1991.  [115] t. g. clarkson, c. k. ng, and y. guan. the pram: an adaptive vlsi  chip. ieee trans. on neural networks, 4(3):408-412, 1993.  [116] a. f. murray, s. churcher, a. hamilton, a. j. holmes, and g. b. jackson  et al. pulse stream vlsi neural networks. ieee micro, 14(3):29 39, june  1994.  [117] g. cauwenberghs. reinforcement learning in a nonlinear noise shaping  oversampled a/d converter. in proc. int. symp. circuits and systems,  hong kong, june 1997.  nonidealities and error models  [118] m. j. s. smith. n analog integrated neural network capable of learning the  feigenbaum logistic map. ieee transactions on circuits and systems,  37(6):841-844, 1990.  [119] r. c. frye, e. a. rietman, and c.c. wong. back-propagation learning  and nonidealities in analog neural network hardware. ieee transactions  on neural networks, 2(1):110-117, 1991.  [120] l. m. reyneri and e. filippi. an analysis on the performance of sili-  con implementations of backpropagation algorithms for artificial neural  networks. ieee comput, 40(12):1380-1389, 1991.  [121] a. murray and p. j. edwards. synaptic noise during mlp training en-  hances fault-tolerance, generalization and learning trajectory. in ad-  vances in neural information processing systems, volume 5, pages 491-  498. morgan kaufman, san mateo, ca, 1993.  [122] p. thiran and m. hasler. self-organization of a one-dimensional kohonen  network with quantized weights and inputs. neural networks, 7(9):1427-  1439, 1994.  [123] g. cairns and l. tarassenko. precision issues for learning with analog  vlsi multilayer perceptrons. ieee micro, 15(3):54-56, june 1995.  [124] b. k. dolenko and h. c. card. tolerance to analog hardware of on-  chip learning in backpropagation networks. ieee t. neural networks,  6(5):1045-1052, september 1995.  [125] mode1-~ree learning  a. dembo and t. kailath. model-free distributed learning. ieee trans-  actions on neural networks, 1(1):58-70, 1990.402  [126] neuromorphic systems engineering  m. jabri and b. flower. weight perturbation: an optimal architecture  and learning technique for analog vlsi feedforward and recurrent multi-  layered networks. ieee transactions on neural networks, 3(1):154-157,  1992.  [127] g. cauwenberghs. a fast stochastic error-descent algorithm for super-  vised learning and optimization. in advances in neural information pro-  cessing systems, volume 5, pages 244-251, san mateo, ca, 1993. morgan  kaufman.  [128] j. alspector, r. meir, b. yuhas, and a. jayakumar. a parallel gradient  descent method for learning in analog vlsi neural networks. in advances  in neural information processing systems, volume 5, pages 836-844, san  mateo, ca, 1993. morgan kaufman.  [129] b. flower and m. jabri. summed weight neuron perturbation: an ~(n)  improvement over weight perturbation. in advances in neural informa-  tion processing systems, volume 5, pages 212-219, san mateo, ca, 1993.  morgan kaufman.  [130] d. kirk, d. kerns, k. fleischer, and a. barr. analog vlsi implementa-  tion of gradient descent. in advances in neural information processing  systems, volume 5, pages 789-796, san mateo, ca, 1993. morgan kauf-  man.  [131] g. cauwenberghs. a learning analog neural network chip with  continuous-recurrent dynamics. in advances in neural information pro-  cessing systems, volume 6, pages 858-865, san mateo, ca, 1994. morgan  kaufman.  [132] p. hollis and j. paulos. a neural network learning algorithm tailored for  vlsi implementation. ieee tran. neural networks, 5(5):784-791, 1994.  [133] g. cauwenberghs. an analog vlsi recurrent neural network learning  a continuous-time trajectory. ieee transactions on neural networks,  7(2), march 1996.  [134] a. j. montalvo, r. s. gyurcsik, and j. j. paulos. toward a general-  purpose analog vlsi neural-network with on-chip learning. ieee t.  neural networks, 8(2):413-423, march 1997.  chip-in-the-loop training  [135] m. holler, s. tam, h. castro, and r. benson. an electrically trainable  artificial neural network (etann) with 10240 floating gate synapses. in  proc. int. joint conf. neural networks, pages 191-196, washington dc,  1989.  [136] s. satyanarayana, y. tsividis, and h. p. graf. a reconfigurable analog  vlsi neural network chip. in advances in neural information processing  systems, volume 2, pages 758-768. morgan kaufman, san mateo, ca,  1990.analog vlsi stochastic perturbative learning 403  [137] e. sackinger, b. e. boser, and l. d. jackel. a neurocomputer board  based on the anna neural network chip. in advances in neural infor-  mation processing systems, volume 4, pages 773-780. morgan kaufman,  san mateo, ca, 1992.  [138] j. a. lansner. an experimental hardware neural-network using a cascad-  able, analog chipset. int j elect, 78(4):679 690, april 1995.  [139] j. o. klein, h. pujol, and p. garda. chip-in-the-loop learning algorithm  for boltzmann machine. electronics letters, 31(12):986 988, june 1995.  digital implementations  [140] a. johannet, l. personnaz, g. dreyfus, j. d. oascuel, and m. weinfeld.  specification and implementation of a digital hopfield-type associative  memory with on-chip training. ieee t. neural networks, 3(4):529 539,  july 1992.  [141] t. shima, t. kimura, y. kamatani, t. itakura, y. fujita, and t. iida.  neuro chips with on-chip back-propagation and/or hebbian learning.  ieee j. of solid-state circuits, 27(12):1868-1875, 1992.  [142] m. yasunaga, n. masuda, m. yagyu, m. asai, and k. shibata et al. a  self-learning digital neural network using wafer-scale lsi. ieee j. solid-  state circuits, 28(2):106 114, february 1993.  [143] c. lehmann, m. viredaz, and f. blayo. a generic systolic array building-  block for neural networks with on-chip learning. ieee t. neural net-  works, 4(3):400-407, may 1993.  [144] m. fujita, y. kobayashi, k. shiozawa, t. takahashi, and f. mizuno et al.  development and fabrication of digital neural-network wsis. ieice t.  electronics, e76c(7):1182 1190, july 1993.  [145] p. murtagh, a. c. tsoi, and n. bergmann. bit-serial systolic array im-  plementation of a multilayer perceptron. in ieee proc e, volume 140-5,  pages 27~288, september 1993.  [146] t. morishita and i. teramoto. neural-network multiprocessors applied  with dynamically reconfigurable pipeline architecture. ieice t. elec-  tronics, e77c(12):1937-1943, december 1994.  [147] z. tang and o. ishizuka. design and implementations of a learn-  ing t-model neural-network. ieice t. fund. el. comm. comp. sci.,  e78a(2):259-263, february 1995.  [148] m. p. perrone and l. n. cooper. the nil000: high speed parallel vlsi  for implementing multilayer perceptrons. in advances in neural infor-  mation processing systems, volume 7, pages 747 754. morgan kaufman,  san mateo, ca, 1995.  [149] et al. j. wawrzynek. spert-ii: a vector microprocessor system and its  application to large problems in backpropagation training. in advances in404 neuromorphic systems engineering  [150] neural information processing systems, volume 8, pages 619-625. mor-  gan kaufman, san mateo, ca, 1996.  s. rehfuss and d. hammerstrom. model matching and sfmd computa-  tion. in advances in neural information processing systems, volume 8,  pages 713-719. morgan kaufman, san mateo, ca, 1996.  optical and optoelectronic implementations  [151] j. ohta, y. nitta, and k. kyuma. dynamic optical neurochip using  variable-sensitivity photodiodes. optics lett, 16(10):744-746, 1991.  [152] d.z. anderson, c. benkert, v. hebler, j.-s. jang, d, montgomery, and  m. saffman. optical implementation of a self-organizing feature extrac-  tor. in advances in neural information processing systems, volume 4,  pages 821-828. morgan kaufman, san mateo, ca, 1992.  [153] y. nitta, j. ohta, s. tai, and k. kyuma. optical learning neurochip with  internal analog memory. appl optics, 32(8):1264-1274, march 1993.  [154] k. wagner and t. m. slagle. optical competitive learning with vlsi  liquid-crystal winner-take-all modulators. appl optics, 32(8):1408 1435,  march 1993.  [155] m. oita, y. nitta, s. tai, and k. kyuma. optical associative memory  using optoelectronic neurochips for image-processing. ieice t. electron-  ics, e77c(1):56-62, january 1994.  [156] e. lange, y. nitta, and k. kyuma. optical neural chips. ieee micro,  14(6):29-41, december 1994.  [157] a. j. waddie and j. f. snowdon. a smart-pixel optical neural-network  design using customized error propagation. inst. phys. conf. series,  139:511-514, 1995.  [158] k. tsuji, h. yonezu, k. hosono, k. shibao, and n. ohshima et al. an op-  tical adaptive device and its application to a competitive learning circuit.  in japan jap 1, volume 34-2b, pages 1056-1060, february 1995.  [159] w. e. foor and m. a. neifeld. adaptive, optical, radial basis func-  tion neural-network for handwritten digit recognition. appl optics,  34(32):7545-7555, november 1995.  architectural novelties  [160] j. alspector, j. w. gannett, s. haber, m. b. parker, and r. chu.  a vlsi-efficient technique for generating multiple uncorrelated noise  sources and its application to stochastic neural networks. ieee t. circ.  syst., 38(1):109-123, 1991.  [161] p. a. shoemaker, m. j. carlin, and r. l. shimabukuro. back propagation  learning with trinary quantization of weight updates. neural networks,  4(2):231-241, 1991.]162]  [163]  [164]  [165]  [166]  [167]  [168]  [169]  [170] analog vlsi stochastic perturbative learning 405  y. h. pao and w. hafez. analog computational models of concept-  formation. int. j. analog integ. circ. signal proc., 4(2):265-272, novem-  ber 1992.  t. morie and y. amemiya. deterministic boltzmann machine learn-  ing improved for analog lsi implementation. ieice t. electronics,  e76c(7):1167-1173, july 1993.  s. p. deweerth and d. m. wilson. fixed-ratio adaptive thresholding  using cmos circuits. electronics letters, 31(10):788 789, may 1995.  m. vandaalen, j. zhao, and j. shawetaylor. 'real-time output derivatives  for on chip learning using digital stochastic bit stream neurons. electron-  ics letters, 30(21):1775-1777, october 1994.  v. petridis and k. paraschidis. on the properties of the feedforward  method - a simple training law for on-chip learning. ieee t. neural  networks, 6(6):1536-1541, november 1995.  h. singh, h. s. bawa, and l. anneberg. boolean neural-network real-  ization of an adder subtractor cell. microel rel, 36(3):367 369, march  1996.  t. lehmann, e. bruun, and c. dietrich. mixed analog-digital matrix-  vector multiplier for neural-network synapses. int. j. analog integ. circ.  signal proc., 9(1):55-63, january 1996.  t. serrano-gotarredona and b. linares-barranco. a modified art-1  algorithm more suitable for vlsi implementations. neural networks,  9(6):1025-1043, august 1996.  m. l. marchesi, f. piazza, and a. uncini. backpropagation without  multiplier for multilayer neural networks. in ieee p. circ., volume 143-  4, pages 229 232, august 1996.  [171] systems applications of learning  general purpose neural emulators  p. mueller, j. van der spiegel, d. blackman, t. chiu, t. clare, c. don-  ham, t.p. hsieh, and m. lionaz. design and fabrication of vlsi com-  ponents for a general purpose analog neural computer. in analog vlsi  implementation of neural systems, pages 135-169. kluwer, norwell, ma,  1989.  blind signal processing  [172] e. vittoz and x. arreguit. cmos integration of herault-jutten cells for  separation of sources. in analog vlsi implementation of neural systems,  pages 57-83. kluwer, norwell, ma, 1989.406  [1731  [174] neuromorphic systems engineering  m. h. cohen and a. g. andreou. current-node subthreshold mos imple-  tnentation of the herault-jutten autoadaptive network. ieee j. of solid  state circuits, 27:714-727, 1992.  r. p. mackey, j. j. rodriguez, j. d. carothers, and s. b. k. vrudhula.  asynchronous vlsi architecture for adaptive echo cancellation. electron-  ics letters, 32(8):710-711, april 1996.  [175] biomedical adaptive signal processing  r. coggins, m. jabri, m. flower, and s. pickard. iceg morphology clas-  sification using an analogue vlsi neural network. in advances in neural  information processing systems, volume 7, pages 731-738. morgan kauf-  man, san marco, ca, 1995.  speech research  [176] et ah j. wawrzynek. spert-ih a vector microprocessor system and its  application to large problems in backpropagation training. in advances in  neural information processing systems, volume 8, pages 619 625. mor-  gan kaufman, san mateo, ca, 1996.  [177] john lazzaro. temporal adaptation in a silicon auditory nerve. in john e.  moody, steve j. hanson, and richard p. lippmann, editors, advances in  neural information processing systems, volume 4, pages 813-820. mor-  gan kaufmann publishers, inc., 1992.  [1781 olfactory sensory processing  p. a. shoemaker, c. g. hutchens, and s. b. patil. a hierarchical-  clustering network based on a model of olfactory processing. int. j.  analog integ. circ. signal proc., 2(4):297-311, november 1992.  focal-plane sensors and adaptive vision systems  [179] 3. tanner and c. a. mead. an integrated analog optical motion sensor. in  s. Â¥. kung, editor, vlsi signal processing ii, pages 59-76. ieee press,  new york, 1986.  [180] c. a. mead. adaptive retina. in c. mead and m. ismail, editors, analog  vlsi implementation of neural systems, pages 239-246. kluwer aca-  demic pub., norwell, ma, 1989.  [181] m. mahowald. an analog vlsi stereoscopic vision system. kluwer  academic, boston, ma, 1994.  [182] t. delbriick. silicon retina with correlation-based velocity-tuned pixels.  ieee transactions on neural networks, 4(3):529-541, may 1993.analog vlsi stochastic perturbative learning 407  [183] j. c. lee, b. j. sheu, and w. c. fang. vlsi neuroprocessors for video  motion detection. ieee transactions on neural networks, 4(2):78-191,  1993.  [184] r. etienne-cummings, j. van der spiegel, and p. mueller. vi,si model of  primate visual smooth pursuit. in advances in neural information pro-  cessing systems, volume 8, pages 707-712. morgan kaufman, san marco,  ca, 1996.  [185] r. sarpeshkar, j. kramer, g. indiveri, and c. koch. analog vlsi ar-  chitectures for motion processing - from fundamental limits to system  applications. p ieee, 84(7):969-987, july 1996.  [186] k. a. boahen. a retinomorphic vision system. ieee micro, 16(5):30-39,  october 1996.  [187] s. c. liu and c. mead. continuous-time adaptive delay system. ieee  t. circ. syst. ii, 43(11):744-751, november 1996.  optical character recognition  [188] b. y. chen, m. w. mao, and j. b. kuo. coded block neural network  vlsi system using an adaptive learning-rate technique to train chinese  character patterns. electronics letters, 28(21):1941 1942, october 1992.  [189] c. s. miou, t. m. shieh, g. h. chang, b. s. chien, and m. w. chang  et al. optical chinese character-recognition system using a new pipelined  matching and sorting vlsi. opt eng, 32(7):1623-1632, july 1993.  [190] s. maruno, t. kohda, h. nakahira, s. sakiyama, and m. maruyama.  quantizer neuron model and neuroprocessor-named quantizer neuron  chip. ieee j. sel. areas comm., 12(9):1503 1509, december 1994.  [191] image compression  w. c. fang, b. j. sheu, o. t. c. chen, and j. choi. a vlsi neural  processor for image data-compression using self-organization networks.  ieee transactions on neural networks, 3(3):506-518, 1992.  communications and decoding  [192] j. g. choi, s. h. bang, and b. j. sheu. a programmable analog vlsi  neural-network processor for communication receivers. ieee t. neural  networ/~s, 4(3):484-495, may 1993.  [193] m.i. chan, w. t. lee, m. c. lin, and l. g. chen. ic design of an adaptive  viterbi decoder. ieee t. cons. el., 42(1):52-62, february 1996.  [194] r. mittal, k. c. bracken, l. r. carley, and d. j. allstot. a low-power  backward equalizer for dfe read-channel applications. ieee j. sosd-state  circuits, 32(2):270-273, february 1997.408  [195] neuromorphic systems engineering  b. c. rothenberg, j. e. c. brown, p. j. hurst, and s. h. lewis. a  mixed-signal ram decision-feedback equalizer for disk drives. ieee j.  solid-state circuits, 32(5):713-721, 1997.  clock skew timing control  [196] w. d. grover, j. brown, t. priesen, and s. marsh. all-digital multipoint  adaptive delay compensation circuit for low skew clock distribution. elec-  tronics letters, 31(23):1996-1998, november 1995.  [197] m. mizuno, m. yamashina, k. furuta, h. igura, and h. abiko et al. a  ghz mos adaptive pipeline technique using mos current-mode logic.  ieee j. solid-state circuits, 31(6):784-791, june 1996.  [198] e. w. justh and f. j. kub. analog cmos continuous-time tapped delay-  line circuit. electronics letters, 31(21):1793-1794, october 1995.  control and autonomous systems  [199] y. harata, n. ohta, k. hayakawa, t. shigematsu, and y. kita. a  fuzzy inference lsi for an automotive control. ieice t. electronics,  e76c(12):1780-1781, december 1993.  [200] g. jackson and a. f. murray. competence acquisition is an autonomous  mobile robot using hardware neural techniques. in adv. neural informa-  tion processing systems, volume 8, pages 1031-1037. mit press, cam-  bridge, ma, 1996.  high-energy physics  [201] t. lindblad, c. s. lindsey, f. block, and a. jayakumar. using software  and hardware neural networks in a higgs search. nucl inst a, 356(2-  3):498-506, march 1995.  [202] c. s. lindsey, t. lindblad, g. sekhniaidze, g. szkely, and m. miner-  skjold. experience with the ibm zisc036 neural-network chip. int j.  modern phys. c, 6(4):579-584, august 1995.  [203] g. anzellotti, r. battiti, i. lazzizzera, g. soncini, and a. zorat et al.  totem - a highly parallel chip for triggering applications with induc-  tive learning based on the reactive tabu search. int j. modern phys. c,  6(4):555-560, august 1995.18 analog vlsi stochastic  perturbative learning  architectures  gert cauwenberghs  18.1 introduction  learning and adaptation are central to the design of neuromorphic vlsi sys-  tems that perform robustly in variable and unpredictable environments.  learning algorithms that are efficiently implemented on general-purpose dig-  ital computers do not necessarily map efficiently onto analog vlsi hardware.  even if the learning algorithm supports a parallel and scalable architecture  suitable for analog vlsi implementation, inaccuracies in the implementation  of the learning functions may significantly affect the performance of the trained  system. learning can only effectively compensate for inaccuracies in the net-  work implementation when their physical sources are contained directly inside  the learning feedback loop. algorithms which assume a particular model for  the underlying characteristics of the system being trained perform poorer than  algorithms which directly probe the response of the system to external and  internal stimuli.  a second source of concern in the design of neuromorphic vlsi learning  systems has to do with the assumptions made on the particular form of the  performance criterion being optimized. in typical physical systems, the learning  objectives can not be clearly defined in terms of a target response or desired  state of the system. learning from external dicrete rewards, in absence of a  well-defined training signal, requires internal mechanisms of credit assignment  which make no prior assumptions on the causal relationships of the system and410 neuromorphic systems engineering  the enviroment in which it operates. the stereotypical example of a system  able to learn from a discrete delayed reward or punishment signal is the pole-  balancer trained with reinforcement learning [2].  we use stochastic perturbative algorithms for model-free estimation of gradi-  ent information [12] in a general framework that includes reinforcement learn-  ing under delayed and discontinuous rewards [2, 15, 28, 29, 31], suitable for  learning in physical systems of which the characteristics nor the optimization  objectives are properly defined. stochastic error-descent architectures for su-  pervised learning [4] and computational primitives of reinforcement learning are  combined into an analog vlsi architecture which offers a modular and cellular  structure, model-free distributed representation, and robustness to noise and  mismatches in the implementation. the combined architecture is applicable to  the most general of learning tasks, where an unknown "black-box" dynamical  system is adapted using a external "black-box" reinforcement-based delayed  and possibly discrete reward signal.  as a proof of principle, we apply the model-free training-free adaptive tech-  niques to blind optimization of a second-order noise-shaping modulator for  oversampled data conversion, controlled by a neural classifier. the only eval-  uative feedback used in training the classifier is a discrete failure signal which  indicates when some of the integrators in the modulation loop saturate.  in the following, we review supervised learning and stochastic perturbative  techniques, and present a corresponding architecture for analog vlsi imple-  mentation. we then cover a generalized form of reinforcement learning, and  introduce a stochastic perturbative analog vlsi architecture for reinforcement  learning. neuromorphic implementations in analog vlsi and system examples  are also included.  18.2 supervised learning  in a metaphorical sense, supervised learning assumes the luxury of a committed  "teacher", who constantly evaluates and corrects the network by continuously  feeding it target values for all network outputs. supervised learning can be  reformulated as an optimization task, where the network parameters (weights)  are adjusted to minimize the distance between the targets and actual network  outputs. generalization and overtraining are important issues in supervised  learning, and are beyond the scope of this paper.  let y(t) be the vector of network outputs with components y~(t), and cor-  respondingly ytarget (t) be the supplied target output vector. the network con-  tains adjustable parameters (or weights) p with components pk, and state vari-  ables x(t) with components xi(t) (which may contain external inputs). then  the task is to minimize the scalar error index  g(p;t) = e target i~ (t) - ~(t)l ~  i  in the parameters p~, using a distance metric with norm u > 0.analog vlsi stochastic perturbative learning 411  18.2.1 gradient descent  gradient descent is the most common optimization technique for supervised  learning in neural networks, which includes the widely used technique of back-  propagation (or "dynamic feedback") [30] for gradient derivation, applicable to  general feedforward multilayered networks.  in general terms, gradient descent minimizes the scalar performance index Â£  by specifying incremental updates in the parameter vector p according to the  error gradient ~'pg:  p(t + 1) = p(t) - r] vpg(t) (18.1)  one significant problem with gradient descent and its variants for on-line super-  vised learning is the complexity of calculating the error gradient components  og/opk from a model of the system. this is especially so for complex systems  involving internal dynamics in the state variables xj (t):  og og(t) oyi(t) ozj(t) (18.2)  opk - e. . oy i oxj opk %?  where derivation of the dependencies oxj/opk over time constitutes a significant  amount of computation that typically scales super-linearly with the dimension  of the network [4]. furthermore, the derivation of the gradient in (18.2) as-  sumes accurate knowledge of the model of the network (y(t) as a function of  x(t), and recurrence relations in the state variables x(t)). accurate model  knowledge cannot be assumed for analog vlsi neural hardware, due to mis-  matches in the physical implementation which can not be predicted at the time  of fabrication. finally, often a model for the system being optimized may not  be readily available, or may be too complicated for practica.1 (real-time) evalu-  ation. in such cases, a black-box approach to optimization is more effective in  every regard. this motivates the use of the well-known technique of stochastic  approximation [21] for blind optimization in analog vlsi systems. we ap-  ply this technique to supervised learning as well as to more advanced models  of "reinforcement" learning under discrete delayed rewards. the connection  between stochastic approximation techniques and principles of neuromorphic  engineering will be illustrated further below, in contrast with gradient descent.  18.2.2 stochastic approximation techniques  stochastic approximation algorithms [21] have long been known as effective  tools for constrained and unconstrained optimization under noisy observations  of system variables [24]. applied to on-line minimization of an error index  Â£(p), the algorithms avoid the computational burden of gradient estimation by  directly observing the dependence of the index g on randomly applied pertur-  bations in the parameter values. variants on the kiefer-wolfowitz algorithm  for stochastic approximation [21], essentially similar to random-direction finite-  difference gradient descent, have been formulated for blind adaptive control [26],412 neuromorphic systems engineering  neural networks [12, 27] and the implementation of learning functions in vlsi  hardware [1, 4, 14, 18].  the broader class of neural network learning algorithms under this category  exhibit the desirable property that the functional form of the parameter up-  dates is "model-free", i.e., independent of the model specifics of the network  or system under optimization. the model-free techniques for on-line super-  vised learning are directly applicable to almost any observable system with  deterministic, slowly varying, and possibly unknown characteristics. parallel  implementation of the stochastic approximation algorithms results into efficient  and modular learning architectures that map well onto vlsi hardware. since  those algorithms use only direct function evaluations and no derivative infor-  mation, they are functionally simple, and their implementation is independent  of the structure of the system under optimization. they exhibit robust conver-  gence properties in the presence of noise in the system and model mismatches  in the implementation.  a brief description of the stochastic error-descent algorithm follows below, as  introduced in [4] for efficient supervised learning in analog vlsi. the integrated  analog vlsi continous-time learning system used in [5, 8] forms the basis for  the architectures outlined in the sections that follow.  18.2.3 stochastic supervised learning  let Â£(p) be the error functional to be minimized, with Â£ a scalar determinis-  tic function in the parameter (or weight) vector p with components pi. the  stochastic algorithm specifies incremental updates in the parameters pi as with  gradient descent (18.1), although using a stochastic approximation to the true  gradient  0Â£(t) est  = 7ri(t). ~(t) (18.3) 0p~  where the differentially perturbed error  1 ~(t) = ~ ($(p(t) + ~r(t)) - Â£(p(t) - 7r(t))) (18.4)  is obtained from two direct observations of $ under complementary activation of  a parallel random perturbation vector ~r(t) with components ~ri(t) onto the pa-  rameter vector p(t). the perturbation components 7ri(t) are fixed in amplitude  and random in sign, ~ri(t) = +a with equal probabilities for both polarities.  the algorithm essentially performs gradient descent in random directions in  the parameter space, as defined by the position of the perturbation vector.  as with exact gradient descent, iteration of the updates using (18.3) con-  verges in the close proximity of a (local) minimum of $, provided the pertur-  bation amplitude a is sufficiently small. the rate of convergence is necessarily  slower than gradient descent, since every observation (18.4) only reveals scalar  information about the gradient vector in one dimension. however, the amountanalog vlsi stochastic perturbative learning 413  of computation required to compute the gradient at every update may out-  weigh the higher convergence rate offered by gradient descent, depending on the  model complexity of the system under optimization. when applied to on-line  supervised learning in recurrent dynamical systems, the stochastic algorithm  provides a net computational efficiency rivaling that of exact gradient descent.  computational efficiency is defined in terms of the total number of operations  required to converge, i.e., reach a certain level of $. a formal derivation of the  convergence properties is presented in [4].  18.2.4 supervised learning architecture  while alternative optimization techniques based on higher-order extensions  on gradient descent will certainly offer superior convergence rates, the above  stochastic method achieves its relative efficiency at a much reduced complexity  of implementation. the only global operations required are the evaluations of  the error function in (18.4), which are obtained from direct observations on  the system under complementary activation of the perturbation vector. the  operations needed to generate and apply the random perturbations, and to  perform the parameter update increments, are strictly local and identical for  each of the parameter components. the functional diagram of the local pa-  rameter processing cell, embedded in the system under optimization, is shown  in figure 18.1. the complementary perturbations and the corresponding error  observations are performed in two separate phases on the same system, rather  than concurrently on separate replications of the system. the sequential acti-  vation of the complementary perturbations and corresponding evaluations of Â£  are synchronized and coordinated with a three-phase clock:  Â¢o : $(p,t)  Â¢+ : $(p+~r,t) (18.5)  Â¢- : $(p- ~r,t)  this is represented schematically in figure 18.1 by a modulation signal o(t),  taking values {-1, 0, 1}. the extra phase Â¢0 (Â¢(t) = 0) is not strictly needed  to compute (18.4)--it is useful otherwise, e.g. to compute finite difference  estimates of second order derivatives for dynamic optimization of the learning  rate ~(t).  the local operations are further simplified owing to the binary nature of the  perturbations, reducing the multiplication in (18.3) to an exclusive-or logical  operation, and the modulation by Â¢(t) to binary multiplexing. besides effi-  ciency of implementation, this has a beneficial effect on the overall accuracy of  the implemented learning system, as will be explained in the context of vlsi  circuit implementation below.  18.2.5 supervised learning in dynamical systems  in the above, it was assumed that the error functional $(p) is directly observable  on the system by applying the parameter values pi. in the context of on-line414 neuromorphic systems engineering  t -n ~(t) ~(t)  __  local i network i -n ~(t) ~-~ i  ' '  "1  '  ~i~ ~ i  ~ global i  ~ ..... ~  figure 18.1 architecture implementing stochastic error-descent supervised learning. the  learning cell is locally embedded in the network. the differential error index is evaluated  globally, and communicated to all cells.  supervised learning in dynamical systems, the error functional takes the form  of the average distance of norm u between the output and target signals over  a moving time window,  fl y ~ target e.t~ g(p;h,tf) = ~ /~ ) - ~(t')l~dt ' ,  â¢ (18.6)  which implicitly depends on the training sequence ytarget (t) and on initial con-  ditions on the internal state variables of the system. an on-line implementation  prohibits simultaneous observation of the error measure (18.6) under different  instances of the parameter vector p, as would be required to evaluate (18.4)  for construction of the parameter updates. however, when the training signals  are periodic and the interval t = tf - ti spans an integer multiple of periods,  the measure (18.6) under constant parameter values is approximately invariant  to time. in that case, the two error observations needed in (18.4) can be per-  formed in sequence, under complementary piecewise constant activation of the  perturbation vector.  in the context of on-line supervised learning in dynamical systems, the re-  quirement of periodicity on the training signals is a limitation of the stochastic  error-descent algorithm. next, this requirement will be relaxed, along with  some more stringent assumptions on the nature of supervised learning. in par-  ticular, a target training signal will no longer be necessary. instead, learning  is based on an external reward signal that conveys only partial and delayed  information about the performance of the network.  18.3 learning from delayed and discrete rewardsanalog vlsi stochastic perturbative learning 415  supervised learning methods rely on a continuous training signal that gives  constant feedback about the direction in which to steer the response of the  network to improve its performance. this continuous signal is available in  the form of target values ytarget(t) for the network outputs y(t). more widely  applicable but also more challenging are learning tasks where target outputs  or other continuous teacher feedback are not available, but instead only a non-  steady, delayed reward (or punishment) signal is available to evaluate the quality  of the outputs (or actions) produced by the network. the difficulty lies in  assigning proper credit for the reward (or punishment) to actions that where  produced by the network in the past, and adapting the network accordingly in  such a way to reinforce the network actions leading to reward (and conversely,  avoid those leading to punishment).  18.3.1 reinforcement learning algorithms  several iterative approaches to dynamic programming have been applied to  solve the credit assignment problem for training a neural network with delayed  rewards [2, 28, 29, 31, 15]. they all invoke an "adaptive critic element" which  is trained along with the network to predict the future reward signal from  the present state of the network. we define "reinforcement learning" essen-  tially as given in [2], which includes as special cases "time difference learning"  or td(Â£) [28], and, to some extent, q-learning [29] and "advanced heuristic  dynamic programming" [31]. the equations are listed below in general form  to clarify the similarity with the above supervised perturbative learning tech-  niques. it will then be shown how the above architectures are extended to allow  learning from delayed and/or impulsive rewards.  let r(t) be the discrete delayed reward signal for state vector x(t) of the  system (components xj(t)), r(t) is zero when no signal is available, and is  negative for a punishment. let y(t) be the (scalar) output of the network in  response to an input (or state) x(t), and q(t) the predicted future reward (or  "value function") associated with state x(t) as produced by the adaptive critic  element. the action taken by the system is determined by the polarity of the  network output, sign(y(t)). for example, in the pole balancing experiment  of [2], y(t) is hard-limited and controls the direction of the fixed-amplitude  force exterted on the moving cart. finally, let w and v (components wi and  vi) be the weights of the network and the adaptive critic element, respectively.  then the weight updates are given by  awl(t) = wi(t + 1) -- wi(t) (18.7)  =  = + 1) - v (t)  = /3 Ã·(t). di(t)  where the "eligibility" functions e~(t) and d~(t) are updated as  5ei(t) + (1 - 5) sign(y(t)) ~ ) (18.8) ei(t + 1) ~w~416 neuromorphic systems engineering  di(t+l) = ikdi(t)+(1-a)oq(t)  ovi  and the reinforcement Ã·(t) is given by  ~(t) = r(t) + 3'q(t) - q(t - 1) (18.9)  the parameters 6 and a define the time span of credit assigned by ei(t) and  di(t) to actions in the past, weighting recent actions stronger than past actions:  t--1  e(t) = (1-6) e 6t-t'-i sign(y(t'))oy(t')  tt =_oc ~wi  t-1  d(t) = (1- a) e at-t'-zoq(t')  ovi iff ~ -- oo  similarly, the parameter ~ defines the time span for the prediction of future  reward by q(t), at convergence:  (3~  t ! q(t)  t~=t+l  for ~ = 1 and y -= q, the equations reduce to td(a). convergence of td(a)  with probability one has been proven in the general case of linear networks of  the form q = ~ vixi [23].  learning algorithms of this type are neuromorphic in the sense that they  emulate classical (pavlovian) conditioning in pattern association as found in  biological systems [17] and their mathematical and cognitive models [16, 22].  furthermore, as shown below, the algorithms lend themselves to analog vlsi  implementation in a parallel distributed architecture which, unlike more compli-  cated gradient-based schemes, resembles the general structure and connectivity  of biological neural systems.  18.3.2 reinforcement learning architecture  while reinforcement learning does not perform gradient descent of a (known)  error functional, the eligibility functions e~(t) and di(t) used in the weight  updates are constructed from derivatives of output functions to the weights.  the eligibility functions in equation (18.8) can be explicitly restated as (low-  pass filtered) gradients of an error function  with s(t) = ly(t)l + q(t)  ei(t+l) = 5ei(t) + (i-5) c~s(~) ow~  di(t + z) = adi(t) + (l-a) oÂ£(t)  ovi (18.10)analog vlsi stochastic perturbative learning 417  rather than calculating the gradients in (18.10) from the network model, we  can again apply stochastic pertubative techniques to estimate the gradients  from direct evaluations on the network. doing so, all properties of robustness,  scalability and modularity that apply to stochastic error descent supervised  learning apply here as well. as in (18.3), stochastic approximation estimates  of the gradient components in (18.10) are  0~(t) est  = ~(t) g(t) owi  c95(t) est  = ~(t) ~(t) ovi  where the differential perturbed error  1 ($(w+w,v+v,t)-g(w-~o,v-v,t)) g(t) = ~ (18.11)  Â¢0 : g(w,v,t)  Â¢+ : g(w+~o,v+v,t)  Â¢- : ~(w-~,v-v,t) (18.12)  in systems with a continuous-time output response, we assume that the time  lag between consecutive observations of the three phases of $ is not an issue,  which amounts to choosing an appropriate sampling rate for t in relation to the  bandwidth of the system.  similarities between the above cellular architectures for supervised learning  and reinforcement learning are apparent: both correlate local perturbation val-  ues ~ri, wi or vi with a global scalar index g that encodes the differential effect is obtained from two-sided parallel random perturbation w -t- w simultaneous  with v + v (iw~l = ivil = ~).  a side benefit of the low-pass filtering of the gradient in (18.10) is an im-  provement of the stochastic gradient estimate (18.11) through averaging. as  with stochastic error descent supervised learning, averaging reduces the vari-  ance of the gradient estimate and produces learning increments that are less  stochastic in nature, although this is not essential for convergence of the learn-  ing process [4].  figure 18.2 shows the block diagram of a reinforcement learning cell and  an adaptive critic cell, with stochastic perturbative estimation of the gradient  according to (18.11). lp~ and lpa denote first-order low-pass filters (18.10)  with time constants determined by d and a. other than the low-pass filtering  and the global multiplicative factor Ã·(t), the architecture is identical to that  of stochastic error descent learning in figure 18.1. as before, the estimation  of ~(t) does not require separate instances of perturbed and non-perturbed  networks shown in figure 18.2, and can be computed sequentially by evaluating  the output of the network and adaptive critic in three phases for every cycle of"lla~ ~p.!j~ a^!~.depv (q) "lla~ :~u!mea i ~.uama~ao.lu!a 8 (~)) 'uop.euu!xoadde ~.ua!pe.~:~  ::)!~,setl::)o~,s ~u~sn ~u!uaea i ~.uauja3joju[aj :~u!~,ualualdlu ! ajn],3a~!l~3je lejaua 9 e'gi ajn:~!_-i  (q)  (Â¢)~a (Â¢)Â¢ + (~)~a (lya  ~,~  ~ ~  ..... a  (~Â¢ (~ u-  (~,)  : - -!~ t .i ~!~  (~)~o~ (~)Â¢ + (~)~m -~ (~)~m  ~,, ...... ,~,;:~~  dnih~[hnidnh s!nh~lgxg dihdho1nohfixn ~ii7analog vlsi stochastic perturbative learning 419  of the perturbations on the output, and both incrementally update the weights  pi, w~ or v~ accordingly. the main difference in reinforcement learning is the  additional gating of the correlate product with a global reinforcement signal Ã·  after temporal filtering. for many applications, the extra overhead that this  implies in hardware resources is more than compensated by the utility of the  reward-based credit assignment mechanism, which does not require an external  teacher. an example is given below in the case of oversampled a/d conversion.  18.3.3 system example: stable higher-order noise-shaping modulation  we evaluated both exact gradient and stochastive perturbative embodiments of  the reinforcement learning algorithms on an adaptive neural classifier, control-  ling a higher-order noise-shaping modulator used for oversampled a/d data  conversion [3]. the order-n modulator comprises a cascade of n integrators  xi(t) operating on the difference between the analog input u(t) and the binary  modulated output y(t):  xo(t + 1) = xo(t) +a (u(t) - y(t)) (18.13)  xi(t+l) = xi(t)+axi_l(t), i=l,...n-1  where a = 0.5. the control objective is to choose the binary sequence y(t) such  as to keep the excursion of the integration variables within bounds, ix~(t)t <  xsat [9].  for the adaptive classifier, we specify a one-hidden-layer neural network,  with inputs x~(t) and output y(t). the network has rn hidden units, a tanh(.)  sigmoidal nonlinearity in the hidden layer, and a linear output layer. for the  simulations we set n = 2 and m = 5. the case n = 2 is equivalent to the single  pole-balancing problem [2].  the only evaluative feedback signal used during learning is a failure signal  which indicates when one or more integration variables saturate, ix~ (t) l _> xsat.  in particular, the signal r(t) counts the number of integrators in saturation:  r(t) = -b e h(ix~(t)l - xs~t) (18.14)  i  where b = 10, and where h(.) denotes a step function (h(x) = 1 if x > 0 and  0 otherwise). the adaptive critic q(t) is implemented with a neural network  of identical structure as for y(t). the learning parameters in (18.7), (18.8)  and (18.9) are 5 = 0.8, ~ = 0.7, 7 = 0.9, c~ = 0.05 and/~ = 0.001. these  values are consistent with [2], adapted to accommodate for differences in the  time scale of the dynamics (18.13). the perturbation strength in the stochastic  version is (r = 0.01.  figure 18.3 shows the learning performance for several trials of both ver-  sions of reinforcement learning, using exact and stochastic gradient estimates.  during learning, the input sequence u(t) is random, uniform in the range  -0.5... 0.5. initially, and every time failure occurs (r(t) < 0), the integration420 neuromorphic systems engineering  104~ l  ~ x x  ['~ x i 10  ~ 10 ~  ~ ~ ~ ~ ~ ~ ~ ~ ~  Ã Ã  100 i  0 5 10 15 20 25  trials  figure 18.3 simulated performance of stochastic perturbative (o) and gradient-based (x)  reinforcement learning in a second-order noise-shaping modulator. time between failures  for consecutive trials from zero initial conditions.  variables x~(t) and eligibilities ek(t) and dk(t) are reset to zero. qualitative  differences observed between the exact and stochastic versions in figure 18.3  are minor. further, in all but one of the 20 cases tried, learning has completed  (i.e., consequent failure is not observed in finite time) in fewer than 20 consec-  utive trial-and-error iterations. notice that a non-zero r(t) is only generated  at failure, i.e., less than 20 times, and no other external evaluative feedback is  needed for learning.  figure 18.4 quantifies the effect of stochastic perturbative estimation of the  gradients (18.10) on the quality of reinforcement learning. the correlation in-  dex c(t) measures the degree of conformity in the eligibilities (both e~(t) and  d~(t)) between stochastic and exact versions of reinforcement learning. corre-  lation is expressed as usual on a scale from -1 to 1, with c(t) = 1 indicating  perfect coincidence. while c(t) is considerably less than 1 in all cases, c(t) > 0  about 95 % of the time, meaning that on average the sign of the parameter  updates (18.7) for exact and stochastic versions are consistent in at least 95 %  of the cases. the scatterplot c(t) vs. Ã·(t) also illustrates how the adaptive  critic produces a positive reinforcement Ã·(t) in most of the cases, even though  the "reward" signal r(t) is never positive by construction. positive reinforce-  ment Ã·(t) under idle conditions of r(t) is desirable for stability. notice thatanalog vlsi stochastic perturbative learning 421  0.6  0.5  0.4  "~ 0â¢3  ~9 ~ 0.2  Â©  ~9 0.1 ~>~  ,6~ ,~  ~ 0 .~  ,~  .~ -o.1  -0.2 ..i, :..;.:." ::.":. .  â¢ â¢ ".' e"=~ !.~ ~'~:~ '~...~..  . .:"~ ...~ ~ :1  â¢  â¢ ",.  i.. !  â¢ ~ ~ ~2~6:. ~'~".'~ n '" "', ,~..~.. ,~.~ -  ~.: ~.~ Â£'  -"~": ,-~ ;i:~. ...-.~,..  â¢ '.;......~ ..:v..:.~.-  â¢ ::.: ;".?:.!~ "  â¢ .~., ,..,  â¢ . .o ~.  -0.3  -0.01 -0.005 0 0.005 0.01 0.015  reinforcement ~(t)  figure 18.4 effect of stochastic perturbative gradient estimation on the reinforcement  learning, c(t) quantifies the degree of conformity in the eli~;ibilities ei(t) and di(t) between  exact and stochastic versionsâ¢  the failure-driven punishment points (where r(t) < 0) are off-scale of the graph  and strongly negative.  we tried reinforcement learning on higher-order modulators, n = 3 and  higher. both exact and stochastic versions were successful for n = 3 in the  majority of cases, but failed to converge for n = 4 with the same parame-  ter settings. on itself, this is not surprising since higher-order delta-sigma  modulators tend to become increasingly prone to unstabilities and sensitive to  small changes in parameters with increasing order n, which is why they are  almost never used in practice [3]. it is possible that more advanced reinforce-  ment learning techniques such as "advanced heuristic dynamic programming"  (ahdp) [31] would succeed to converge for orders n > 3. ahdp offers im-  proved learning efficiency using a more advanced, gradient-based adaptive critic  element for prediction of reward, although it is not clear at present how to map  the algorithm efficiently onto analog vlsi.  the above stochastic perturbative architectures for both supervised and re-  inforcement learning support common "neuromorphs" and corresponding ana-  log vlsi implementations. neuromorphs of learning in analog vlsi are the  subject of next section.422 neuromorphic systems engineering  18.4 neuromorphicanalogvlsi learning  18.4.1 adaptive circuits  the model-free nature of the stochastic perturbative learning algorithms does  not impose any particular conditions on the implementation of computational  functions required for learning. by far the most critical element in limiting  learning performance is the quality of the parameter update increments and  decrements, in particular the correctness of their polarity. relative fluctuations  in amplitude of the learning updates do not affect the learning process to first  order, since their effect is equivalent to relative fluctuations in the learning  rate. on the other hand, errors in the polarity of the learning updates might  adversely affect learning performance even at small update amplitudes.  a binary controlled charge-pump adaptive element is described next. of-  fering precise control of the update polarity, this circuit element provides the  primitives for learning as well as memory in the analog vlsi systems covered  further below.  charge-pump adaptive element. figure 18.5 shows the circuit diagram  of a charge-pump adaptive element implementing a volatile synapse. the cir-  cuit is a simplified version of the charge pump used in [10] and [8]. when  enabled by enn and enp (at gnd and vdd potentials, respectively), the cir-  cuit generates an incremental update of which the polarity is determined by  pol. the amplitude of the current supplying the incremental update is deter-  mined by gate voltages vb~ and vbp, biased deep in subthreshold to allow fine  (sub-fc) increments if needed. the increment amplitude is also determined by  the duration of the enabled current, controlled by the timing of enn and enp.  when both en~ and enp are set midway between gnd and vdd, the current  output is disabled. notice that the switch-off transient is (virtually) free of  clock feedthrough charge injection, because the current-supplying transistors  are switched from their source terminals, with the gate terminals being kept at  constant voltage [10].  measurements on a charge pump with c = 0.5 pf fabricated in a 2 #m  cmos process are shown in figure 18.6. under pulsed activation of enn  and enp, the resulting voltage increments and decrements are recorded as a  function of the gate bias voltages vb~ and vbp, for both polarities of pol, and  for three different values of the pulse width at (23 #see, 1 msec and 40 msec).  in all tests, the pulse period extends 2 msec beyond the pulse width. the  exponential subthreshold characteristics are evident from figure 18.6, with  increments and decrements spanning four orders of magnitude in amplitude.  the lower limit is mainly determined by junction diode leakage currents, as  shown in figure 18.6 (a) for at = 0 (0.01 mv per 2 msec interval at room  temperature). this is more than adequate to accommodate learning over a  typical range of learning rates. also, the binary control pol of the polarity  of the update is effective for increments and decrements down to 0.05 mv in  amplitude, corresponding to charge transfers of only a few hundred electrons.analog vlsi stochastic perturbative learning 423  pol --  figure 18.5 en,  ybp  g.  enn /adapt wstore d  *  aqadapt l  __c 2  __  _  charge-pump adaptive element implementing a volatile synapse.  analog storage. because of the volatile nature of the adaptive element used,  a dynamic refresh mechanism is required if long-term local storage of the weight  values after learning is desired. a robust and efficient self-contained mechanism  that does not require external storage is "partial incremental refresh" [10]  + 1) = - (18.15)  obtained from binary quantization q of the parameter value. stable discrete  states of the analog dynamic memory under periodic actication of (18.15) are  located at the positive transitions of q, illustrated in figure 18.7. long-term  stability and robustness to noise and errors in the quantization requires that  the separation between neighboring discrete states a be much larger than the  amplitude of the parameter updates 6, which in turn needs to exceed the spon-  taneous drift in the parameter value due to leakage between consecutive refresh  cycles [10].  partial incremental refresh can be directly implemented using the adaptive  element in figure 18.8 by driving pol with a binary function of the weight  value [7]. as in [7], the binary quantization function can be multiplexed over  an array of storage cells, and can be implemented by retaining the lsb from  a/d/a conversion [6] of the value to be stored. experimental observation of  quantization and refresh in a fabricated 128-element array of memory cells has  confirmed stable retention of analog storage at 8-bit effective resolution over a  time interval exceeding 109 refresh cycles (several days) [7].  a non-volatile equivalent of the charge-pump adaptive element in fig-  ure 18.5, which does not require dynamic refresh, is described in [13]. corre-  spondingly, a non-volatile learning cell performing stochastic error descent can  be obtained by substitution of the core adaptive element in figure 18.8 below,  and more intricate volatile and non-volatile circuits implementing stochastic  reinforcement learning can be derived from extensions on figure 18.8 and [13].424 neuromorphic systems engineering  ~ 10Â° i  ~ 10-1t ~ ~ lo-2!  ~ 10_3  ~ 10 -4  o  > lo-5 ~om~ ~ /  at=o  . . â¢ . . â¢ ......  .... . . .  . . â¢ . . .  0 0.1 0.2 0.3 0.4 0.5 0.6  gate voltage vbn (v)  (a)  e  o > lo o  10-1  10 -2  10-3  10 -4  10 .5  0 0.1 0.2 0.3 0.4 0.5 0.6  gate voltage vbp (v)  (b)  figure 18.6 measured characteristics of charge-pump adaptive element in 2 #m cmos  with o ---- 0.5 pf. (a) n-type decrements (pol -- 0); (b) p-type increments  (pol = 1).figure 18.7  tal refresh. analog vlsi stochastic perturbative learning 425  q (pi)  +1 ....  i l  ..,,----~  : a  example illustrating binary quantization q and partial incremen-  the non-volatile solution is especially attractive if long-term storage is a more  pressing concern than speed of adaptation and flexibility of programming.  stochastic perturbative learning cell. the circuit schematic of a learning  cell implementing stochastic error descent is given in figure 18.8. the incre-  mental update -~ri~ to be performed in (18.4) is first decomposed in amplitude  and sign components. this allows for a hybrid digital-analog implementation  of the learning cell, in which amplitudes of certain opcrands are processed in  analog format, and their polarities implemented in logic. since i~ril -- 1, the  amplitude ~1~1 is conventiently communicated as a global signal to all cells,  in the form of two gate voltages vbn and vbp. the (inverted) polarity pol is  obtained as the^(inverted) exclusive-or combination of the perturbation 7~i and  the polarity of $. the decomposition of sign and amplitude ensures proper con-  vergence of the learning increments in the presence of mismatches and offsets in  the physical implementation of the learning cell. this is because the polarities  of the increments are more accurately implemented through logic-controlled  circuitry, which are independent of analog mismatches in the implementation.  the perturbation ~ri is applied to pi in three phases (18.4) by capacitive  coupling onto the storage node c. the binary state of the local perturbation  ~ri selects one of two global perturbation signals to couple onto c. the pertur-  bation signals (v~ + and its complement va- ) globally control the three phases  Â¢o, Â¢+ and Â¢- of (18.4), and set the perturbation amplitude o. the simple  configuration using a one-bit multiplexer is possible because each perturbation  component can only take one of two values +0.  18.4.2 learning systems  continuous-time trajectory learning in an analog vlsi recurrent  neural network. on-chip learning of continuous-time recurrent dynamics  has been demonstrated in an analog vlsi neural network, using stochastic  error descent i5, 8]. we briefly summarize the architecuture, operation and re-  sults here. the chip contains an integrated network of six fully interconnected426 neuromorphic systems engineering  ^  sign(~g) y~+  enp t  enn cperturb  ~ ~  pi(t) + ~a(t) r~i(t)  cstore  __  figure 18.8 circuit schematic of a learning cell implementing stochastic error descent,  using the charge pump adaptive element.  continuous-time neurons  d 6  t ~x~ = --xi q-e  j=l w~ a(xj - 0j) + y~ , (18.16)  with xi(t) the neuron states representing the outputs of the network, yi(t) the  external inputs to the network, and a(.) a sigmoidal activation function. the  36 connection strengths wij and 6 thresholds 0j constitute the free parameters  to be learned, and the time constant r is kept fixed and identical for all neurons.  the network is trained with target output signals x~(t) and xt~(t) for two  neuron outputs, i = 1, 2. the other four neurons are hidden to the output,  and the internal dynamics of these hidden neuron state variables play an im-  portant part in optimizing the output. learning consists of minimizing the  time-averaged error (18.6) with respect to the parameters wij and 0j, using  stochastic error descent. for a consistent evaluation of the stochastic gradient,  the perturbed function measurements $(p Â± ~-) are performed on a time scale  significantly (60 times) larger than the period of the target signals.  all local learning functions, including the generation of pseudo-random per-  turbations and the stochastic learning update, are embedded with the synaptic  functions (18.16) in a scalable 2-d array of parameter cells wij and 0j. the  circuitry implementing the learning functions is essentially that of figure 18.8.  the dynamic refresh scheme described above is incorporated locally in the pa-  rameters cell for long-term storage of the parameters. a micrograph of the chip  is shown in figure 18.9. power dissipation is 1.2 mw from a 5 v supply, for a  1 khz signal being trained.analog vlsi stochastic perturbative learning 427  figure 18.9 micrograph of an analog vlsi recurrent neural network chip that  learns continuous-time internal dynamics using stochastic error-descent. cen-  ter: 6 Ã 7 array of weight and threshold parameter cells with integrated learning  and storage functions. bottom: random binary array generator providing the  parallel parameter perturbations.428 neuromorphic systems engineering  Â¢b)  ~:igure 18.10 oscillograms of the network outputs and target signals after  learning, (a) under weak residual teacher forcing, and (b) with teacher forcing  removed. top traces: xl(t) and xit(t). bottom traces: x~(t) and x2t(t).  the results of training the chip with a periodic analog target signal repre-  senting a quadrature-phase oscillator are illustrated in figure 18.10. learning is  achieved in roughly 1500 training cycles of 60 msec each, using "teacher forcing"  during training for synchronization between network and target dynamics, and  by careful but unbiased choice of initial conditions for the weight parameters  to avoid local minima. these conditions are less critical in more general appli-  cations of nonlinear system identification where the network during training is  presented input signals to be associated with the the target output signals.  reinforcement learning in a vlsi neural classifier for nonlinear  noise-shaping del~a-sigma modulation. a vlsi classifier consisting of  64 locally tuned, hard-thresholding neurons was trained using reinforcement  learning to produce stable noise-shaping modulation of orders one and two [9].  while this system does not implement the stochastic version of reinforcement  learning studied above, it presents a particularly simple vlsi implementation  and serves to demonstrate some of the properties also expected of more ad-  vanced implementations that incorporate stochastic learning with continuous  neurons.  similar to the "boxes-system" used in [2], the classifier implements a look-up  table from a binary address-encoded representation of the state space spanned  by u(t) and xi(t). in particular, y(t) = yx(t) and q(t) = qx(t) where x(t) is  the index of the address determined by hard-limiting thresholding operations  on the components u(t) and xi(t). each neuron cell, identified by address k,  locally stores the two parameters yk and qk in analog format, and updates them  according to the external reinforcement signal defined in (18.14).  in its simplest form, the implemented reinforcement learning performs up-  dates in the the eligible y~ parameters opposite to their thresholded output  values, each time failure occurs (r(t) = -1). hysteresis is included in the dy-analog vlsi stochastic perturbative learning 429  namics of the yk updates to ensure some degree of stability under persistent  negative reinforcement during training, even without use of the adaptive critic  qk. although this simple form of reinforcement learning with non-adaptive  hysteretic critic is not meant to be adequate for more complex tasks, it has  proven sufficient to train the vlsi neural classifier to produce noise-shaping  modulation of orders 1 and 2.  the integrated system contains a cascade of 6 integrators, an ll-bit address  state encoder, and an address-encoded classifier with 64 reinforcement learning  neurons on a 2.2 mm Ã 2.2 mm chip in 2 #m cmos technology. a record of a  learning experiment reinforcing first-order noise-shaping modulation in the first  integrator, using 2-bit address encoding x(t) of the polarities of u(t) and xl(t),  is shown in figure 18.11. as in the simulations above, the input sequence u(t)  during training is uniformly random with half full-scale maximum amplitude  (1 v pp), and the integrator variables x~(t) as well as the eligibilities ek(t) are  reset to zero after every occurrence of failure, r(t) = -1. the dynamics of  the state variables and parameters recorded in figure 18.11 shows convergence  after roughly 150 input presentations. the time step in the experiments was  t -- 2.5 msec, limited by the bandwidth of the instrumentation equipment in  the recording. notice that the learned pattern of ya at convergence conforms  to that of a standard first-order delta-sigma modulator [3], which it should in  this rather simple case. learning succeeded at various values of the learning  constants 5 and c~, affecting mainly the rate of convergence.  tests for higher-order noise-shaping modulation on the same learning system  only succeeded for order n -- 2, using a total of 8 parameters yk. for higher  orders of noise-shaping, a continuous neuron representation and learning are  required, as the above simulations of the stochastic reinforcement system indi-  cate.  18.4.3 structural properties  the general structure of neuromorphic information processing systems has some  properties differentiating them from some more conventional human-engineered  computing machinery, which are typically optimized for general-purpose digital  programming. some of the desirable properties for neuromorphic architectures  are: fault-tolerance and robustness to noise through a redundant distributed  representation, robustness to changes in operating conditions through on-line  adaptation, real-time bandwidth through massive parallelism, and modularity  as a result of locality in space and time. we illustrate these properties in  the two architectures for supervised and reinforcement learning in figures 18.1  and 18.2. since both architectures are similar, a distinction between them will  not explicitly be made.  fault-tolerance through statistical averaging in a distributed repre-  sentation:. direct implementation of gradient descent, based on an explicit  model of the network, is prone to errors due to unaccounted discrepancies in430 neuromorphic systems engineering  ". "~''~ Â° Â¢~%'" a" ..... ~" ~,~'~'Â°Â¢'." " "~ "'.t,.;'~" . " .. .'t:f~' o..'~,.v...,.~ ":".t ""Â°.'"  -.~.~.~-. "~'"4". "~" ~ ~',.'--~'. , ~ "n" ~ .&'e, ~,_.:~ .~, ~3:.'.~ -" ~ &.. r-;'.'," "~" ,~'i,-,~#2.a,.'+-f' u ( t ) g.~ ,:.%..'. ,~.'~,l,:~ ,,,~" ~'eoo,. o,.,,~.e~o-, ,,'~. ~f,~:..,..,,,,,,~, .go ~ Â°,,.:.'g~'~.l ,.~" ": -.~"  â¢ :::: ::..: ":<:.,:..:.:.,;: %.  :.'...". ." : - â¢ :vo*:..'.;.'.. ,..,":.;.,,.'."if,,,..'.... .":'.,':.;,:,.'.'; .,,',.;::~%'~ .:" ,:  - *- - .- - .- ..... -- .~:.:~.--'~ ~ .:.~, ~...'~-a'_~-~.'.~ ~. a;.'-,:~-,.-_.~.-a:;a.:~., xl (t) "~ :; ; ~" "'ie..'" ~" ..~. :~.';;....,.-'.~..~'. ".: :. "..". ".~o..Â£"  ,:  fit)  ....... y(t) ~~~_  __j-~ ......................................... yll  ~ ylo  j .......................... yo1  yo0  0 100 200 300 400 500  time t (units t)  figure 18.11 first-order modulator experiments: recorded dynamics of state variables  and parameters during on-chip learning.analog vlsi stochastic perturbative learning 431  the network model and mismatches in the physical implementation of the gra-  dient. this is due to the localized representation in the computation of the  gradient as calculated from the model, in which any discrepancy in one part  may drastically affect the final result. for this and other reasons, it is unlikely  that biology performs explicit gradient calculation on complex systems such as  recurrent neural networks with continuous-time dynamics. stochastic error de-  scent avoids errors of various kinds by physically probing the gradient onto the  system rather than deriving it. using simultaneous and uncorrelated parallel  perturbations of the weights, the effect of a single error on the outcome is thus  significantly reduced, by virtue of the statistical nature of the computation.  however, critical in the accuracy of the implemented learning system is the  precise derivation and faithful distribution of the global learning signals ~(t)  and Ã·(t). stictly speaking, it is essential only to guarantee the correct polarity  and not the exact amplitude of the global learning signals, as implemented in  figure 18.8.  robustness to changes in the environment through on-line adapta-  tion:. this property is inherent to the on-line incremental nature of the stud-  ied supervised and reinforcement learning algorithms, which track structural  changes in ~(t) or ~(t) on a characteristic time scale determined by learning  rate constants such as ~ or ~ and/~. learning rates can be reduced as con-  vergence is approached, as in the popular notion in cognitive neuroscience that  neural plasticity decreases with age and experience.  real-time bandwidth through parallelism:. all learning operations  are performed in parallel, with exception of the three-phase perturbation  scheme (18.4) or (18.11) to obtain the differential index ~ under sequential  activation of complementary perturbations ~r and -~r. we note that the syn-  chronous three-phase scheme is not essential and could be replaced by an asyn-  chronous perturbation scheme as in [12] and [20]. while this probably re-  sembles biology more closely, the synchronous gradient estimate (18.3) using  complementary perturbations is computationally more efficient as it cancels  error terms up to second order in the perturbation strength a [21]. in the  asynchronous scheme, one could envision the role of random noise naturally  present in biological systems as a source of perturbations, although it is not  clear how noise sources can be effectively isolated to produce the correlation  measures necessary for gradient estimation.  modular architecture with local connectivity:. the learning operations  are local in the sense that a need for excessive global interconnects between dis-  tant cells is avoided. the global signals are few in number and common for  all cells, which implies that no signal interconnects are required between cells  across the learning architecture, but all global signals are communicated uni-  formly across cells instead. this allows to embed the learning cells directly432 neuromorphic systems engineering  into the network (or adaptive critic) architecture, where they interface phys-  ically with the synapses they adapt, as in biological systems. the common  global signals include the differential index ~(t) and reinforcement signal Ã·(t),  besides common bias levels and timing signals. ~(t) and Ã·(t) are obtained by  any global mechanism that quantifies the "fitness" of the network response in  terms of teacher target values or discrete rewards (punishments). physiologi-  cal experiments support evidence of local (hebbian [19]) and sparsely globally  interconnected (reinforcement [17]) mechanisms of learning and adaptation in  biological neural systems [11, 25].  ].8.5 conclusion  neuromorphic analog vlsi architectures for a general class of learning tasks im-  plementation. the architectures make use of distributed stochastic techniques  for robust estimation of gradient information, accurately probing the effect  of parameter changes on the performance of the network. two architectures  have been presented: one implementing stochastic error-descent for supervised  learning, and the other implementing a stochastic variant on a generalized form  of reinforcement learning. the two architectures are similar in structure, and  both are suitable for scalable and robust analog vlsi implementation.  while both learning architectures can operate on (and be integrated in)  arbitrary systems of which the characteristics and structure does not need to  be known, the reinforcement learning architecture additionally supports a more  general form of learning, using an arbitrary, externally supplied, reward or  punishment signal. this allows the development of more powerful, generally  applicable devices for "black-box" sensor-motor control which make no prior  assumptions on the structure of the network and the specifics of the desired  network response.  we presented results that demonstrate the effectiveness of perturbative sto-  chastic gradient estimation for supervised learning and reinforcement learning,  applied to nonlinear system identification and adaptive oversampled data con-  version. a recurrent neural network was trained to generate internal dynamics  producing a target periodic orbit at the outputs. a neural classifier controlling  the second-order noise-shaping modulator was trained for optimal performance  with no more evaluative feedback than a discrete failure signal indicating when-  ever any of the modulation integrators saturate. the critical part in the vlsi  implementation of adaptive systems of this type is the precision of the polarity,  rather than the amplitude, of the implemented weight parameter updates. a  binary controlled charge-pump provides voltage increments and decrements of  precise polarity spanning four orders of magnitude in amplitude, with charge  transfers down to a few hundred electrons.analog vlsi stochastic perturbative learning 433  references  [1] j. alspector, r. melt, b. yuhas, and a. jayakumar. a parallel gradient  descent method for learning in analog vlsi neural networks. in advances  in neural information processing systems, volume 5, pages 836-844, san  mateo, ca, 1993. morgan kaufman.  [2] a. g. barto, r. s. sutton, and c. w. anderson. neuronlike adaptive  elements that can solve difficult learning control problems. ieee trans-  actions on systems, man, and cybernetics, 13(5):834-846, 1983.  [3] j. c. candy and g. c. temes. oversampled methods for a/d and d/a  conversion. in oversampled delta-sigma data converters, pages 1-29.  ieee press, 1992.  [4] g. cauwenberghs. a fast stochastic error-descent algorithm for supervised  learning and optimization. in advances in neural information processing  systems, volume 5, pages 244-251, san mateo, ca, 1993. morgan kauf-  man.  [5] g. cauwenberghs. a learning analog neural network chip with continuous-  recurrent dynamics. in advances in neural information processing sys-  terns, volume 6, pages 858 865, san mateo, ca, 1994. morgan kaufman.  [6] g. cauwenberghs. a micropower cmos algorithmic a/d/a converter.  ieee transactions on circuits and systems i: fundamental theory and  applications, 42(11):913-919, 1995.  [?] g. cauwenberghs. analog vlsi long-term dynamic storage. in proceedings  of the ieee international symposium on circuits and systems, atlanta,  ga, 1996.  [8] g. cauwenberghs. an analog vlsi recurrent neural network learning a  continuous-time trajectory. ieee transactions on neural networks, 7(2),  march 1996.  [9] g. cauwenberghs. reinforcement learning in a nonlinear noise shaping  oversampled a/d converter. in proc. int. syrup. circuits and systems,  hong kong, june 1997.  [10] g. cauwenberghs and a. yariv. fault-tolerant dynamic multi-level stor-  age in analog vlsi. ieee transactions on circuits and systems ii,,  41(12):827-829, 1994.  [11] p. churchland and t. sejnowski. the computational brain. mit press,  1993.  [12] a. dembo and t. kailath. model-free distributed learning. ieee trans-  actions on neural networks, 1(1):58-70, 1990.  [13] c. diorio, p. hassler, b. minch, and c. a. mead. 'a single-transistor silicon  synapse. to appear in ieee transactions on electron devices.  [14] b. flower and m. jabri. summed weight neuron perturbation: an ~(n)  improvement over weight perturbation. in advances in neural informa-434 neuromorphic systems engineering  tion processing systems, volume 5, pages 212-219, san mateo, ca, 1993.  morgan kaufman.  [15] s. grossberg. a neural model of attention, reinforcement, and discrimina-  tion learning. international review of neurobiology, 18:263-327, 1975.  [16] s. grossberg and d. s. levine. neural dynamics of attentionally modulated  pavlovian conditioning: blocking, inter-stimulus interval, and secondary  reinforcement. applied optics, 26:5015-5030, 1987.  [17] r. d. hawkins, t. w. abrams, t. j. carew, and e. r. kandell. a cel-  lular mechanism of classical conditioning in aplysia: activity-dependent  amplification of presynaptic facilitation. science, 219:400-405, 1983.  [18] m. jabri and b. flower. weight perturbation: an optimal architecture and  learning technique for analog vlsi feedforward and recurrent multilayered  networks. ieee transactions on neural networks, 3(1):154-157, 1992.  [19] s. r. kelso and t. h. brown. differential conditioning of associative  synaptic enhancement in hippocampal brain slices. science, 232:85-87,  1986.  [20] d. kirk, d. kerns, k. fleischer, and a. bart. analog vlsi implementa-  tion of gradient descent. in advances in neural information processing  systems, volume 5, pages 789-796, san mateo, ca, 1993. morgan kauf-  man.  [21] h. j. kushner and d. s. clark. stochastic approximation methods for  constrained and unconstrained systems. springer-verlag, new york, ny,  1978.  [22] p. r. montague, p. dayan, c. person, and t. j. sejnowski. bee forag-  ing in uncertain environments using predictive hebbian learning. nature,  377(6551) :725-728, 1996.  [23] f. pineda. mean-field theory for batched-td(a). in neural computation,  1996.  [24] h. robins and s. monro. a stochastic approximation method. annals of  mathematical statistics, 22:400-407, 1951.  [25] g. m. shepherd. the synaptic organization of the brain. oxford univ.  press, new york, 3 edition, 1992.  [26] j. c. spall. a stochastic approximation technique for generating maximum  likelihood parameter estimates. in proceedings of the 1987 american con-  trol conference, minneapolis, 1987.  [27] m. a. styblinski and t.-s. tang. experiments in nonconvex optimization:  stochastic approximation with function smoothing and simulated anneal-  ing. neural networks, 3(4):467-483, 1990.  [28] r. s. sutton. learning to predict by the methods of temporal differences.  machine learning, 3:9-44, 1988.  [29] c. watkins and p. dayan. q-learning. machine learning, 8:279-292, 1992.analog vlsi stochastic perturbative learning 435  [30] p. werbos. beyond regression: new tools for prediction and analysis in the  behavioral sciences. in the roots of backpropagation. wiley, new york,  1993.  [31] p. j. werbos. a menu of designs for reinforcement learning over time. in  w. t. miller, r. s. sutton, and p. j. werbos, editors, neural networks for  control, pages 67-95. mit press, cambridge, ma, 1990.a  philippe o. 9 winner-takes-all  associative memory:  hamming distance vector  quantizer  pouliquen, andreas g. andreou and kim strohbehn  19.1 introduction  associative processing and associative memories [10, 13, 14] are neuromorphic  computational paradigms, inspired by the high level brain functions of asso-  ciative memory and recall. several experimental vlsi systems with digital  storage but analog processing have been reported in the literature since the  seminal work by sivilotti, emerling and mead [8]; (see for example [3, 11, 21]  and chapters 16 and 18 of [10]). systems that incorporate analog storage  capabilities have also been reported [7, 12, 24].  in this paper we revisit associative processing in the context of vector quan-  tization and memory-based-processor architectures designed and optimized for  low energy dissipation. energy efficient processing is achieved through a pre-  cision-on-demand architecture. by exploiting the statistics in the structure of  the problem we have designed a system that performs the bulk of the compu-  tations in parallel using low precision, subthreshold analog circuits. however,  power hungry fast recursive processing can be employed on demand, to perform  the small volume of high precision computations.  the analog vlsi system presented in this paper has evolved from our pre-  vious work, and combines current-mode techniques and ideas employed in the  implementation of bidirectional associative memory (bam) models [4, 5] with438 neuromorphic systems engineering  b ~ bll b01 boo b~o  (a) (b)  figure 19.1 memory cells. (a) four transistor sram cell, (b) six transistor enhanced  sram cell.  improved circuits (described in section 19.2) to achieve better performance and  a much higher bit density [21] in an "industrial strength" design. a winner-  takes-all (wta) architecture with positive feedback allows the present system  to select only one of many competing templates, avoiding the undesirable mix-  ing of output templates common to bam architectures.  19.2 circuits and design methodology  in this section, we present the core circuits that are used in the designs. we  begin with the memory cells which stores the templates, followed by the an-  cillary circuits which are needed for biasing and processing the output of the  memory.  we operate our circuits in subthreshold region [18, 25] of the mos transistor.  this enables the design of low-power, energy efficient current-mode circuits [2]  which can be operated at low voltages. in the past, we have applied current  mode techniques to the implementation of bidirectional associative memories  (bam) ([4, 5]). these small chips proved that current mode techniques such as  the clamped bit-line architecture now standard in srams ([22, 23]) and wta  circuits are useful in the design of larger size chips.  19.2.1 memory cells  the bistable memory cell that we developed for use in our associative processors  is based on lyon and schediwy's [17] four transistor static ram (4t-sram)  cell, reproduced in figure 19.1(a).  __  the 4t-sram cell is biased by clamping the bit lines b and b to a voltage  vbias and holding the word line w sufficiently above vbias so that a small amount  of current can flow through the n-type transistors. the cell is then bistable:  one of the internal nodes is at vda (the positive supply rail) while the other at  vbi~. the state of the cell can be sensed by comparing the currents on the bitwinner-takes-all associative memory 439  lines. for instance, if the stored state c is a 1 (i.e. vc = vdd and v~ = vbias),  then current flows on line b, otherwise current would flow on line ~.  the 4t-sram cell can be augmented to add a local arithmetic logic unit  (alu) by adding two transistors, as shown in figure 19.1(b). this enhanced  static ram (e-sram) cell is capable of performing simple boolean functions  of the input w and the stored state c. as in the 4t-sram cell, the e-sram  cell is biased by clamping the bit lines boo, b01, b10, and bll to vbias. the  --  word lines w and w are normally in complimentary states: one word line is  held sufficiently above ybias while the other is held below vbias- under these  conditions, current flows on only one of the four bit lines, so that the cell  effectively computes the logical and of the input and the stored state on bit  line bll.  however, if both word lines are held high, current will flow on two bit lines,  whereas if both word lines are below vbias, no current will flow out of the cell  (and the state of the cell will eventually be lost). for instance, if vdd = 5v  and vbi~ = 3v, we can let the word lines swing from 0v to 5v, and we obtain  the results shown in table 19.1. for the 2.0#m n well cmos processes that  we have used, this results in a bit line current ib of approximately 10na.  other boolean functions can be computed by combining the currents from  multiple bit lines. for instance, if bit lines b01 and b~0 are combined into a  single line bmismatch, current will flow on bit line bmisraatch only if the input  and the stored state are different: the cell therefore effectively computes the  logical xor of the input and the stored state. similarly, bit lines boo and  bll can be combined into a single line bmatc h to compute the logical xnor.  combining three bit lines yields still more boolean functions such as the logical  nand and the logical nor. in fact, it is easy to see that the e-sram cell can  be constructed to perform any boolean function of the input and the stored  state.  19.2.2 memory cell arrays  the e-sram cell is usually arranged in an array, as shown in figure 19.2.  binary template vectors co,..., ck-1 are then stored in the array, one vector  to each row. a binary input vector w is then presented to the array so that  the array can perform a massively parallel computation.  since all the e-sram cells on a given row j share the same bit lines, their  output currents are summed together. thus the current i11j on bit line b11y  is: 1  rt--1  illj --ibew/acj/ =ib iwacj i  i=0  thus we can compute correlations between the input vector and the template  vectors.  alternatively, by combining bit lines into a bmatch bit line and bmismatch  line (as we proposed in section 19.2.1), we can compute440 neuromorphic systems engineering  w,~_~ w~_~ w~ wo  , sl lk-1 blok-i  ,, ,, ,, ,, ,, ,, ,, ,,  ~_ _ ~ _ _ boo~ bo~ 11~--~ b~o~  figure 19.2 typical e-sram cell array. each memory cell (mc) is an e-sram cell.  n-1 ~ ~ ~c, ~: e (~, ~) ~ (~ ~) - '0~, Ã·'~0 ,  ib i----0  which is the hamming distance between the input and template vector cj.  for certain applications, we may choose to first mirror the current from each  of the four bit line rather than directly combining them. for instance, we might  want to compute  i w~cj i  iwl  this requires computing [ w Â® cj i and  i wl=lwacj iÃ·lwa~l = aljÃ·zloj ib  simultaneously. therefore bit lines bo~ and b~o cannot be directly combined.  19.2.3 writing to the memory cells  the state of either memory cell is changed by altering the balance of currents  --  in the cell. for instance, assume that c = 1 and c = 0. then the cell is  flipped to its complementary state by increasing the current drive of a n-type  transistor connected to node c until it overpowers the p-type pull-up.  thus, the state of the of the four transistor sram cell is usually altered  by pulsing one of the bit lines high while the cell is selected (i.e. word line  high). this action momentarily shorts one of the internal nodes to ybias (via  the corresponding n-type transistor) so that the other internal node is pulled  high. for instance, pulsing bit line b stores a 1 and pulsing ~ stores a 0.winner-takes-all associative memory 441  table 19.1 example of e-sram operating states for vdd : 5v and vbias : 3v.  ib ~ lona.  input state output  vc vw v w  0v 5v  0v 5v 3v 5v  5v 3v /boo ibol iblo ibll  ib 0 0 0  o i~ o o  5v 0v 3v 5v 0 0 ~ 0  5v 0v 5v 3v 0 0 0 ~  5v 5v 3v 5v ~ 0 ~ 0  5v 5v 5v 3v 0 ~ 0 ~  0v 0v -- -- 0 0 0 0  in contrast, the most common method used to alter the state of the e-sram  cell is to pulse bit lines b01 and b10 low simultaneously, so that the state of the  __  input w is written to the cell. for instance, if w is low and w is high, pulsing  bit line b01 low pulls c low also.  note that the difference in the approach to writing these memory cells is  due to the manner in which the cells are arrayed: in a static ram array, we  want to write to a single word, that is we want to update all the cells connected  to a particular word line. in contrast, when we array the e-sram cell for  associative processing, we usually want to write to a single template, that is we  want to update all the cells that are connected to the same bit lines.  a restriction of this write method is that pulsing bit lines b0~ and b~0 low  precludes directly combining bit lines b01 and b~0 with any other bit lines. for  some boolean functions it will therefore be necessary to first mirror the bit line  currents and then sum the mirrored currents appropriately. alternatively, it  may be possible to update the templates by using multiple steps. for instance,  pulsing bit lines boo and b~0 low sets all the e-sram cells to state 1. this  can be followed by pulsing bit line b01 low to clear those cells for which the  input is a 0. thus we can still write to the array even in bit lines boo and b10  have been combined into a single line.  even if the bit lines have not been combined, it may be desirable to pulse  a single bit line in order to update only some of the cells in a template vector.  for instance, in carpenter and grossberg's art1 algorithm, which is described  in [6], a template is updated to the logical and of the template with the input.  this is easily achieved by pulsing only bit line b01 low.  19.2.4 current conveyors  the bit lines in the e-sram array need to be clamped to ybias while the current  is sensed or mirrored by other circuits. this bit line clamping is achieved  through the use of current conveyors.442 neuromorphic systems engineering  __:,  (a) (b)  figure 19.3 current conveyors. (a) single transistor voltage controlled. (b) two transistor  current controlled.  figure 19.4 bidirectional communication using a current conveyor.  a current conveyor is a simple circuit that allows a node to be clamped to a  particular voltage while the current flowing into (or out of) the node is conveyed  elsewhere (see also [2]). the simplest instance of such a circuit is a single mos  transistor as shown in figure 19.3(a): the gate node y clamps the source node  x to vx = vy - vas, while the current ix at node x is conveyed to node z.  the control y can also be a current instead of a voltage as shown fig-  ure 19.3(b). this current controlled current conveyor can also be used for  bidirectional communication as shown in figure 19.4: the remote circuit simul-  taneously mirrors the current iy as i.~ (by applying the voltage vx to the gate  of a transistor), and sends a current ix back (by directly applying it to node x)  so that it is conveyed to node z as current i~. multiple remote circuits can be  attached to the same x node such that the conveyed current is the sum of the  currents sourced by the remote circuits. thus, the current controlled current  conveyor was used extensively in earlier bams to implement a "neuron" in  which node x is used as axon and dendritic tree simultaneously (see [3]).  however, neither of the two current conveyors shown in figure 19.3 is appro-  priate for clamping the bit lines, because they source (rather than sink) current  and because the voltage at node x is process dependent. the first problem can  be addressed by replacing the n-type transistors with p-type transistors. the  second problem is solved by embedding a transconductance amplifier (as shown  in figure 19.5) so that the node voltage at x will equal the voltage at y.winner-takes-all associative memory  j  vb< y 115}x  ~ ~ 443  figure 19.5 current conveyor as used for bit line clamping.  t,o  1 1  o1,1 pil~i2~tg:n-fl~ ~1! ~ ~lt ___ ~  ~b~i~ = # l . ~  (~) (b)  figure 19.6 winner-takes-all circuits. (a) voltaÂ£e mode. (b) current mode.  the current conveyors shown in figures 19.3 and 19.5 can only handle uni-  directional currents at node x. although this limitation can be addressed by  designing bidirectional current conveyors, we have found it useful for imple-  menting winner-takes-all structures.  the simplest voltage mode and current mode winner-takes-all circuits (see  also [16]) are shown in figure 19.6. in either case, the circuit is composed of  multiple unidirectional current conveyors attached together at their x nodes,  and a current sink ib. since each current conveyor can only pull the common  x node up, the current conveyor with the largest input will control the x node  voltage and its output (or conveyed) current will equal ib. if multiple current  conveyors have the same (or very nearly the same) input, then they will share  the bias current equally (or nearly equally). therefore, if the output current of  a particular current conveyor exceeds 1 fib, then it clearly must have the largest  input.  current controlled current conveyors are also used to implement cmos  translinear circuits. translinear circuits (see also [9]) are a class of circuits  which take advantage of the exponential dependence of drain current on gate  to source voltage (in the mos transistor in subthreshold operation, see [1, 2])  to perform multiplication and division of currents. the archetypical cmos  translinear circuit is the normalizer circuit shown in figure 19.7.444 neuromorphic systems engineering  g__ +  figure 19.7 cmos translinear normalizer.  by using the approximation that ids = ioe~ in subthreshold, we discover  that the circuit computes  iaib  out -- ic  with a few translinear circuits, some current mirrors and current summing  junctions, we can design circuits which implement almost any polynomial or  fractional function.  19.2.5 mapping neuromorphic algorithms to vlsi  as an example of a simple integrated circuit that can be implemented with  the circuits described in the previous sections, consider the implementation of  carpenter and grossberg's art1 algorithm ([6]).  given the input pattern w, we need to find the template cj that maximizes  iwacj }  o~+ i ~ i  subject to the vigilance constraint  iwac~l  iwl  where a and p are external parameter. >p  with the templates stored in the e-sram array, and the bit lines clamped  by the circuit in figure 19.5, i w a cj l, i cj t, and i w i can be computed  trivially by combining the bit line currents as follows:  11 lj i wacj i= ib  i c i;i wacj i Ã· iwacj i; zllj Ã·iolj g  i w i--i wacu i + iwi~ i = illj Ã·i10j /uwinner-takes-all associative memory 445  therefore, using the translinear normalizer (figure 19.7, we can compute  iaÂ¢- i wic~ lib_ illj  o~q- [ cj [ ia q- illj q- iolj ib  ibj -- i wacj lib - illj  i w i  once the template is selected, it is updated to  cjnew ~ w a cjold  this update is accomplished by pulsing bit line b01j low.  if no template satisfies the vigilance, a new template must be created. for  a cmos implementation, the chip would be initialized with all template bits  set to one, and all templates disabled. then, as new templates are needed,  they would be enabled and written to using the same technique as for updating  templates.  19.3 a winner-takes-all associative memory  the wam associative memory can be mathematically described as a hamming  distance vector quantizer. given the input pattern w, we need to find the  template cj that maximizes  t w*cj i  we have designed a cmos chip using the circuits described in the previous  section (see also [21]) to implement the above function, i.e. to compare an  input binary vector with the stored templates, and return the id (or template  number) of the closest matching template. this chip can store up to 116 binary  vectors (or templates), each of which is up to 124 bits in length. because it uses  a winner-takes-all circuit to select the closest matching template, it is called a  winner-takes-all associative memory, or wam.  19.3.1 architecture  the organization of the wam chip is depicted in figure 19.8.  it is composed mainly of an array of memory cells (labeled mc), which  stores the templates and computes the bitwise exclusive-or of the input with  each template, and a winner-takes-all circuit (labeled wta) to select the closest  matching template.  in addition, shift registers are used to load the input pattern into the chip  (shift register sr1), select a row to update (shift register sr2), and selectively  disable a column (shift register sr3) or a row (shift register sr4). note that  the input shift register has an additional latch (which is not shown) which  allows a new input pattern to be shifted-in while the rest of the chip computes  the closest match on the previous input pattern.  finally, a rom table (labeled rom) converts the output of the winner-  takes-all into a seven bit row id which is the chip's output.446 neuromorphic systems engineering  s  r  2 sr1  sr3  mc tw r s  0 r  a m 4  figure 19.8 architecture of the wary1 chip.  3v  vj bmisma~ch  figure 19.9 write control circuit.  19.3.2 implementation  the array of e-sram cells is like the one shown in figure 19.2, except that  the bit lines have been combined into match and mismatch lines as described  in section 19.2.1.  each mismatch line bmismatchj is connected to a write control circuit shown  in figure 19.9. the write control circuit is controlled by an external write  signal u, and the select signal vj from the update shift register sr2. when  row j is selected for writing, the corresponding signal vj is set high, and the  write signal u is pulsed high momentarily. this pulses bmismatchj low so that  the data in the input latch is written to row j.  each match line bm~tch is connected to a current conveyor of the type shown  in figure 19.5. the conveyed current is fed to node q of the modified current  mode winner-takes-all circuit shown in figure 19.10.  this winner-takes-all circuit is composed of two transistors, mi and m2  which form the current controlled current conveyor, a boost circui~ (detailed in  figure 19.11), and some supplementary transistors.winner-takes-all associative memory 447  3v  mlo  disable~ f  -- common  x line  _  ~4 m~i 1~2 m3~ ~reset  figure 19.10 modified wam winner-takes-all circuit.  transistor m3 is part of the current sink ib required for the winner-takes-all  circuit (as described in section 19.2.4. this current sink is distributed among  each winner-takes-m1 circuit to minimize voltage drops across the common x  line which would otherwise favor the row closest to the current sink. transistor  m4 is connected to shift register sr4 and is used to inhibit the winner-takes-all  by sinking all the current from the brnatchj line conveyed to node q.  the boost circuit is shown in detail in figure 19.11. its main purpose is  to detect when row j is winning. to this end, current isense is mirrored by 1 transistors m5 and m6 and compared to current /threshold ---- ~ib generated by  transistor mt.  3v m~  ~/winner __z i sonso  ~-vthreshold  figure 19.11 details of the winner-takes-all boost circuit.  the resulting signal s and its complement ~ (generated by ms and mg) are  used to drive the rom cell, an example of which is shown in figure 19.12.  each rom cell is composed of eight mos transistors, seven of which encode  the row number in binary, while the eighth is connected to a "valid" line v (to  signal the host computer that a valid row number is present on the output). a  typical rom cell is shown in figure 19.12.  since the threshold for the boost circuit is 1 ~ib, only one row can have a high  s signal. therefore, only one rom cell can drive the output lines at any given448 neuromorphic systems engineering  d6 d5 d4 d3 d~  i d1 do v  figure 19.12 rom line for row 37 (binary 0100101).  time. however, to prevent oscillations between two potential winning rows,  the ~ signal is also connected to transistor m10 in the winner-takes-all circuit:  when row j wins, transistor m10 dumps a large amount of current into the  input (node q) of the winner-takes-all circuit. this effectively pulls node q up  to 3v, raising the common x line sufficiently high (approximately 2.2v) so that  no other row can win thereafter. of course, this positive feedback necessitates  the introduction of a reset signal which drives the common x line to 5v via  transistor mll. with the common x line at 5v and node q at 3v, transistor  m2 passes no current so that the boost circuit is shut off.  the final purpose of the boost circuit is to provide an external adc with a  copy of the bmatc h current from the winning row. this is accomplished with  transistors m12 and m13. transistor m13's gate is connected to to the gate  of transistor mc of the bit line clamp shown in figure 19.5: when transistor  m12 is switched on by the boost circuit, m13's source is driven to 3v so that  it mirrors the current conveyed by the bit line clamp.  the column driving circuitry, shown in figure 19.13, takes advantage of  the special modes of operation of the e-sram (shown in the lower portion of  table 19.1). a column can be disabled by setting e to 0. then, the word lines  --  w and w are both set to 1 (so that all cells in that column output current on  both bit lines regardless of the stored state), or to 0 (so that no cell in that  column outputs a current), depending on the input d.  column or row disabling is used mainly to reduce the effective size of the  memory cell array to the size of the associative processing problem being com-  puted. however, disabling can also be used in the case of a defective memory  cell or winner-takes-all circuit to enhanced the "fault tolerance" of the chip.  since the outcome of the winner-takes-all depends on the magnitude of the  currents from the memory cells, it is important that all the match currents  from the memory cells be the same. this means that the voltages on the  -- bmatch, bmismatch, w, and w lines must be the same. it also means that the  transistors in the memory cells, the current conveyors, and the winner-takes-all  circuits must be well matched.winner-takes-all associative memory 449  :;:i [  [  (a) e dww  0 0 0 0  0 1 1 1  1 0 0 1  1 1 1 0  (b)  figure 19.13 column driver. (a) circuit. (b) operation.  more specifically, since the two n-type transistors attached to the bmatch  line control the current to the winner-takes-all circuit, they must be particularly  well matched. consequently, the memory cell uses n-type transistors in that  position rather than p-type transistors because prior research [20] has shown  n-type transistors to be better matched than their complementary p-types for  n-well processes.  19.3.3 fabrication, testing, and operation  the associative memory chip was implemented in a standard single-poly,  double-metal 2.0#m n-well cmos process on a 6.smm by 6.9mm die. the  memory array occupies 54% of the chip area, the wta 17%, the digital cir-  cuits (including the rom table) 16%, and the input/output pad circuitry 8%.  the remaining is occupied by miscellaneous circuitry and routing channels.  the chip has 124 columns and 116 rows (15kb) for a total bit density of 534  bits/mm 2. a photomicrograph of the chip is shown in figure 19.14.  we also designed a printed circuit support board to facilitate testing of the  wam. the support board allows the host computer to directly control the  wam functions. however, the support board also contains fifo data buffers  and a finite state machine so that it can be operated in an automatic pipelined  mode.  we have tested the wam in a simulated problem of optical character recog-  nition. we loaded the wam with a 8 Ã 15 character font containing the 94  common keyboard characters (ascii codes 33 through 126) and 22 additional  miscellaneous symbols.  we have found that the wam always correctly classifies an input pattern  as long as there are no other templates within a hamming distance of 10 from  the correct template. this is consistent with the known transistor mismatching  limits. indeed, given the size of the transistors in the memory cell, a variation  of up to Â±10% in the current is quite normal. therefore, if two templates450 neuromorphic systems engineering  figure 19.14 photomicrograph of the chip.  are competing, with one template having one hundred matching bits, and the  other one hundred and ten, it is still possible that the first template will win.  therefore, for the proper template to always win under normal circumstances,  it must have a margin of at least ten percent over the other runner-ups.  one of the worst cases is shown in figure 19.15. the characters 'c', 'e', and  'o' are all quite close to each other. in fact, there are many input patterns that  equidistant (in hamming distance) to 'c', 'e', and 'o', one of which is shown  on the right in figure 19.15. this particular pattern is at a hamming distance  of 5 (i.e 5 bits don't match and 115 bits match) from 'c', 'e', and 'o', and can  therefore be classified as any of 'c', 'e', or 'o'. however, if we change one, two  or even three pixels to bring it closer to either 'c', 'e', or 'o', the wam may  incorrectly classify it.  most character bit patterns are rather far from each other in a hamming  distance sense (especially if we limit ourselves to the 94 keyboard characters),  so that in practice, we don't often encounter mis-classified inputs. figure 19.16winner-takes-all associative memory 451  shows a histogram of the hamming distances between pairs of characters in the  font. the dark-colored histogram is for the 94 keyboard character subset.  l: .... :l  lilunllu jiillnli  liliiii!  liiniiin  i-'-::::-"i  figure 19.15 wam processing example. on the left, bit patterns for 'c', 'e', and 'o'. on  the right, input bit pattern equidistant to 'c', 'e', and 'o'.  we have compared the chip to state of the art commercially available mi-  croprocessor based systems. for a given machine, the hamming distance clas-  sification algorithm was implemented in the most efficient way (i.e. we do not  simulate the wam on the general purpose processor.) in an dec-alpha based  general purpose computer it takes 10000 cycles to do a single pattern matching  computation and thus it takes a total of 20#s per classification. power dissi-  pation is 30w at 500 mhz and therefore the energy per classification is 600#j.  the pentium-pro is worse, because it requires 30w at 150 mhz and more than  10000 cycles for a single pattern matching. in contrast, the total current in the  wam is: (124x116x10) na continuous bias current for the memory cells at 5v.  computation time is approximately 70#s for a total energy per classification of  approximately 100 nj. the power dissipation in the buses and memory of the  general purpose processor systems are not included in this comparison.  from the above calculations, it can be seen that the wam chip performs  virtually as fast as the general purpose microprocessor systems but with three  orders of magnitude or more of savings in energy per classification.  19.3.4 arbritrary precision classification  by modifying slightly the wam architecture, we have developed algorihtms  to correct misclassifications due to the low precision analog hardware. when  comparing patterns with a large proportion of matching bits, the templates  in the top ten percent category must have an equally large number of bits in  common. for instance, if as before, one template has one hundred matching  bits, and the other one hundred and ten, then there must be at least 90 bits  in the input that match both templates simultaneously, because there are only  120 bits in each pattern.  therefore, by temporarily masking out the columns corresponding to these  common bits, the effect of device mismatch can be greatly mitigated. in the  above example, we can ignore at least 90 bits, leaving the first template with  10 matching bits and the second template with 20 matching bits out of a total  of 30 bits (or a margin of 33.3% rather than 8.3%).452 neuromorphic systems engineering  we can therefore make use of a modified, iterative algorithm with the cur-  rent wam: at each iteration, the "winning" template and the "runner-ups" in  the top ten-percent are selected. the remaining "losing" templates are then  disabled as well as the common bits, and the process is repeated until a satis-  factory "winner" is found or further iteration is unproductive.  selection of the "runner-ups" is easily achieved by observing the digitized  current from the boost circuit (which indicates the percentage of matching  bits). the current "winner" is disabled allowing the next runner-up to win, a  process that is repeated until the percentage of matching bits drops by more  than ten percent of the original winner.  ~p of  pairs  300.'  200  100  00 10 20 30 40 50 60  hamming distance  figure 19.16 histogram of the hamming distance between character pairs from the font  used in wam testing. the subset of 94 keyboard characters is in black.  19.4 future work  limitation of device matching for this wam chip has already been analyzed  in [15]. this research has allowed us to focus our efforts on those parts of  the wam chip whose increased area would improve the device matching and  thereby most affect the wam's performance.  we have also examined the incorporation of uv adaptation in the winner-  takes-all circuit to improve the resolution of the wam (see [19]).  finally, we have reorganized the layout of the memory cell thereby reducing  its total area by 16% while increasing the area of the n-type transistors to  improve memory cell matching. a long-word version of the wam could be  fabricated in the state-of-the-art process using a large die. for a die area (e.g.  1.4 x 1.2 cm) and a small fabrication feature size (e.g. 0.6 #m process), we  should be able to achieve a 1.2mb density with 7.5m transistors. it should be  noted that the wam implementation already contains features to deal withwinner-takes-all associative memory 453  fault/defect tolerance which will permit us to achieve high yields despite the  large die area.  we also plan to modify the design to allow multichip associative memory  architectures. the current design limits the wam to single chip devices. how-  ever, a different wta circuits could allow the winner-takes-all process to extend  beyond the chip boundaries by allowing multiple wam chips to compete for  the best match.  in a future design we will also add a small digital processor to implement  the arbritrary precision classification algorithm on chip.  19.5 conclusions  we have described our methodology for building high-density current-mode  analog vlsi associative processors. we begin with a very flexible six transistor  memory cell with which we can perform simple boolean operations of the input  and the stored bit. we then process the currents from an array of these memory  cells to perform global computations such as the max and/or normalization  operations.  we have also describe the implementation of such an associative processor  chip. this chip determines the closest match between an input bit pattern  and multiple stored bit templates based on the hamming distance. it is pro-  grammable for template sets of up to 124 bits per template and can store up  to 116 templates. a fully functional 6.smm by 6.9mm chip has been fabricated  in a standard single-poly, double-metal 2.0#m n-well cmos process.  the design abstracts on several principles found in biological systems.  1. memory and processing are integrated in a single structure; this is anal-  ogous to the synapse in biology.  2. the system has an internal model that is related to the problem to be  solved (prior knowledge). this is the template set of patterns to be  classified.  3. the system is capable of learning i.e. templates can be changed to adapt  to a different character set (different problem). this is done at the expense  of storage capacity -we use a ram based cell instead of a more compact  rom cell-.  4. the system processes information in a parallel and hierarchical fashion  in a variable precision architecture. i.e. given the statistics of the prob-  lem, most of the computation is carried out with low precision (three or  four bit) analog hardware. yet arbritrary precision computation is pos-  sible through recursive processing that exploits a programmable wta  (capability to mask specific bits in the winner takes all circuitry).  5. the system is fault tolerant and gracefully degrades. the same structures  that is used in the precision-on-demand architecture can also be used454 neuromorphic systems engineering  to reconfigure the system for defects in the fabrication process. the  components of the chip that are worse matched can be disabled during  operation.  the experimental system presented in this paper suggests that, robust,  miniature, and energetically efficient hardware vlsi systems can ultimately be  achieved by following a methodology which optimizes the design at and between  all levels of system integration; from the device and circuit technique levels all  the way to algorithmic and architectural level considerations. in the future as  technologies for memory based systems become available for experimentation,  we will be able to address the complexity of real world problems. however,  even with non optimized processor-oriented fabrication processes, useful and  perhaps even practical memory based neuromorphic systems are still feasible.  acknowledgments  the research was partially supported by nsf grant ecs-9313934; paul werbos is the  program monitor, by the johns hopkins center for speech and by an onr multidis-  ciplinary university research initiative (muri) for automated vision and sensing  systems n00014-95-1-0409. the final version of this document was prepared by one  of the authors (aga) during his sabbatical leave at caltech. we thank carver mead  for his continuing support and encouragement. chip fabrication was provided by  mosis.  notes  1. a, v, and ~ represent the bit-wise logical and, logical or, and logical xor re-  spectively. i i is the logical norm, or number of bits set to 1.  references  [1] a. andreou and k. boahen. translinear circuits in subthreshold mos. j.  analog integrated circ. sig. proc., 9:141-166, 1996.  [2] a. g. andreou and k. a. boahen. analog vlsi signal and information  processing. in m. ismail and t. fiez, editors, neural information process-  ing ii, pages 358-413. mcgraw-hill, new york, 1994.  [3] k. a. boahen and a. g. andreou. 17. in m. hassoun, editor, associative  neural memories: theory and implementation. oxford university press,  new york, 1993.  [4] k. a. boahen, a. g. andreou, and p. o. pouliquen. architectures for  associative memories using current-mode analog mos circuits. in c. l.  seitz, editor, advanced research in vlsi: proc. dec. caltech conference  on vlsi, cambridge, ma, 1989. mit.  [5] k. a. boahen, p. o. pouliquen, a. g. andreou, and r. e. jenkins. a  heteroassociative memory using current-mode mos analog vlsi circuits.  ieee t. circ. syst, 36(5):747 755, 1989.winner-takes-all associative memory 455  [6] g. a. carpenter and s. grossberg. a massively parallel architecture for  a self-organizing neural pattern recognition machine. computer vision,  graphics, and image processing, 37:54-115, 1987.  [7] g. cauwenberghs, c. f. neugebauer, and a. yariv. analysis and verifica-  tion of an analog vlsi outer-product incremental learning system. ieee  transactions on neural networks, 3(3):488-497, 1992.  [8] m. r. emerling, m. a. sivilotti, and c. a. mead. vlsi architectures  for implementation of neural networks. in j. j. denker, editor, neural  networks for computing. aip, snowbird ut, 1986.  [9] b. gilbert. translinear circuits: a proposed classification. electronics  letters, 11(1):14-16, january 1975.  [10] m. h. hassoun, editor. associative neural memories: theory and imple-  mentation. oxford university press, new york, 1993.  [11] y. he, u. cilingiroglu, and e. sÂ£nchez-sinencio. a high density and  low-power charge-based hamming network. ieee trans. vlsi systems,  1(1):55-62, march 1993.  [12] y. horio and s. nakamura. analog memories for vlsi neurocomputing.  in e. sÂ£nchez-sinencio and c. lau, editors, artificial neural networks:  paradigms, applications, and hardware implementations, pages 344-363.  ieee press, 1992.  [13] t. kohonen. content-addressable memories. springer verlag, berlin, 2  edition, 1987.  [14] t. kohonen. self-organisation and associative memory. springer-verlag,  berlin, 2 edition, 1988.  [15] n. kumar, p. o. pouliquen, and a. g. andreou. device mismatch limita-  tions on the performance of an associative memory system. in proceedings  of the 36 th midwest symposium on circuits and systems, volume 1, pages  570-573, 1993.  [16] j. lazzaro, s. ryckebusch, m. a. mahowald, and c. a. mead. winner-  take-all networks of o(n) complexity. in d.s. touretzky, editor, advances  in neural information processing systems, volume 2, pages 703-711, san  mateo - ca, 1989. morgan kaufmann.  [17] r. f. lyon and r. r. schcdiwy. cmos static memory with a new four-  transistor memory cell. in p. losleben, editor, advanced research in vlsi,  pages 110-132. mit press, cambridge, ma, 1987.  [18] c. a. mead. analog vlsi and neural systems. addison-wesley, reading,  ma, 1989.  [19] h. miwa, n. kumar, p. o. pouliquen, and a. g. andreou. memory en-  hancement techniques for mixed digital memory-analog computational en-  gines. in proc. ieee int. symp. on circuits and systems, volume 5, pages  45-48, june 1994.456 neuromorphic systems engineering  [20] a. pavasovi~, a. g. andreou, and c. r. westgate. characterization of  subthreshold mos mismatch in transistors for vlsi systems. journal of  analog integrated circuits and signal processing, 6:75-85, july 1994.  [21] p. o. pouliquen, a. g. andreou, k. strohbehn, and r. e. jenkins. an  associative memory integrated system for character recognition. in proc.  36th midwest syrup. on circuits and systems, pages 762-765, detroit, mi,  august 1993.  [22] k. sasaki, k. ueda, k. takasugi, h. toyoshima, k. ishibashi, t. ya-  manaka, n. hashimoto, and n. ohki. a 16-mb cmos sram with a 2.3-  #m 2 single bit line memory cell. ieee j. solid-state circuits, 28(11):1125  1130, november 1993.  [23] k. seno, k. knorpp, l.-l. shu, n. teshima, h. kihari, h. sato, f. miyaji,  m. takeda, m. sasaki, y. tomo, p. t. chuang, and k. kobayashi. a 9-  ns 16-rob cmos sram with offset-compensated current sense amplifier.  ieee j. solid-state circuits, 28(11):1119-1124, november 1993.  [24] h. yang and b. j. sheu and j.-c. lee. a nonvolatile analog neural memory  using floating-gate mos transistors. in analog integrated circuits and  signal processing, volume 2-1, february 1992.  [25] e. vittoz and j. fellrath. cmos analog integrated circuits based on weak  inversion operation. ieee journal on solid-state circuits, 12(3):224-231,  1977.index  active undamping, 8  adaptation algorithm, 343  adaptation circuits, 298  adaptation rules, 342  adaptation, 74, 176, 187, 381 382  adapting the sampling rate, 136  adaptive critic element, 415  adaptive neuron, 142, 237  adaptive photoreceptor, 372  adaptive phototransduction, 219  adaptive quantization, 136, 146  adaptive resonance, 384  adaptive sampling, 237  adaptive traveling-wave, 50  adaptive, 3  address event protocol, 344  address-event representation, 234  address-event streams~ 250  address-event-representation, 194  aer, 110  aging, 196  aloha, 196  amplitude distribution, 132, 136  analog memory storage, 315  analog preprocessing, 132  analog versus digital, 94  analog vlsi learning, 422  arbitration, 202, 217  arbitration, 220  arbitration, 238  arreguit, 269  artificial dendrites, 341  artificial nervous systems, 339  artificial synapses, 340  association, 381  associative memory, 437  asynchronous retina, 219  asynchronous, 217  attention system, 171  attentional processing, 152 attenuating low frequencies, 146  attenuating the firing rate, 146  auditory models, 106  auditory representations, 107, 123  auto-associative memories, 384  autocorrelation, 109  automatic gain comtrol, 28  automatic gain control, 13  automatic gain control, 31  automatic gain control, 34  automatic gain control, 51, 92  automatic gain control, 122, 132, 232  automotic gain control, 51  avalanche-breakdown phenomenon, 330  average distance, 414  axonal spike, 340  backpropagation, 116, 343, 384, 389, 411  bandpass filterbank, 15  bandpass filtering, 141  bandpass filtering, 146  bandpass spatial filter, 139  bandpass spatiotemporal filtering, 133  bandp~ss spatiotemporal filtering, 136  bandpass spatiotemporal filtering, 232  bandpass temporal filter, 139  basilar membrane, 5, 20  bidirectional associative memory, 437  biological cochlea, 99  biological perceptive systems, 203  biological synaptic function, 340  blooming, 132  boltzman learning, 389  boltzman statistics, 385  boltzmann, 263  boolean functions, 439  boost circuit, 446  bump linearization, 273  capacitive memory, 386  capacitive-divider, 298458 neuromorphic systems engineering  capacity, 229  carrier sense multiple access, 196  cascaded filters, 6  cascode mirrors, 268  cellular neural networks, 390  central limit theorem, 235  chain rule, 384  channel capactiy, 235  characteristic frequency, 7, 20  charge conservation, 263  charge, 341  charge-pumping, 255  classification, 381  clustered, 233  cochlear implant, 101  cochlear models, 122  coefficient of variation, 235  collision, 202  common-mode characteristics, 276  communication channel, 130  communication processor, 220  compact implementations, 146  compact probabilistic description, 234  comparison, 98  compartment capacitor, 352  composite gains, 9  compression, 7, 24, 44  compressive nonlinearity, 8, 100  compressive, 3  computational primitives, 264  connection adaptation, 355  contention, 238  continuous sensing, 136  continuous-time dynamics, 431  contour map, 375  controlling the gain, 146  convolved excitation, 153  convolved excitation, 163  corner frequency, 51, 62  correlate product, 419  critical paths, 253  crosstalk, 325  crowding, 196  current conveyor, 442  current-mode resistive network, 160  current-mode, 375, 438  current-source synapses, 349  current-spreading networks, 136  delta rule, 389  dendrite parameters, 349  differential pair, 269  differential perturbed error, 417  differentiation, 384, 389  differentiator, 21  diffusor network, 368  diode-capacitor integrator, 143  discrete excitation, 161 discrete exicitation, 153  discrete-output, 370  dispersion relation, 5  distortion, 60, 289  distributed processing system, 217  distributed representation, 429  distributed, 388,432  distributing locally, 367  dynamic programming, 116, 415  dynamic range, 85, 100, 107, 132, 289, 295  dynamical systems, 413  dynamically refreshed capacitors, 344  early voltage, 269  edge enhancement, 206  effective resistance, 157  eikonal, 5  ekv model, 36, 38  emitter degeneration, 267, 271  encoding of variation, 201  enhances edges, 141  error performance, 118  error-free operation, 257  event-driven communication, 255  excitatory feedback, 154  excitatory synaptic conductance, 349  excitatory synaptic currents, 342  exploit locality, 243  exponential nonlinearities, 269  failure-driven, 421  fairness, 248  fault-tolerance, 429  feature extraction, 114  feature vector, 116  features, 32  feedback control, 12  filter-cascade, 3, 7-8  finite-element, 107  floating gate, 386  floating gates, 344  floating-gate adaptation, 298  floating-gate amplifier, 185  floating-gate transistors, 315  floating-gates, 176  follower-integrator, 286  four transistor static ram, 438  foveated architecture, 233  l~requency response, 78  frequency-response, 69  frequency-to-place, 6, 100  full handshaking, 217  fuzzy neural systems, 385  gain adaptation, 79  gain characteristics, 280  gammatone filterbank, 15  gate degeneration, 269  gate degeneration, 271  gaussian distribution, 153index 459  gaussian, 235  generalization, 410  generalized wta architecture, 372  gilbert gain cell, 268  gilbert, 344  gradient descent, 411  gradient estimation, 431  group delays, 9  half-wave rectification, 233  hamming distance, 445  hamming window, 115  handshake circuit, 244  hearing aids, 3  hebbian learning, 357, 384  hebbian learning, 389  hebbian, 339  hidden markov model, 106  hidden markov models, 19  hierarchical arbitration, 225  hierarchical, 247  highpass temporal and spatial filtering, 232  hippocampus, 342  hopfield, 389  horizontal resistor, 346  hot-electron gate current, 316  hot-electron injection, 321,330  hydrodynamic system, 3  hyperpolarization, 349  hysteretic behavior, 367, 370  hysteretic window, 156  image-processing systems, 264  impact ionization, 324  information, 263  inhibition-of-return, 171  inhibitory supply potential, 349  inhibitory synapses, 351  injection efficiency, 323  injection of electrons, 179  injection weight-update, 332  injection, 317  inner-hair-cell, 51  input-referred noise, 289  instability problems, 367  instantaneous distortion nonlinearity, 12  instantaneous nonlinear distortion, 12  integrity, 230  inter-chip communication, 217, 340  interchip communication channel, 256  interchip communication, 144  intracellular potential, 340  kohonen map, 212  kohonen, 178, 389  laplacian, 140  latency, 229  lateral excitation, 367, 370  leakage pathway, 355  learning performance, 419 learning, 381 382  learning-rate degradation, 333  linear filter, 6  linear networks, 207  linear range, 289  linear target function, 185  linearly separable, 139  liouville-green, 3  liouville-green, 4  local automatic gain control, 140  local excitation, 153, 155  local feedback, 367  local interconnections, 367  local learning functions, 426  local membrane potential, 340  local perturbation, 417  local storage, 385  localized excitation, 153  logarithmic transfer function, 145  logarithmic, 6  logarithmically, 370  low pass filter, 40  low-frequency attenuation, 75  low-power, 49, 438  low-transconductance, 270  lumped-parameter, 9  magnitude quantization, 342  mahowald, 129, 194, 202, 217, 243  maximum undistorted, 89  mead, 129, 273, 344  mean spiking rate, 21  micropower, 121  minimizing crosstalk, 326  minimum detectable, 89  model-free learning, 391  model-free, 412  motion encoding, 219  multi-layer perceptron, 116, 343  multi-representation, 121  multichip, 229, 453  negative-feedback technique, 269  neural synapses, 315  neural systems, 264  neuromorphic network, 342  neuromorphic systems, 229, 243  neuromorphic, 3, 6, 9, 102, 114, 152, 176  neuromorphic, 193  neuromorphic, 218, 230, 264, 340, 368, 375,  381  neuromorphic, 388  neuromorphic, 409, 416  neuromorphic, 422  neuromorphic, 429, 437  neuromorphic, 444  neuronal dispersion, 230  neuronal ensemble, 234  neuronal ensemble, 240460 neuromorphic systems engineering  neuronal ensembles, 234  neuronal latency, 230  noise accumulation, 101  noise amplitude, 86  noise spectral density, 58  noise spectrum, 84  non-arbitrated communication, 204  non-linear filter, 372  non-linear transfer function, 343  non-linear, 122  non-volatile memories, 386  nonlinear computation, 155  nonlinearities, 70  nonlinearity, 267  nonvolatile floating-gate, 315  normalization, 153  normalized, 205, 235  normalizer, 207  notch, 9  offset adaptation, 78  offset characteristics, 281  offset-adaptation, 55  offset-compensation, 51-52  offset-correction, 55  ohm's law, 340  on-chip adaptation, 340, 358  on-chip analog adaptation, 358  on-chip computation, 344  optimization, 383  organizing principles, 264  output noise, 57  overlapping-cascades architecture, 74  overtraining, 410  oxide trapping, 333  parallelism, 193  parasitic capacitance, 255  passive dendrites, 341  peak detector, 51  peak frequency, 100  peak throughput, 252  perception, 19, 381  perceptive systems, 213  perceptual linear prediction, 117  permanent storage, 346  phase characteristics, 286  phasic transient-sustained response, 233  phoneme probability vector, 116  phototransistors, 166  pipelined communication channel, 230  pipelining, 225, 243  pixel-parallel communication, 230  poisson process, 204  polarity, 432  position-encoding, 154  post-synaptic membrane, 342  post-synaptic potentials, 341  preattentive, 151 precharging, 354  presynaptic depolarization, 342  precision-on-demand, 437  programmable amorphous silicon resistors,  358  pseudoresonance, 20, 25  pseudoresonant distance, 27  pseudoresonant frequency, 25  pseudoresonant, 7  punctuated event, 233  q-learning, 384, 415  quality factor control, 31  quality factor modulation distance, 29  quality factor, 22, 51  quantal noise, 343  quantization, 135  quantized fluctuations, 342  quantizing, 132  real time, 105  recognition performance, 121  recognition, 117  rectifier, 38  recurrent neural networks, 391  redundancy, 129, 135, 193, 202, 232  redundant, 429  reinforcement learning, 384  reinforcement learning, 415-416  relative timing, 250  resistive-network, 369  resistive-spreading excitation, 153  resolution, 367  resonant, 111  retention time, 387  retinomorphic pixel, 136  retinomorphic system, 237  retinomorphic vision system, 129  retinomorphic, 129  retinormorphic imager, 145  reversal potential, 340  reward/punishment, 381  robust coding, 122  robust estimation, 432  robust mechanism, 367  robustness, 429  saccade, 175  saccadic eye movements, 187  saccadic learning, 178  saliency map, 151  salt-and-pepper noise, 132, 369  sampling circuitry, 354  scale invariant, 50  schmitt trigger, 208  second-order filter, 9, 21, 34, 51, 55, 107  second-order filters, 50  self-organizing, 384  self-timed synapse, 354  semiconductor electronics, 264index 461  sequential polling, 256  short-term adaptation, 175  silicon learning systems, 315, 333  silicon neurons, 339, 342  silicon retina, 206, 375  silicon synapses, 333  sivilotti, 217, 243  slew rate, 35  slope coefficient, 369  slope factor, 36  smoothing, 368  soma, 341  source degeneration, 271  source-degeneration, 269  space constant, 158, 369  sparse activity, 233  spatial derivatives, 152  spatial dimension, 4  spatial frequency, 5  spatial position, 369  spatially distributed synapses, 341  spatio-temporal computation, 341  spatio-temporal filtering, 375  spatiotemporal signal, 145  spectral distribution, 132  spectral subtraction, 123  speech recognition, 114  speech recognition, 116  speech recognizers, 105  speech-recognition systems, 3  spike train, 341  stability, 32  statistical information coding, 193  stochastic approximation, 411  stochastic approximation, 411  stochastic component, 234  stochastic error-descent, 414  stochastic learning, 178  stochastic pertubative, 417  stochastic perturbative, 410, 421  stochastic supervised learning, 412  stochastic techniques, 432  stochastic, 341  structural organization, 131  structured pattern, 372  sublinear summation, 341,358  supervised learning, 343  supervised learning, 383  supervised learning, 410-411  supervised learning, 414  supervised, 381  switched capacitor, 355  synapse transistors, 315  synapse weight, 317  synapse weight-update rule, 333  synapses charge, 345  synaptic activations, 340 synaptic arrays, 333  synaptic efficacies, 342  synaptic efficacy, 340  synaptic weight, 340  temperature, 120  temporal adaptation, 109, 111  temporal autocorrelation, 111  temporal bandwidths, 136  temporal derivative, 177, 330  temporal derivatives, 152  temporal dispersion, 230  temporal dispersion, 241  temporal frequency, 133  temporary local weight, 357  thermal noise, 298  three-stage pipeline, 256  throughput specification, 241  throughput, 230  time difference learning, 384, 415  time-multiplexed channel, 255  timing analysis, 250  total harmonic distortion, 50  total integrated noise, 88  tracking, 164  training time, 183  transconductance amplifier, 269  transconductance, 267  translinear loop, 23  translinear normalizer, 445  translinear, 51  translinear, 140, 207, 268, 386, 443  transmission noise, 212  trapping, 333  traveling wave, 107  traveling-wave, 100  tunneling process, 320  tunneling, 179, 317  unidirectionality, 4  uniform distribution, 184  unsupervised clustering, 385  unsupervised learning, 384  unsupervised, 381  variable-gain, 8  vector quantization, 437  vector quantizer, 384  velocity distribution, 135  virtual wires, 344  visual processing, 151  vlsi implementation, 34  volatile memories, 387  volatile synapse, 422  voltage distribution, 369  wave propagation, 3  wavelet, 107  wavenumber, 4  weak arbitration, 195  went zel-kr amers-brillouin, 4462 neuromorphic systems engineering  wider linear range, 269  winner-take-all, 153  winner-take-all, 197 winner-take-all, 264, 367, 375  winner-takes-all, 219, 443  zweig, 5  overlapping cochlear cascades, 51